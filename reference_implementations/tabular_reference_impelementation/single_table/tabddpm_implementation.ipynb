{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "117b3c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 21 22:45:19 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   32C    P8    15W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-logging",
   "metadata": {},
   "source": [
    "<center>\n",
    "  \n",
    "# TABDDPM: Modelling Tabular Data with Diffusion Models\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d7a55",
   "metadata": {},
   "source": [
    "Directly applying diffusion models to general tabular problems can be challenging because data points are typically represented by vectors of heterogeneous features. The inherent heterogeneity of tabular data complicates accurate modeling, as individual features can vary widely in nature; some may be continuous, while others are discrete. In this notebook, we explore **TabDDPM** — a diffusion model that can be universally applied to tabular datasets and effectively handles both categorical and numerical features.\n",
    "\n",
    "Our primary focus in this work is synthetic data generation, which is in high demand for many tabular tasks. Firstly, tabular datasets are often limited in size, unlike vision or NLP problems where large amounts of additional data are readily available online. Secondly, properly generated synthetic datasets do not contain actual user data, thus avoiding GDPR-like regulations and allowing for public sharing without compromising anonymity.\n",
    "\n",
    "In the following sections, we will delve deeper into the implementation of this method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a5190",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c75e8",
   "metadata": {},
   "source": [
    "In this section, we import all necessary libraries and modules required for setting up the environment. This includes basic libraries such as numpy and pandas. We also import essential modules for data loading, model creation, and dataset downloading and processing dataset. We also specify list of possible datasets and their download URL in `NAME_URL_DICT_UCI` where you can use each of these datasets for the rest of this notebook. Furthermore based on `DATA_DIR` we specify path to raw and processed data in `RAW_DATA_DIR` and `PROCESSED_DATA_DIR` to further download the data and process it in a desired format.  Here we will focus on `\"adult\"` dataset thus we will specify it in `DATA_NAME`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cooperative-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import src\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from scripts.download_dataset import download_from_uci\n",
    "from scripts.process_dataset import process_data\n",
    "\n",
    "from src.data import make_dataset\n",
    "from src.baselines.tabddpm.pipeline import TabDDPM\n",
    "\n",
    "\n",
    "NAME_URL_DICT_UCI = {\n",
    "    \"adult\": \"https://archive.ics.uci.edu/static/public/2/adult.zip\",\n",
    "    \"default\": \"https://archive.ics.uci.edu/static/public/350/default+of+credit+card+clients.zip\",\n",
    "    \"magic\": \"https://archive.ics.uci.edu/static/public/159/magic+gamma+telescope.zip\",\n",
    "    \"shoppers\": \"https://archive.ics.uci.edu/static/public/468/online+shoppers+purchasing+intention+dataset.zip\",\n",
    "    \"beijing\": \"https://archive.ics.uci.edu/static/public/381/beijing+pm2+5+data.zip\",\n",
    "    \"news\": \"https://archive.ics.uci.edu/static/public/332/online+news+popularity.zip\",\n",
    "}\n",
    "\n",
    "DATA_DIR = \"/projects/aieng/diffusion_bootcamp/data/tabular\"\n",
    "RAW_DATA_DIR = f\"{DATA_DIR}/raw_data\"\n",
    "PROCESSED_DATA_DIR = f\"{DATA_DIR}/processed_data\"\n",
    "SYNTH_DATA_DIR = f\"{DATA_DIR}/synthetic_data\"\n",
    "DATA_NAME = \"adult\"\n",
    "\n",
    "MODEL_PATH = f\"/projects/aieng/diffusion_bootcamp/models/tabular/tabddpm\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc51ac5c",
   "metadata": {},
   "source": [
    "# Adult Dataset\n",
    "\n",
    "In this section, we will download the Adult dataset from the UCI repository and load it into a pandas DataFrame. The Adult dataset contains demographic information about individuals, such as age, education, and occupation, and is commonly used for classification tasks. We will use this dataset to demonstrate the TabDDPM method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-utility",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3100d23",
   "metadata": {},
   "source": [
    "We can download the required adult dataset to the specified directory in `RAW_DATA_DIR` using the download_from_uci function. This function takes the dataset name, the download path, and the URL of the data, and retrieves it from the UCI repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adolescent-korea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing dataset adult from UCI.\n",
      "Aready downloaded.\n"
     ]
    }
   ],
   "source": [
    "download_from_uci(DATA_NAME, RAW_DATA_DIR, NAME_URL_DICT_UCI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-blend",
   "metadata": {},
   "source": [
    "## Process Data\n",
    "\n",
    "Now that we have downloaded the dataset, we need to process it into the desired CSV format using the `process_data` function. To do this, we provide the dataset name, the directory containing the information required for preprocessing, and the original data directory. The `INFO_DIR` contains a JSON file for each dataset, specifying the following:\n",
    "\n",
    "1. task_type: This must be specified and can be binclass (binary classification) or regression, depending on the type of task for each dataset. For the adult dataset, the task type is binclass.\n",
    "2. column_names: This is optional and contains the names of each column.\n",
    "3. num_col_idx: This is necessary to specify the columns with numerical values.\n",
    "4. cat_col_idx: This is necessary to specify the columns with categorical values.\n",
    "5. target_col_idx: This is necessary to specify the column containing the target value for the regression or classification task.\n",
    "6. file_type: This should be set to \"csv\" by default, as we want to preprocess the files as CSV.\n",
    "7. data_path: Optional\n",
    "8. test_path: Optional\n",
    "9. column_info: Optional\n",
    "10. train_num: Optional\n",
    "11. test_num: Optional\n",
    "\n",
    "The `process_data` function divides the raw data into training and test splits, saving them in `PROCESSED_DATA_DIR`. It also saves the processed information of the data as a JSON file in the same directory. Finally, it prints out general information about the training and test data after preprocessing, including:\n",
    "\n",
    "1. Size of the training, validation, and test tables\n",
    "2. Size of the numerical values in the training table\n",
    "3. Size of the categorical values in the training table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "simplified-ending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult (32561, 15) (16281, 15) (32561, 15)\n",
      "Numerical (32561, 6)\n",
      "Categorical (32561, 8)\n",
      "Processing and Saving adult Successfully!\n",
      "adult\n",
      "Total 48842\n",
      "Train 32561\n",
      "Test 16281\n",
      "Num 6\n",
      "Cat 9\n"
     ]
    }
   ],
   "source": [
    "INFO_DIR = \"data_info\"\n",
    "process_data(DATA_NAME, INFO_DIR, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-cologne",
   "metadata": {},
   "source": [
    "## Review Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad79ab5",
   "metadata": {},
   "source": [
    "Following preprocessing, here we will review the train dataset.\n",
    "\n",
    "Adult dataset consist of 15 columns in total where 6 columns are numerical and 9 columns are categorical. The target column is `income` which is a binary classification task. The dataset contains 32561 rows in total. You can see 20 first rows of the dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rapid-booth",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.0</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646.0</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721.0</td>\n",
       "      <td>11th</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>284582.0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>160187.0</td>\n",
       "      <td>9th</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Married-spouse-absent</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>52.0</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>209642.0</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>31.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>45781.0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>14084.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>42.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>159449.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>5178.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>37.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>280464.0</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>30.0</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>141297.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>India</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>23.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>122272.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>205019.0</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>40.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>121772.0</td>\n",
       "      <td>Assoc-voc</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>?</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>34.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>245487.0</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Amer-Indian-Eskimo</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>25.0</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>176756.0</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>186824.0</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>38.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>28887.0</td>\n",
       "      <td>11th</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>43.0</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>292175.0</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age          workclass    fnlwgt      education  education.num  \\\n",
       "0   39.0          State-gov   77516.0      Bachelors           13.0   \n",
       "1   50.0   Self-emp-not-inc   83311.0      Bachelors           13.0   \n",
       "2   38.0            Private  215646.0        HS-grad            9.0   \n",
       "3   53.0            Private  234721.0           11th            7.0   \n",
       "4   28.0            Private  338409.0      Bachelors           13.0   \n",
       "5   37.0            Private  284582.0        Masters           14.0   \n",
       "6   49.0            Private  160187.0            9th            5.0   \n",
       "7   52.0   Self-emp-not-inc  209642.0        HS-grad            9.0   \n",
       "8   31.0            Private   45781.0        Masters           14.0   \n",
       "9   42.0            Private  159449.0      Bachelors           13.0   \n",
       "10  37.0            Private  280464.0   Some-college           10.0   \n",
       "11  30.0          State-gov  141297.0      Bachelors           13.0   \n",
       "12  23.0            Private  122272.0      Bachelors           13.0   \n",
       "13  32.0            Private  205019.0     Assoc-acdm           12.0   \n",
       "14  40.0            Private  121772.0      Assoc-voc           11.0   \n",
       "15  34.0            Private  245487.0        7th-8th            4.0   \n",
       "16  25.0   Self-emp-not-inc  176756.0        HS-grad            9.0   \n",
       "17  32.0            Private  186824.0        HS-grad            9.0   \n",
       "18  38.0            Private   28887.0           11th            7.0   \n",
       "19  43.0   Self-emp-not-inc  292175.0        Masters           14.0   \n",
       "\n",
       "            marital.status          occupation    relationship  \\\n",
       "0            Never-married        Adm-clerical   Not-in-family   \n",
       "1       Married-civ-spouse     Exec-managerial         Husband   \n",
       "2                 Divorced   Handlers-cleaners   Not-in-family   \n",
       "3       Married-civ-spouse   Handlers-cleaners         Husband   \n",
       "4       Married-civ-spouse      Prof-specialty            Wife   \n",
       "5       Married-civ-spouse     Exec-managerial            Wife   \n",
       "6    Married-spouse-absent       Other-service   Not-in-family   \n",
       "7       Married-civ-spouse     Exec-managerial         Husband   \n",
       "8            Never-married      Prof-specialty   Not-in-family   \n",
       "9       Married-civ-spouse     Exec-managerial         Husband   \n",
       "10      Married-civ-spouse     Exec-managerial         Husband   \n",
       "11      Married-civ-spouse      Prof-specialty         Husband   \n",
       "12           Never-married        Adm-clerical       Own-child   \n",
       "13           Never-married               Sales   Not-in-family   \n",
       "14      Married-civ-spouse        Craft-repair         Husband   \n",
       "15      Married-civ-spouse    Transport-moving         Husband   \n",
       "16           Never-married     Farming-fishing       Own-child   \n",
       "17           Never-married   Machine-op-inspct       Unmarried   \n",
       "18      Married-civ-spouse               Sales         Husband   \n",
       "19                Divorced     Exec-managerial       Unmarried   \n",
       "\n",
       "                   race      sex  capital.gain  capital.loss  hours.per.week  \\\n",
       "0                 White     Male        2174.0           0.0            40.0   \n",
       "1                 White     Male           0.0           0.0            13.0   \n",
       "2                 White     Male           0.0           0.0            40.0   \n",
       "3                 Black     Male           0.0           0.0            40.0   \n",
       "4                 Black   Female           0.0           0.0            40.0   \n",
       "5                 White   Female           0.0           0.0            40.0   \n",
       "6                 Black   Female           0.0           0.0            16.0   \n",
       "7                 White     Male           0.0           0.0            45.0   \n",
       "8                 White   Female       14084.0           0.0            50.0   \n",
       "9                 White     Male        5178.0           0.0            40.0   \n",
       "10                Black     Male           0.0           0.0            80.0   \n",
       "11   Asian-Pac-Islander     Male           0.0           0.0            40.0   \n",
       "12                White   Female           0.0           0.0            30.0   \n",
       "13                Black     Male           0.0           0.0            50.0   \n",
       "14   Asian-Pac-Islander     Male           0.0           0.0            40.0   \n",
       "15   Amer-Indian-Eskimo     Male           0.0           0.0            45.0   \n",
       "16                White     Male           0.0           0.0            35.0   \n",
       "17                White     Male           0.0           0.0            40.0   \n",
       "18                White     Male           0.0           0.0            50.0   \n",
       "19                White   Female           0.0           0.0            45.0   \n",
       "\n",
       "    native.country  income  \n",
       "0    United-States   <=50K  \n",
       "1    United-States   <=50K  \n",
       "2    United-States   <=50K  \n",
       "3    United-States   <=50K  \n",
       "4             Cuba   <=50K  \n",
       "5    United-States   <=50K  \n",
       "6          Jamaica   <=50K  \n",
       "7    United-States    >50K  \n",
       "8    United-States    >50K  \n",
       "9    United-States    >50K  \n",
       "10   United-States    >50K  \n",
       "11           India    >50K  \n",
       "12   United-States   <=50K  \n",
       "13   United-States   <=50K  \n",
       "14               ?    >50K  \n",
       "15          Mexico   <=50K  \n",
       "16   United-States   <=50K  \n",
       "17   United-States   <=50K  \n",
       "18   United-States   <=50K  \n",
       "19   United-States    >50K  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{PROCESSED_DATA_DIR}/{DATA_NAME}/train.csv\")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ee997",
   "metadata": {},
   "source": [
    "Also this dataset contains missing values which are represented as `?` in the dataset. We will replace these missing values with the most frequent value in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "brown-curtis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ? exists in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "value = \" ?\"\n",
    "if value in df.values:\n",
    "    print(f\"{value} exists in the DataFrame.\")\n",
    "else:\n",
    "    print(f\"{value} does not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adjusted-respect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'adult', 'task_type': 'binclass', 'header': None, 'column_names': ['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'capital.gain', 'capital.loss', 'hours.per.week', 'native.country', 'income'], 'num_col_idx': [0, 2, 4, 10, 11, 12], 'cat_col_idx': [1, 3, 5, 6, 7, 8, 9, 13], 'target_col_idx': [14], 'file_type': 'csv', 'data_path': '/projects/aieng/diffusion_bootcamp/data/tabular/raw_data/adult/adult.data', 'test_path': '/projects/aieng/diffusion_bootcamp/data/tabular/raw_data/adult/adult.test', 'column_info': {'0': {}, 'type': 'categorical', 'max': 99.0, 'min': 1.0, '2': {}, '4': {}, '10': {}, '11': {}, '12': {}, '1': {}, 'categorizes': [' <=50K', ' >50K'], '3': {}, '5': {}, '6': {}, '7': {}, '8': {}, '9': {}, '13': {}, '14': {}}, 'train_num': 32561, 'test_num': 16281, 'idx_mapping': {'0': 0, '1': 6, '2': 1, '3': 7, '4': 2, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, '10': 3, '11': 4, '12': 5, '13': 13, '14': 14}, 'inverse_idx_mapping': {'0': 0, '6': 1, '1': 2, '7': 3, '2': 4, '8': 5, '9': 6, '10': 7, '11': 8, '12': 9, '3': 10, '4': 11, '5': 12, '13': 13, '14': 14}, 'idx_name_mapping': {'0': 'age', '1': 'workclass', '2': 'fnlwgt', '3': 'education', '4': 'education.num', '5': 'marital.status', '6': 'occupation', '7': 'relationship', '8': 'race', '9': 'sex', '10': 'capital.gain', '11': 'capital.loss', '12': 'hours.per.week', '13': 'native.country', '14': 'income'}, 'metadata': {'columns': {'0': {'sdtype': 'numerical', 'computer_representation': 'Float'}, '2': {'sdtype': 'numerical', 'computer_representation': 'Float'}, '4': {'sdtype': 'numerical', 'computer_representation': 'Float'}, '10': {'sdtype': 'numerical', 'computer_representation': 'Float'}, '11': {'sdtype': 'numerical', 'computer_representation': 'Float'}, '12': {'sdtype': 'numerical', 'computer_representation': 'Float'}, '1': {'sdtype': 'categorical'}, '3': {'sdtype': 'categorical'}, '5': {'sdtype': 'categorical'}, '6': {'sdtype': 'categorical'}, '7': {'sdtype': 'categorical'}, '8': {'sdtype': 'categorical'}, '9': {'sdtype': 'categorical'}, '13': {'sdtype': 'categorical'}, '14': {'sdtype': 'categorical'}}}}\n"
     ]
    }
   ],
   "source": [
    "# Open the JSON file and read its contents\n",
    "with open(f\"{PROCESSED_DATA_DIR}/{DATA_NAME}/info.json\", \"r\") as file:\n",
    "    data_info = json.load(file)\n",
    "\n",
    "# Display the JSON data\n",
    "print(data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-factory",
   "metadata": {},
   "source": [
    "# TabDDPM Algorithem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daaffdc",
   "metadata": {},
   "source": [
    "In this section, we will describe the design of TabDDPM as well as its main hyperparameters loaded through config, which affect the model’s effectiveness. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b425e7",
   "metadata": {},
   "source": [
    "**TabDDPM:** uses the multinomial diffusion to model the categorical and binary features, and the Gaussian diffusion to model the numerical ones. The model is trained using the diffusion process, which is a continuous-time Markov chain that models the data distribution. In more detail, for a tabular data sample that consists of N numerical features and C categorical features with Ki categories each, TabDDPM takes one-hot encoded versions of categorical features as an input, and normalized numerical features. The figure below illustrates the diffusion process for classification problems; t, y and l denote a diffusion timestep, a class label, and logits, respectively.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabddpm.png\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37161c0",
   "metadata": {},
   "source": [
    "**Diffusion models:**  are likelihood-based generative models that handle the data through forward and reverse Markov processes. The forward process gradually adds noise to an initial sample x0 from the data distribution q(x0) sampling noise from the predefined distributions q(xt|xt−1) with variances {β1, ..., βT}.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/forward.png\" width=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e6bfff",
   "metadata": {},
   "source": [
    "The reverse diffusion proces gradually denoises a latent variable xT∼q(xT) and allows generating new data samples from q(x0). Distributions p(xt−1|xt) are usually unknown and approximated by a neural network with parameters θ.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/backward.png\" width=\"280\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c2058",
   "metadata": {},
   "source": [
    "**Gaussian diffusion models:** operate in continuous spaces where forward and reverse processes are characterized by Gaussian distributions:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/gaussian.png\" width=\"440\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad099f5",
   "metadata": {},
   "source": [
    "While in general θ parameters are learned from the data by optimizing a variational lower bound, in practice for Gaussian modeling, this objective can be simplified to the sum of mean-squared errors between εθ(xt ,t) and ε over all timesteps t as follows:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/gaussian_loss.png\" width=\"330\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a46d1",
   "metadata": {},
   "source": [
    "**Multinomial diffusion models:** are designed to generate categorical data where samples are a one-hot encoded categorical variable with K values. The multinomial forward diffusion process defines q(xt|xt−1) as a categorical distribution that corrupts the data by uniform noise over K classes: \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/multinomial.png\" width=\"440\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d537779d",
   "metadata": {},
   "source": [
    "The reverse distribution pθ(xt−1|xt) is parameterized as q(xt−1|xt,xˆ0(xt,t)), where xˆ0 is predicted by a neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f362ca1",
   "metadata": {},
   "source": [
    "## Load Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7213754",
   "metadata": {},
   "source": [
    "In this section, we will load the configuration file that contains the hyperparameters for the TabDDPM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8a5701a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'parent_dir': 'configs/adult', 'model_save_path': 'ckpt/adult', 'sample_save_path': 'sampled/adult', 'real_data_path': 'Data/adult', 'num_numerical_features': 6, 'model_type': 'mlp', 'task_type': 'binclass', 'model_params': {'num_classes': 2, 'is_y_cond': False, 'rtdl_params': {'d_layers': [1024, 2048, 2048, 1024], 'dropout': 0.0}}, 'diffusion_params': {'num_timesteps': 1000, 'gaussian_loss_type': 'mse'}, 'train': {'main': {'steps': 100000, 'lr': 0.001809824563637657, 'weight_decay': 0.0005, 'batch_size': 4096}, 'T': {'seed': 0, 'normalization': 'quantile', 'num_nan_policy': 'mean', 'cat_nan_policy': None, 'cat_min_frequency': None, 'cat_encoding': None, 'y_policy': 'default'}}, 'sample': {'num_samples': 32561, 'batch_size': 10000, 'seed': 0}, 'eval': {'type': {'eval_model': 'mlp', 'eval_type': 'synthetic'}, 'T': {'seed': 0, 'normalization': 'quantile', 'num_nan_policy': None, 'cat_nan_policy': None, 'cat_min_frequency': None, 'cat_encoding': 'one-hot', 'y_policy': 'default'}}}\n"
     ]
    }
   ],
   "source": [
    "config_path = f\"src/baselines/tabddpm/configs/{DATA_NAME}.toml\"\n",
    "## Tabddpm is trained unconditional in this repo but conditional in main paper\n",
    "raw_config = src.load_config(config_path)\n",
    "\n",
    "print(raw_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871a2bc8",
   "metadata": {},
   "source": [
    "The configuration file is a TOML file that contains the following hyperparameters:\n",
    "\n",
    "1. **model_type:**  specifies the type of backbone model to be used for learning the denoising process. For Adult dataset, this should be set to \"mlp\". The reverse diffusion step in TabDDPM is modelled by a multi-layer neural network that has an output of the same dimensionality as x0, where the first N coordinates are the predictions of ε for the Gaussian diffusion and the rest are the predictions of x_ohe for the multinomial diffusions. This model takes as input the corrupted data xt and the timestep t, and outputs the denoised data xt−1 as follows:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/architecture.png\" width=\"440\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b8823",
   "metadata": {},
   "source": [
    "2. **model_params:** contains the hyperparameters for the backbone model. For the Adult dataset, we use an MLP model with the following hyperparameters:\n",
    "    - is_y_cond: Whether the model is trained to be conditioned on the target value or not. By default, this should be set to True.\n",
    "    - d_layers: The dimension of layers in the MLP model.\n",
    "    - dropout: The dropout rate.\n",
    "\n",
    "\n",
    "    \n",
    "3. **task_type:** specifies the task type that we will use to conidition our training to and can be binclass (binary classification) or regression, depending on each dataset. For the Adult dataset, the task type is binclass. For classification datasets, we use a class- conditional model, i.e. pθ(xt−1|xt, y) is learned. For regression datasets, we consider a target value as an additional numerical feature, and the joint distribution is learned.\n",
    "\n",
    "4. **diffusion_params:** contains number of total diffusion steps, the diffusion step size, and type of loss used to minimize the predicited noise in Gaussian diffusion. \n",
    "\n",
    "5. **train.main:** contains the basic hyperparameters for training the model such number of epochs (steps), the batch size (batch_size), the learning rate(lr), and the rate of weight decay (weight_decay).\n",
    "\n",
    "6. **train.T:** contains the defined transformations on train data such as normalization, standardization, and one-hot encoding. \n",
    "    - For preprocessing numerical features, we use the gaussian quantile transformation and replace the Nan values with mean of each row. \n",
    "    - For categorical features, we use the one-hot encoding method. Each categorical feature is handled by a separate forward diffusion process, i.e., the noise components for all features are sampled independently. \n",
    "\n",
    "7. **sample:** contains the hyperparameters for sampling the data from the trained model. It includes the number of samples to generate, the batch size of sampling, and the seed used for random noise initialization.\n",
    "\n",
    "8. **eval.T:** contains the defined transformations on evaluation data such as normalization, standardization, and one-hot encoding. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-quality",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b6854c",
   "metadata": {},
   "source": [
    "Now that we have processed the data, we can create a dataset object. First we instantiate transformations needed for the dataset, such as normalization, standardization, and one-hot encoding in `T`. Next using `make_dataset` function we create a dataset object that contains the training and test data, as well as the column names, numerical column indices, and categorical column indices. This function takes the directory containing the processed data, the transformation and task type as input. Also it takes a boolean argument `change_val` that if it set true it will change the validation data to a split of train data rather than test data.\n",
    "It returns a dataset object that contains the training and test data, as well as the column names, numerical column indices, and categorical column indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "second-romance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path /projects/aieng/diffusion_bootcamp/data/tabular/processed_data/adult\n",
      "No NaNs in numerical features, skipping\n"
     ]
    }
   ],
   "source": [
    "T = src.Transformations(**raw_config[\"train\"][\"T\"])\n",
    "\n",
    "dataset = make_dataset(\n",
    "    f\"{PROCESSED_DATA_DIR}/{DATA_NAME}\",\n",
    "    T,\n",
    "    task_type=raw_config[\"task_type\"],\n",
    "    change_val=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "billion-convention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset num shape:  (32561, 6)\n",
      "test dataset num shape:  (16281, 6)\n",
      "train dataset cat shape:  (32561, 9)\n",
      "test dataset cat shape:  (16281, 9)\n",
      "{'policy': 'default'}\n",
      "binclass\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"train dataset num shape: \", dataset.X_num[\"train\"].shape)\n",
    "print(\"test dataset num shape: \", dataset.X_num[\"test\"].shape)\n",
    "\n",
    "print(\"train dataset cat shape: \", dataset.X_cat[\"train\"].shape)\n",
    "print(\"test dataset cat shape: \", dataset.X_cat[\"test\"].shape)\n",
    "\n",
    "print(dataset.y_info)\n",
    "print(dataset.task_type)\n",
    "print(dataset.n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b294c",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a84c6",
   "metadata": {},
   "source": [
    "Next, we instantiate the TabDDPM model. To do so first we need to apply some modifactions to loaded configs based on the dataset. We will set the number of numerical and size of one-hot coded categorical features based on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "allied-sunday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  9 16  7 15  6  5  2 42]\n"
     ]
    }
   ],
   "source": [
    "dim_categorical_features = np.array(dataset.get_category_sizes(\"train\"))\n",
    "print(dim_categorical_features)\n",
    "if len(dim_categorical_features) == 0 or raw_config[\"train\"][\"T\"][\"cat_encoding\"] == \"one-hot\":\n",
    "    dim_categorical_features = np.array([0])\n",
    "\n",
    "num_numerical_features = (\n",
    "    dataset.X_num[\"train\"].shape[1] if dataset.X_num is not None else 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59532a94",
   "metadata": {},
   "source": [
    "We will also set the input size of the model as sum of size of categorical plus length of numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c2fb00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "dim_input = np.sum(dim_categorical_features) + num_numerical_features\n",
    "raw_config[\"model_params\"][\"d_in\"] = dim_input\n",
    "print(dim_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a14719",
   "metadata": {},
   "source": [
    "Also we set device to be \"cuda\" if available otherwise we will use \"cpu\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f47437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661314c",
   "metadata": {},
   "source": [
    "Finally, we will instantiate the model using the `TabDDPM` class. TabDDPM class takes the following arguments:\n",
    "\n",
    "1. dataset: The dataset object containing the training and test data, as well as the column names, numerical column indices, and categorical column indices.\n",
    "2. num_numerical_features: The number of numerical features in the dataset.\n",
    "3. dim_categorical_features: The list of size of the one-hot encoded categorical features.\n",
    "4. model_type: The type of backbone model to be used for learning the denoising process.\n",
    "5. model_params: The hyperparameters for the backbone model.\n",
    "6. diffusion_params: The hyperparameters for the diffusion process.\n",
    "7. device: The device to use for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "developed-gross",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_classes': 2, 'is_y_cond': False, 'rtdl_params': {'d_layers': [1024, 2048, 2048, 1024], 'dropout': 0.0}, 'd_in': 110}\n",
      "mlp\n",
      "the number of parameters 11768942\n"
     ]
    }
   ],
   "source": [
    "tabddpm = TabDDPM(\n",
    "    dataset=dataset,\n",
    "    real_data_path=None,\n",
    "    num_numerical_features=num_numerical_features,\n",
    "    num_classes=dim_categorical_features,\n",
    "    model_type=raw_config[\"model_type\"],\n",
    "    model_params=raw_config[\"model_params\"],\n",
    "    device=device,\n",
    "    **raw_config[\"diffusion_params\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38efa0e",
   "metadata": {},
   "source": [
    "TabDDPM class first instantiates the backbone model, which is a multi-layer neural network that models the reverse diffusion process using `get_model` function. It then passes this model to `GaussianMultinomialDiffusion` class to initializes the diffusion process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-boost",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f204dce9",
   "metadata": {},
   "source": [
    "Now that we have instantiated the model, we can train it using the `train` function. This function takes the training arguments from the config file and path to save the trained model. It trains the model on the training data using the `Trainer` class. It returns the trained model and the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "russian-jimmy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps:  100000\n",
      "Step 1/100000 MLoss: 2.2689 GLoss: 1.3328 Sum: 3.6017\n",
      "Time:  3.0133402347564697\n",
      "Step 2/100000 MLoss: 71.3832 GLoss: 81195.7812 Sum: 81267.1644\n",
      "Time:  0.6674737930297852\n",
      "Step 3/100000 MLoss: 1.4556 GLoss: 2.1067 Sum: 3.5623\n",
      "Time:  0.9954547882080078\n",
      "Step 4/100000 MLoss: 1.8169 GLoss: 1.3838 Sum: 3.2007\n",
      "Time:  0.17519283294677734\n",
      "Step 5/100000 MLoss: 1.557 GLoss: 1.0342 Sum: 2.5911999999999997\n",
      "Time:  0.15912580490112305\n",
      "Step 6/100000 MLoss: 1.3366 GLoss: 1.3013 Sum: 2.6379\n",
      "Time:  0.08142685890197754\n",
      "Step 7/100000 MLoss: 1.8318 GLoss: 9.6681 Sum: 11.4999\n",
      "Time:  0.0809018611907959\n",
      "Step 8/100000 MLoss: 1.2443 GLoss: 1.1241 Sum: 2.3684000000000003\n",
      "Time:  1.3292090892791748\n",
      "Step 9/100000 MLoss: 1.2951 GLoss: 1.1845 Sum: 2.4796\n",
      "Time:  0.07293200492858887\n",
      "Step 10/100000 MLoss: 1.2851 GLoss: 1.0437 Sum: 2.3288\n",
      "Time:  0.1494612693786621\n",
      "Step 11/100000 MLoss: 1.5444 GLoss: 3.1473 Sum: 4.6917\n",
      "Time:  0.06942224502563477\n",
      "Step 12/100000 MLoss: 2.3898 GLoss: 2.9666 Sum: 5.356400000000001\n",
      "Time:  0.07774591445922852\n",
      "Step 13/100000 MLoss: 1.609 GLoss: 2.4894 Sum: 4.0984\n",
      "Time:  0.07498431205749512\n",
      "Step 14/100000 MLoss: 2.2907 GLoss: 1.4248 Sum: 3.7155000000000005\n",
      "Time:  0.07447457313537598\n",
      "Step 15/100000 MLoss: 1.6947 GLoss: 5.3074 Sum: 7.0021\n",
      "Time:  0.07558727264404297\n",
      "Step 16/100000 MLoss: 2.0253 GLoss: 5.5798 Sum: 7.6051\n",
      "Time:  0.07527709007263184\n",
      "Step 17/100000 MLoss: 2.2885 GLoss: 1.1053 Sum: 3.3937999999999997\n",
      "Time:  0.07678842544555664\n",
      "Step 18/100000 MLoss: 1.4896 GLoss: 3.1149 Sum: 4.6045\n",
      "Time:  0.07253694534301758\n",
      "Step 19/100000 MLoss: 1.3081 GLoss: 1.8178 Sum: 3.1259\n",
      "Time:  0.07922863960266113\n",
      "Step 20/100000 MLoss: 1.4182 GLoss: 1.4945 Sum: 2.9127\n",
      "Time:  0.07529282569885254\n",
      "Step 21/100000 MLoss: 1.3712 GLoss: 1.5678 Sum: 2.939\n",
      "Time:  0.07525062561035156\n",
      "Step 22/100000 MLoss: 1.3129 GLoss: 1.8796 Sum: 3.1925\n",
      "Time:  0.07477569580078125\n",
      "Step 23/100000 MLoss: 1.308 GLoss: 1.358 Sum: 2.6660000000000004\n",
      "Time:  0.07612133026123047\n",
      "Step 24/100000 MLoss: 1.3999 GLoss: 1.1793 Sum: 2.5792\n",
      "Time:  0.0756385326385498\n",
      "Step 25/100000 MLoss: 1.3573 GLoss: 1.2288 Sum: 2.5861\n",
      "Time:  0.07586479187011719\n",
      "Step 26/100000 MLoss: 1.2823 GLoss: 1.0564 Sum: 2.3387000000000002\n",
      "Time:  0.07057428359985352\n",
      "Step 27/100000 MLoss: 1.278 GLoss: 1.1268 Sum: 2.4048\n",
      "Time:  0.07612037658691406\n",
      "Step 28/100000 MLoss: 1.2456 GLoss: 1.0289 Sum: 2.2744999999999997\n",
      "Time:  0.14986324310302734\n",
      "Step 29/100000 MLoss: 1.2361 GLoss: 1.0987 Sum: 2.3348\n",
      "Time:  0.0699009895324707\n",
      "Step 30/100000 MLoss: 1.2391 GLoss: 0.9966 Sum: 2.2357\n",
      "Time:  0.14687347412109375\n",
      "Step 31/100000 MLoss: 1.2468 GLoss: 1.0336 Sum: 2.2804\n",
      "Time:  0.06670284271240234\n",
      "Step 32/100000 MLoss: 1.281 GLoss: 1.0136 Sum: 2.2946\n",
      "Time:  0.07642459869384766\n",
      "Step 33/100000 MLoss: 1.2556 GLoss: 1.0046 Sum: 2.2602\n",
      "Time:  0.07716989517211914\n",
      "Step 34/100000 MLoss: 1.2398 GLoss: 1.0061 Sum: 2.2459\n",
      "Time:  0.0745995044708252\n",
      "Step 35/100000 MLoss: 1.2321 GLoss: 0.9841 Sum: 2.2161999999999997\n",
      "Time:  0.14472699165344238\n",
      "Step 36/100000 MLoss: 1.3031 GLoss: 1.0007 Sum: 2.3038\n",
      "Time:  0.06749463081359863\n",
      "Step 37/100000 MLoss: 1.2397 GLoss: 0.9992 Sum: 2.2389\n",
      "Time:  0.07233309745788574\n",
      "Step 38/100000 MLoss: 1.2302 GLoss: 0.9968 Sum: 2.227\n",
      "Time:  0.07603192329406738\n",
      "Step 39/100000 MLoss: 1.2476 GLoss: 1.0151 Sum: 2.2626999999999997\n",
      "Time:  0.0754547119140625\n",
      "Step 40/100000 MLoss: 1.3042 GLoss: 1.0034 Sum: 2.3076\n",
      "Time:  0.07448720932006836\n",
      "Step 41/100000 MLoss: 1.2009 GLoss: 0.9989 Sum: 2.1998\n",
      "Time:  0.1488351821899414\n",
      "Step 42/100000 MLoss: 1.218 GLoss: 1.0048 Sum: 2.2228\n",
      "Time:  0.06969141960144043\n",
      "Step 43/100000 MLoss: 1.1623 GLoss: 1.0038 Sum: 2.1661\n",
      "Time:  0.1486661434173584\n",
      "Step 44/100000 MLoss: 1.1814 GLoss: 1.0099 Sum: 2.1913\n",
      "Time:  0.06953191757202148\n",
      "Step 45/100000 MLoss: 1.1907 GLoss: 1.0051 Sum: 2.1958\n",
      "Time:  0.08039331436157227\n",
      "Step 46/100000 MLoss: 1.1736 GLoss: 0.9957 Sum: 2.1693\n",
      "Time:  0.07454419136047363\n",
      "Step 47/100000 MLoss: 1.1942 GLoss: 0.9999 Sum: 2.1940999999999997\n",
      "Time:  0.07349944114685059\n",
      "Step 48/100000 MLoss: 1.215 GLoss: 1.0063 Sum: 2.2213000000000003\n",
      "Time:  0.07387423515319824\n",
      "Step 49/100000 MLoss: 1.2231 GLoss: 1.0113 Sum: 2.2344\n",
      "Time:  0.07692551612854004\n",
      "Step 50/100000 MLoss: 1.2118 GLoss: 0.9956 Sum: 2.2074\n",
      "Time:  0.07349729537963867\n",
      "Step 51/100000 MLoss: 1.23 GLoss: 1.0133 Sum: 2.2433\n",
      "Time:  0.07507848739624023\n",
      "Step 52/100000 MLoss: 1.1473 GLoss: 0.9933 Sum: 2.1406\n",
      "Time:  0.14533376693725586\n",
      "Step 53/100000 MLoss: 1.1779 GLoss: 0.9993 Sum: 2.1772\n",
      "Time:  0.06298184394836426\n",
      "Step 54/100000 MLoss: 1.1957 GLoss: 0.9902 Sum: 2.1859\n",
      "Time:  0.07174110412597656\n",
      "Step 55/100000 MLoss: 1.1256 GLoss: 0.996 Sum: 2.1216\n",
      "Time:  0.13931512832641602\n",
      "Step 56/100000 MLoss: 1.1705 GLoss: 1.0199 Sum: 2.1904000000000003\n",
      "Time:  0.06290364265441895\n",
      "Step 57/100000 MLoss: 1.1527 GLoss: 0.9951 Sum: 2.1478\n",
      "Time:  0.07364773750305176\n",
      "Step 58/100000 MLoss: 1.2147 GLoss: 1.0109 Sum: 2.2256\n",
      "Time:  0.06834530830383301\n",
      "Step 59/100000 MLoss: 1.2131 GLoss: 1.0012 Sum: 2.2143\n",
      "Time:  0.07092428207397461\n",
      "Step 60/100000 MLoss: 1.2382 GLoss: 1.0053 Sum: 2.2435\n",
      "Time:  0.06985259056091309\n",
      "Step 61/100000 MLoss: 1.1997 GLoss: 0.9955 Sum: 2.1952\n",
      "Time:  0.07181835174560547\n",
      "Step 62/100000 MLoss: 1.2357 GLoss: 1.0024 Sum: 2.2381\n",
      "Time:  0.06970739364624023\n",
      "Step 63/100000 MLoss: 1.1832 GLoss: 0.9972 Sum: 2.1804\n",
      "Time:  0.07193470001220703\n",
      "Step 64/100000 MLoss: 1.1978 GLoss: 0.9986 Sum: 2.1964\n",
      "Time:  0.06995320320129395\n",
      "Step 65/100000 MLoss: 1.1731 GLoss: 1.0059 Sum: 2.1790000000000003\n",
      "Time:  0.0727548599243164\n",
      "Step 66/100000 MLoss: 1.183 GLoss: 1.011 Sum: 2.194\n",
      "Time:  0.06941461563110352\n",
      "Step 67/100000 MLoss: 1.177 GLoss: 0.9816 Sum: 2.1586\n",
      "Time:  0.07166361808776855\n",
      "Step 68/100000 MLoss: 1.1514 GLoss: 1.0063 Sum: 2.1577\n",
      "Time:  0.07007479667663574\n",
      "Step 69/100000 MLoss: 1.1752 GLoss: 1.0116 Sum: 2.1868\n",
      "Time:  0.07059073448181152\n",
      "Step 70/100000 MLoss: 1.1801 GLoss: 0.9911 Sum: 2.1712\n",
      "Time:  0.0748300552368164\n",
      "Step 71/100000 MLoss: 1.1533 GLoss: 1.0068 Sum: 2.1601\n",
      "Time:  0.07108831405639648\n",
      "Step 72/100000 MLoss: 1.2423 GLoss: 1.0 Sum: 2.2423\n",
      "Time:  0.06763911247253418\n",
      "Step 73/100000 MLoss: 1.2389 GLoss: 1.0076 Sum: 2.2465\n",
      "Time:  0.07309865951538086\n",
      "Step 74/100000 MLoss: 1.269 GLoss: 1.0159 Sum: 2.2849\n",
      "Time:  0.06991267204284668\n",
      "Step 75/100000 MLoss: 1.1802 GLoss: 1.0054 Sum: 2.1856\n",
      "Time:  0.07198762893676758\n",
      "Step 76/100000 MLoss: 1.177 GLoss: 1.011 Sum: 2.1879999999999997\n",
      "Time:  0.06962728500366211\n",
      "Step 77/100000 MLoss: 1.1878 GLoss: 0.9976 Sum: 2.1854\n",
      "Time:  0.06965756416320801\n",
      "Step 78/100000 MLoss: 1.1869 GLoss: 1.002 Sum: 2.1889000000000003\n",
      "Time:  0.07136154174804688\n",
      "Step 79/100000 MLoss: 1.1407 GLoss: 0.9958 Sum: 2.1365\n",
      "Time:  0.07267427444458008\n",
      "Step 80/100000 MLoss: 1.2152 GLoss: 0.9996 Sum: 2.2148000000000003\n",
      "Time:  0.07024455070495605\n",
      "Step 81/100000 MLoss: 1.2607 GLoss: 0.9935 Sum: 2.2542\n",
      "Time:  0.07294821739196777\n",
      "Step 82/100000 MLoss: 1.1707 GLoss: 1.0157 Sum: 2.1864\n",
      "Time:  0.06974053382873535\n",
      "Step 83/100000 MLoss: 1.2264 GLoss: 1.0071 Sum: 2.2335000000000003\n",
      "Time:  0.07252216339111328\n",
      "Step 84/100000 MLoss: 1.1852 GLoss: 1.0129 Sum: 2.1981\n",
      "Time:  0.07584786415100098\n",
      "Step 85/100000 MLoss: 1.1528 GLoss: 0.9935 Sum: 2.1463\n",
      "Time:  0.07615375518798828\n",
      "Step 86/100000 MLoss: 1.2047 GLoss: 0.994 Sum: 2.1987\n",
      "Time:  0.0749807357788086\n",
      "Step 87/100000 MLoss: 1.1495 GLoss: 0.9886 Sum: 2.1381\n",
      "Time:  0.07367372512817383\n",
      "Step 88/100000 MLoss: 1.1693 GLoss: 0.9996 Sum: 2.1689\n",
      "Time:  0.07545614242553711\n",
      "Step 89/100000 MLoss: 1.1703 GLoss: 1.0009 Sum: 2.1712\n",
      "Time:  0.07665181159973145\n",
      "Step 90/100000 MLoss: 1.3773 GLoss: 1.0011 Sum: 2.3784\n",
      "Time:  0.07534170150756836\n",
      "Step 91/100000 MLoss: 1.2069 GLoss: 0.9823 Sum: 2.1892\n",
      "Time:  0.07473397254943848\n",
      "Step 92/100000 MLoss: 1.1723 GLoss: 0.9968 Sum: 2.1691\n",
      "Time:  0.07644152641296387\n",
      "Step 93/100000 MLoss: 1.1776 GLoss: 1.0078 Sum: 2.1854\n",
      "Time:  0.07621502876281738\n",
      "Step 94/100000 MLoss: 1.1769 GLoss: 0.9943 Sum: 2.1712\n",
      "Time:  0.07371354103088379\n",
      "Step 95/100000 MLoss: 1.2158 GLoss: 0.9946 Sum: 2.2104\n",
      "Time:  0.0773928165435791\n",
      "Step 96/100000 MLoss: 1.3803 GLoss: 1.0115 Sum: 2.3918\n",
      "Time:  0.0753469467163086\n",
      "Step 97/100000 MLoss: 1.1812 GLoss: 0.9972 Sum: 2.1784\n",
      "Time:  0.07631897926330566\n",
      "Step 98/100000 MLoss: 1.2878 GLoss: 1.0005 Sum: 2.2883\n",
      "Time:  0.07433962821960449\n",
      "Step 99/100000 MLoss: 1.203 GLoss: 1.0042 Sum: 2.2072000000000003\n",
      "Time:  0.07628798484802246\n",
      "Step 100/100000 MLoss: 1.2032 GLoss: 0.9886 Sum: 2.1918\n",
      "Time:  0.07561278343200684\n",
      "Step 101/100000 MLoss: 1.236 GLoss: 1.0033 Sum: 2.2393\n",
      "Time:  0.07522058486938477\n",
      "Step 102/100000 MLoss: 1.1741 GLoss: 1.0197 Sum: 2.1938\n",
      "Time:  0.06995844841003418\n",
      "Step 103/100000 MLoss: 1.5021 GLoss: 1.0139 Sum: 2.516\n",
      "Time:  0.07555770874023438\n",
      "Step 104/100000 MLoss: 1.2359 GLoss: 1.0045 Sum: 2.2404\n",
      "Time:  0.07431244850158691\n",
      "Step 105/100000 MLoss: 1.2214 GLoss: 0.9958 Sum: 2.2172\n",
      "Time:  0.07765698432922363\n",
      "Step 106/100000 MLoss: 1.1786 GLoss: 1.0014 Sum: 2.18\n",
      "Time:  0.07478713989257812\n",
      "Step 107/100000 MLoss: 1.1828 GLoss: 1.0017 Sum: 2.1845\n",
      "Time:  0.07632923126220703\n",
      "Step 108/100000 MLoss: 1.2004 GLoss: 1.0066 Sum: 2.207\n",
      "Time:  0.07435226440429688\n",
      "Step 109/100000 MLoss: 1.1845 GLoss: 1.0115 Sum: 2.196\n",
      "Time:  0.07824087142944336\n",
      "Step 110/100000 MLoss: 1.1747 GLoss: 0.9939 Sum: 2.1686\n",
      "Time:  0.07126450538635254\n",
      "Step 111/100000 MLoss: 1.2148 GLoss: 1.0025 Sum: 2.2173\n",
      "Time:  0.07099771499633789\n",
      "Step 112/100000 MLoss: 1.241 GLoss: 0.9889 Sum: 2.2299\n",
      "Time:  0.07116150856018066\n",
      "Step 113/100000 MLoss: 1.2236 GLoss: 1.0225 Sum: 2.2461\n",
      "Time:  0.07319211959838867\n",
      "Step 114/100000 MLoss: 1.1687 GLoss: 1.0014 Sum: 2.1701\n",
      "Time:  0.07224416732788086\n",
      "Step 115/100000 MLoss: 1.1781 GLoss: 0.9968 Sum: 2.1749\n",
      "Time:  0.07097268104553223\n",
      "Step 116/100000 MLoss: 1.1707 GLoss: 1.0056 Sum: 2.1763000000000003\n",
      "Time:  0.07106566429138184\n",
      "Step 117/100000 MLoss: 1.2147 GLoss: 0.9873 Sum: 2.202\n",
      "Time:  0.07255744934082031\n",
      "Step 118/100000 MLoss: 1.2366 GLoss: 1.0016 Sum: 2.2382\n",
      "Time:  0.069732666015625\n",
      "Step 119/100000 MLoss: 1.1908 GLoss: 1.0031 Sum: 2.1939\n",
      "Time:  0.07151436805725098\n",
      "Step 120/100000 MLoss: 1.2347 GLoss: 0.9958 Sum: 2.2305\n",
      "Time:  0.0710141658782959\n",
      "Step 121/100000 MLoss: 1.1622 GLoss: 0.9973 Sum: 2.1595\n",
      "Time:  0.07187056541442871\n",
      "Step 122/100000 MLoss: 1.2825 GLoss: 0.9953 Sum: 2.2778\n",
      "Time:  0.06921577453613281\n",
      "Step 123/100000 MLoss: 1.1729 GLoss: 1.0125 Sum: 2.1854\n",
      "Time:  0.07267451286315918\n",
      "Step 124/100000 MLoss: 1.1823 GLoss: 1.0066 Sum: 2.1889\n",
      "Time:  0.07018017768859863\n",
      "Step 125/100000 MLoss: 1.1266 GLoss: 0.9993 Sum: 2.1259\n",
      "Time:  0.07232952117919922\n",
      "Step 126/100000 MLoss: 1.1756 GLoss: 1.0095 Sum: 2.1851000000000003\n",
      "Time:  0.07049226760864258\n",
      "Step 127/100000 MLoss: 1.184 GLoss: 1.0115 Sum: 2.1955\n",
      "Time:  0.07167172431945801\n",
      "Step 128/100000 MLoss: 1.1944 GLoss: 1.0015 Sum: 2.1959\n",
      "Time:  0.06904101371765137\n",
      "Step 129/100000 MLoss: 1.161 GLoss: 1.0157 Sum: 2.1767000000000003\n",
      "Time:  0.07636117935180664\n",
      "Step 130/100000 MLoss: 1.168 GLoss: 0.9973 Sum: 2.1653\n",
      "Time:  0.07472991943359375\n",
      "Step 131/100000 MLoss: 1.2248 GLoss: 0.9941 Sum: 2.2189\n",
      "Time:  0.07544708251953125\n",
      "Step 132/100000 MLoss: 1.1529 GLoss: 1.0022 Sum: 2.1551\n",
      "Time:  0.07582688331604004\n",
      "Step 133/100000 MLoss: 1.157 GLoss: 1.0017 Sum: 2.1587\n",
      "Time:  0.07507538795471191\n",
      "Step 134/100000 MLoss: 1.186 GLoss: 0.9997 Sum: 2.1856999999999998\n",
      "Time:  0.07404613494873047\n",
      "Step 135/100000 MLoss: 1.1703 GLoss: 0.9994 Sum: 2.1696999999999997\n",
      "Time:  0.07653284072875977\n",
      "Step 136/100000 MLoss: 1.103 GLoss: 0.9857 Sum: 2.0887000000000002\n",
      "Time:  0.146134614944458\n",
      "Step 137/100000 MLoss: 1.1996 GLoss: 1.0021 Sum: 2.2016999999999998\n",
      "Time:  0.06978654861450195\n",
      "Step 138/100000 MLoss: 1.2052 GLoss: 0.9865 Sum: 2.1917\n",
      "Time:  0.07023978233337402\n",
      "Step 139/100000 MLoss: 1.1388 GLoss: 1.0045 Sum: 2.1433\n",
      "Time:  0.07224822044372559\n",
      "Step 140/100000 MLoss: 1.1925 GLoss: 0.997 Sum: 2.1895\n",
      "Time:  0.07122540473937988\n",
      "Step 141/100000 MLoss: 1.1499 GLoss: 1.0082 Sum: 2.1581\n",
      "Time:  0.07236361503601074\n",
      "Step 142/100000 MLoss: 1.2037 GLoss: 0.9943 Sum: 2.198\n",
      "Time:  0.07175993919372559\n",
      "Step 143/100000 MLoss: 1.1737 GLoss: 1.0084 Sum: 2.1821\n",
      "Time:  0.07040262222290039\n",
      "Step 144/100000 MLoss: 1.1714 GLoss: 1.0039 Sum: 2.1753\n",
      "Time:  0.07133722305297852\n",
      "Step 145/100000 MLoss: 1.1625 GLoss: 1.0076 Sum: 2.1701\n",
      "Time:  0.07182884216308594\n",
      "Step 146/100000 MLoss: 1.1294 GLoss: 1.0139 Sum: 2.1433\n",
      "Time:  0.07209658622741699\n",
      "Step 147/100000 MLoss: 1.1757 GLoss: 1.0047 Sum: 2.1803999999999997\n",
      "Time:  0.0693979263305664\n",
      "Step 148/100000 MLoss: 1.158 GLoss: 0.9953 Sum: 2.1532999999999998\n",
      "Time:  0.07183623313903809\n",
      "Step 149/100000 MLoss: 1.1706 GLoss: 0.9978 Sum: 2.1684\n",
      "Time:  0.07306957244873047\n",
      "Step 150/100000 MLoss: 1.166 GLoss: 1.0027 Sum: 2.1687\n",
      "Time:  0.07330203056335449\n",
      "Step 151/100000 MLoss: 1.2103 GLoss: 1.0042 Sum: 2.2145\n",
      "Time:  0.0697479248046875\n",
      "Step 152/100000 MLoss: 1.16 GLoss: 0.987 Sum: 2.147\n",
      "Time:  0.07241272926330566\n",
      "Step 153/100000 MLoss: 1.1413 GLoss: 1.0058 Sum: 2.1471\n",
      "Time:  0.07314682006835938\n",
      "Step 154/100000 MLoss: 1.1629 GLoss: 0.9871 Sum: 2.15\n",
      "Time:  0.07017636299133301\n",
      "Step 155/100000 MLoss: 1.2128 GLoss: 0.9895 Sum: 2.2023\n",
      "Time:  0.07181978225708008\n",
      "Step 156/100000 MLoss: 1.1625 GLoss: 0.9948 Sum: 2.1573\n",
      "Time:  0.07718396186828613\n",
      "Step 157/100000 MLoss: 1.159 GLoss: 1.006 Sum: 2.165\n",
      "Time:  0.07567119598388672\n",
      "Step 158/100000 MLoss: 1.1838 GLoss: 1.0134 Sum: 2.1972\n",
      "Time:  0.07357978820800781\n",
      "Step 159/100000 MLoss: 1.1768 GLoss: 1.0049 Sum: 2.1817\n",
      "Time:  0.07356977462768555\n",
      "Step 160/100000 MLoss: 1.1799 GLoss: 1.0017 Sum: 2.1816\n",
      "Time:  0.07567787170410156\n",
      "Step 161/100000 MLoss: 1.1665 GLoss: 0.9896 Sum: 2.1561000000000003\n",
      "Time:  0.07774710655212402\n",
      "Step 162/100000 MLoss: 1.204 GLoss: 1.0014 Sum: 2.2054\n",
      "Time:  0.07770133018493652\n",
      "Step 163/100000 MLoss: 1.1006 GLoss: 1.0015 Sum: 2.1021\n",
      "Time:  0.07414722442626953\n",
      "Step 164/100000 MLoss: 1.1474 GLoss: 1.0002 Sum: 2.1475999999999997\n",
      "Time:  0.07658863067626953\n",
      "Step 165/100000 MLoss: 1.1309 GLoss: 1.0019 Sum: 2.1328\n",
      "Time:  0.0761270523071289\n",
      "Step 166/100000 MLoss: 1.1751 GLoss: 0.9896 Sum: 2.1647\n",
      "Time:  0.07544946670532227\n",
      "Step 167/100000 MLoss: 1.1323 GLoss: 1.0122 Sum: 2.1445\n",
      "Time:  0.07475566864013672\n",
      "Step 168/100000 MLoss: 1.1597 GLoss: 0.9963 Sum: 2.1559999999999997\n",
      "Time:  0.07584476470947266\n",
      "Step 169/100000 MLoss: 1.1774 GLoss: 0.9917 Sum: 2.1691000000000003\n",
      "Time:  0.0773172378540039\n",
      "Step 170/100000 MLoss: 1.1257 GLoss: 1.0105 Sum: 2.1361999999999997\n",
      "Time:  0.07539582252502441\n",
      "Step 171/100000 MLoss: 1.2315 GLoss: 0.9998 Sum: 2.2313\n",
      "Time:  0.07494449615478516\n",
      "Step 172/100000 MLoss: 1.1222 GLoss: 1.0054 Sum: 2.1276\n",
      "Time:  0.07760071754455566\n",
      "Step 173/100000 MLoss: 1.137 GLoss: 1.0157 Sum: 2.1527000000000003\n",
      "Time:  0.07620882987976074\n",
      "Step 174/100000 MLoss: 1.1833 GLoss: 1.0033 Sum: 2.1866000000000003\n",
      "Time:  0.07144427299499512\n",
      "Step 175/100000 MLoss: 1.1516 GLoss: 0.9925 Sum: 2.1441\n",
      "Time:  0.0711359977722168\n",
      "Step 176/100000 MLoss: 1.1333 GLoss: 0.9914 Sum: 2.1247\n",
      "Time:  0.07100486755371094\n",
      "Step 177/100000 MLoss: 1.1548 GLoss: 0.9912 Sum: 2.146\n",
      "Time:  0.07161140441894531\n",
      "Step 178/100000 MLoss: 1.1869 GLoss: 1.0106 Sum: 2.1975\n",
      "Time:  0.07100129127502441\n",
      "Step 179/100000 MLoss: 1.1488 GLoss: 0.9886 Sum: 2.1374\n",
      "Time:  0.07097792625427246\n",
      "Step 180/100000 MLoss: 1.0908 GLoss: 1.0092 Sum: 2.1\n",
      "Time:  0.07156968116760254\n",
      "Step 181/100000 MLoss: 1.1033 GLoss: 1.0211 Sum: 2.1243999999999996\n",
      "Time:  0.07007646560668945\n",
      "Step 182/100000 MLoss: 1.1607 GLoss: 0.9975 Sum: 2.1582\n",
      "Time:  0.07165956497192383\n",
      "Step 183/100000 MLoss: 1.1383 GLoss: 0.9878 Sum: 2.1261\n",
      "Time:  0.0709528923034668\n",
      "Step 184/100000 MLoss: 1.1431 GLoss: 1.0035 Sum: 2.1466000000000003\n",
      "Time:  0.07217907905578613\n",
      "Step 185/100000 MLoss: 1.1225 GLoss: 1.0049 Sum: 2.1273999999999997\n",
      "Time:  0.0726630687713623\n",
      "Step 186/100000 MLoss: 1.1297 GLoss: 0.9937 Sum: 2.1234\n",
      "Time:  0.07332801818847656\n",
      "Step 187/100000 MLoss: 1.157 GLoss: 1.0035 Sum: 2.1605\n",
      "Time:  0.07151651382446289\n",
      "Step 188/100000 MLoss: 1.1865 GLoss: 0.9794 Sum: 2.1659\n",
      "Time:  0.07275152206420898\n",
      "Step 189/100000 MLoss: 1.176 GLoss: 1.0112 Sum: 2.1872\n",
      "Time:  0.07116174697875977\n",
      "Step 190/100000 MLoss: 1.1064 GLoss: 0.9861 Sum: 2.0925000000000002\n",
      "Time:  0.07159900665283203\n",
      "Step 191/100000 MLoss: 1.092 GLoss: 0.9989 Sum: 2.0909\n",
      "Time:  0.07168722152709961\n",
      "Step 192/100000 MLoss: 1.1862 GLoss: 1.0013 Sum: 2.1875\n",
      "Time:  0.06841516494750977\n",
      "Step 193/100000 MLoss: 1.0923 GLoss: 0.9896 Sum: 2.0819\n",
      "Time:  0.1468501091003418\n",
      "Step 194/100000 MLoss: 1.193 GLoss: 1.0082 Sum: 2.2012\n",
      "Time:  0.07048249244689941\n",
      "Step 195/100000 MLoss: 1.1316 GLoss: 1.0024 Sum: 2.134\n",
      "Time:  0.07380890846252441\n",
      "Step 196/100000 MLoss: 1.1644 GLoss: 1.0108 Sum: 2.1752000000000002\n",
      "Time:  0.07720041275024414\n",
      "Step 197/100000 MLoss: 1.1364 GLoss: 1.0039 Sum: 2.1403\n",
      "Time:  0.0761866569519043\n",
      "Step 198/100000 MLoss: 1.1811 GLoss: 0.9886 Sum: 2.1697\n",
      "Time:  0.07623410224914551\n",
      "Step 199/100000 MLoss: 1.1329 GLoss: 1.0108 Sum: 2.1437\n",
      "Time:  0.07411384582519531\n",
      "Step 200/100000 MLoss: 1.1183 GLoss: 1.0097 Sum: 2.128\n",
      "Time:  0.0762166976928711\n",
      "Step 201/100000 MLoss: 1.1467 GLoss: 0.9988 Sum: 2.1455\n",
      "Time:  0.07705426216125488\n",
      "Step 202/100000 MLoss: 1.1292 GLoss: 1.0148 Sum: 2.144\n",
      "Time:  0.07665538787841797\n",
      "Step 203/100000 MLoss: 1.1256 GLoss: 0.9984 Sum: 2.1239999999999997\n",
      "Time:  0.07524967193603516\n",
      "Step 204/100000 MLoss: 1.0985 GLoss: 1.0104 Sum: 2.1089\n",
      "Time:  0.07629895210266113\n",
      "Step 205/100000 MLoss: 1.16 GLoss: 1.0024 Sum: 2.1624\n",
      "Time:  0.07185792922973633\n",
      "Step 206/100000 MLoss: 1.0629 GLoss: 0.9984 Sum: 2.0613\n",
      "Time:  0.14280939102172852\n",
      "Step 207/100000 MLoss: 1.157 GLoss: 0.9997 Sum: 2.1567\n",
      "Time:  0.06279373168945312\n",
      "Step 208/100000 MLoss: 1.1572 GLoss: 1.0045 Sum: 2.1616999999999997\n",
      "Time:  0.07074570655822754\n",
      "Step 209/100000 MLoss: 1.1305 GLoss: 0.9996 Sum: 2.1301\n",
      "Time:  0.07462310791015625\n",
      "Step 210/100000 MLoss: 1.1235 GLoss: 1.0099 Sum: 2.1334\n",
      "Time:  0.07166290283203125\n",
      "Step 211/100000 MLoss: 1.1389 GLoss: 1.0039 Sum: 2.1428000000000003\n",
      "Time:  0.07035160064697266\n",
      "Step 212/100000 MLoss: 1.1356 GLoss: 1.0104 Sum: 2.146\n",
      "Time:  0.07143735885620117\n",
      "Step 213/100000 MLoss: 1.1603 GLoss: 1.0024 Sum: 2.1627\n",
      "Time:  0.07145452499389648\n",
      "Step 214/100000 MLoss: 1.1331 GLoss: 1.0029 Sum: 2.136\n",
      "Time:  0.07227253913879395\n",
      "Step 215/100000 MLoss: 1.1287 GLoss: 1.0052 Sum: 2.1339\n",
      "Time:  0.06946110725402832\n",
      "Step 216/100000 MLoss: 1.1258 GLoss: 0.9954 Sum: 2.1212\n",
      "Time:  0.07208752632141113\n",
      "Step 217/100000 MLoss: 1.1196 GLoss: 0.9938 Sum: 2.1134\n",
      "Time:  0.07179617881774902\n",
      "Step 218/100000 MLoss: 1.1723 GLoss: 0.9785 Sum: 2.1508\n",
      "Time:  0.07311248779296875\n",
      "Step 219/100000 MLoss: 1.1605 GLoss: 1.0028 Sum: 2.1633\n",
      "Time:  0.07229185104370117\n",
      "Step 220/100000 MLoss: 1.1179 GLoss: 0.9894 Sum: 2.1073\n",
      "Time:  0.07358908653259277\n",
      "Step 221/100000 MLoss: 1.1779 GLoss: 1.0086 Sum: 2.1864999999999997\n",
      "Time:  0.07174944877624512\n",
      "Step 222/100000 MLoss: 1.1387 GLoss: 0.9929 Sum: 2.1316\n",
      "Time:  0.07245945930480957\n",
      "Step 223/100000 MLoss: 1.1249 GLoss: 1.0059 Sum: 2.1308\n",
      "Time:  0.07188677787780762\n",
      "Step 224/100000 MLoss: 1.1312 GLoss: 0.9916 Sum: 2.1228\n",
      "Time:  0.06888604164123535\n",
      "Step 225/100000 MLoss: 1.0837 GLoss: 1.0147 Sum: 2.0984\n",
      "Time:  0.07378101348876953\n",
      "Step 226/100000 MLoss: 1.1301 GLoss: 0.992 Sum: 2.1221\n",
      "Time:  0.07012462615966797\n",
      "Step 227/100000 MLoss: 1.11 GLoss: 0.9959 Sum: 2.1059\n",
      "Time:  0.07259821891784668\n",
      "Step 228/100000 MLoss: 1.0999 GLoss: 0.9835 Sum: 2.0834\n",
      "Time:  0.06935715675354004\n",
      "Step 229/100000 MLoss: 1.1534 GLoss: 1.0048 Sum: 2.1582\n",
      "Time:  0.0730278491973877\n",
      "Step 230/100000 MLoss: 1.1231 GLoss: 1.0048 Sum: 2.1279\n",
      "Time:  0.06867861747741699\n",
      "Step 231/100000 MLoss: 1.2012 GLoss: 0.9813 Sum: 2.1825\n",
      "Time:  0.072845458984375\n",
      "Step 232/100000 MLoss: 1.1195 GLoss: 0.9802 Sum: 2.0997\n",
      "Time:  0.0687406063079834\n",
      "Step 233/100000 MLoss: 1.1616 GLoss: 1.0069 Sum: 2.1685\n",
      "Time:  0.07553601264953613\n",
      "Step 234/100000 MLoss: 1.1463 GLoss: 0.9737 Sum: 2.12\n",
      "Time:  0.0713648796081543\n",
      "Step 235/100000 MLoss: 1.1122 GLoss: 1.0067 Sum: 2.1189\n",
      "Time:  0.07300066947937012\n",
      "Step 236/100000 MLoss: 1.1048 GLoss: 0.9875 Sum: 2.0923\n",
      "Time:  0.0718381404876709\n",
      "Step 237/100000 MLoss: 1.1246 GLoss: 0.9892 Sum: 2.1138\n",
      "Time:  0.0707850456237793\n",
      "Step 238/100000 MLoss: 1.09 GLoss: 0.9978 Sum: 2.0878\n",
      "Time:  0.07185530662536621\n",
      "Step 239/100000 MLoss: 1.1367 GLoss: 0.9858 Sum: 2.1225\n",
      "Time:  0.07083868980407715\n",
      "Step 240/100000 MLoss: 1.1233 GLoss: 1.0192 Sum: 2.1425\n",
      "Time:  0.07364988327026367\n",
      "Step 241/100000 MLoss: 1.1237 GLoss: 1.002 Sum: 2.1257\n",
      "Time:  0.07327961921691895\n",
      "Step 242/100000 MLoss: 1.0948 GLoss: 0.9952 Sum: 2.09\n",
      "Time:  0.0725553035736084\n",
      "Step 243/100000 MLoss: 1.1016 GLoss: 0.9815 Sum: 2.0831\n",
      "Time:  0.07184362411499023\n",
      "Step 244/100000 MLoss: 1.112 GLoss: 0.9856 Sum: 2.0976\n",
      "Time:  0.070953369140625\n",
      "Step 245/100000 MLoss: 1.1203 GLoss: 0.9958 Sum: 2.1161000000000003\n",
      "Time:  0.07160639762878418\n",
      "Step 246/100000 MLoss: 1.0943 GLoss: 0.9959 Sum: 2.0902000000000003\n",
      "Time:  0.07162332534790039\n",
      "Step 247/100000 MLoss: 1.1241 GLoss: 0.9993 Sum: 2.1234\n",
      "Time:  0.07134747505187988\n",
      "Step 248/100000 MLoss: 1.1347 GLoss: 1.0087 Sum: 2.1433999999999997\n",
      "Time:  0.0695037841796875\n",
      "Step 249/100000 MLoss: 1.1148 GLoss: 0.9977 Sum: 2.1125\n",
      "Time:  0.07343482971191406\n",
      "Step 250/100000 MLoss: 1.1332 GLoss: 0.9963 Sum: 2.1295\n",
      "Time:  0.07010102272033691\n",
      "Step 251/100000 MLoss: 1.1829 GLoss: 0.9908 Sum: 2.1737\n",
      "Time:  0.07193970680236816\n",
      "Step 252/100000 MLoss: 1.0992 GLoss: 1.0091 Sum: 2.1083\n",
      "Time:  0.07234001159667969\n",
      "Step 253/100000 MLoss: 1.1127 GLoss: 0.9926 Sum: 2.1053\n",
      "Time:  0.07251977920532227\n",
      "Step 254/100000 MLoss: 1.1519 GLoss: 1.0201 Sum: 2.1719999999999997\n",
      "Time:  0.06938576698303223\n",
      "Step 255/100000 MLoss: 1.1144 GLoss: 0.9973 Sum: 2.1117\n",
      "Time:  0.07062649726867676\n",
      "Step 256/100000 MLoss: 1.1593 GLoss: 0.9898 Sum: 2.1491\n",
      "Time:  0.07226848602294922\n",
      "Step 257/100000 MLoss: 1.1598 GLoss: 0.9955 Sum: 2.1553\n",
      "Time:  0.07544279098510742\n",
      "Step 258/100000 MLoss: 1.1172 GLoss: 1.0014 Sum: 2.1186\n",
      "Time:  0.07080602645874023\n",
      "Step 259/100000 MLoss: 1.0999 GLoss: 1.0057 Sum: 2.1056\n",
      "Time:  0.07433462142944336\n",
      "Step 260/100000 MLoss: 1.0847 GLoss: 1.0048 Sum: 2.0895\n",
      "Time:  0.0717012882232666\n",
      "Step 261/100000 MLoss: 1.1062 GLoss: 1.0007 Sum: 2.1069\n",
      "Time:  0.07095742225646973\n",
      "Step 262/100000 MLoss: 1.0945 GLoss: 1.0024 Sum: 2.0968999999999998\n",
      "Time:  0.07135343551635742\n",
      "Step 263/100000 MLoss: 1.1452 GLoss: 0.991 Sum: 2.1362\n",
      "Time:  0.07001876831054688\n",
      "Step 264/100000 MLoss: 1.1088 GLoss: 1.0052 Sum: 2.114\n",
      "Time:  0.0711979866027832\n",
      "Step 265/100000 MLoss: 1.0553 GLoss: 1.0098 Sum: 2.0651\n",
      "Time:  0.07263660430908203\n",
      "Step 266/100000 MLoss: 1.1218 GLoss: 1.013 Sum: 2.1348\n",
      "Time:  0.0729990005493164\n",
      "Step 267/100000 MLoss: 1.1382 GLoss: 0.9968 Sum: 2.1350000000000002\n",
      "Time:  0.06957769393920898\n",
      "Step 268/100000 MLoss: 1.1364 GLoss: 1.0127 Sum: 2.1491\n",
      "Time:  0.07613849639892578\n",
      "Step 269/100000 MLoss: 1.1253 GLoss: 0.9877 Sum: 2.113\n",
      "Time:  0.0759131908416748\n",
      "Step 270/100000 MLoss: 1.1233 GLoss: 0.9844 Sum: 2.1077\n",
      "Time:  0.0696101188659668\n",
      "Step 271/100000 MLoss: 1.1159 GLoss: 0.9961 Sum: 2.112\n",
      "Time:  0.07274770736694336\n",
      "Step 272/100000 MLoss: 1.1001 GLoss: 0.9969 Sum: 2.097\n",
      "Time:  0.06856918334960938\n",
      "Step 273/100000 MLoss: 1.1376 GLoss: 1.0036 Sum: 2.1412\n",
      "Time:  0.07486152648925781\n",
      "Step 274/100000 MLoss: 1.1018 GLoss: 1.0003 Sum: 2.1021\n",
      "Time:  0.06895899772644043\n",
      "Step 275/100000 MLoss: 1.0739 GLoss: 1.0176 Sum: 2.0915\n",
      "Time:  0.07382416725158691\n",
      "Step 276/100000 MLoss: 1.0581 GLoss: 0.9929 Sum: 2.051\n",
      "Time:  0.14664149284362793\n",
      "Step 277/100000 MLoss: 1.0743 GLoss: 0.9913 Sum: 2.0656\n",
      "Time:  0.06632614135742188\n",
      "Step 278/100000 MLoss: 1.1274 GLoss: 1.0101 Sum: 2.1375\n",
      "Time:  0.07325553894042969\n",
      "Step 279/100000 MLoss: 1.1041 GLoss: 1.0028 Sum: 2.1069\n",
      "Time:  0.07170438766479492\n",
      "Step 280/100000 MLoss: 1.1066 GLoss: 1.0091 Sum: 2.1157000000000004\n",
      "Time:  0.0697174072265625\n",
      "Step 281/100000 MLoss: 1.1161 GLoss: 0.9872 Sum: 2.1033\n",
      "Time:  0.0726461410522461\n",
      "Step 282/100000 MLoss: 1.0764 GLoss: 1.0066 Sum: 2.083\n",
      "Time:  0.07031035423278809\n",
      "Step 283/100000 MLoss: 1.0865 GLoss: 0.9923 Sum: 2.0788\n",
      "Time:  0.07143497467041016\n",
      "Step 284/100000 MLoss: 1.0953 GLoss: 0.9932 Sum: 2.0885\n",
      "Time:  0.07155179977416992\n",
      "Step 285/100000 MLoss: 1.1426 GLoss: 1.0026 Sum: 2.1452\n",
      "Time:  0.07143831253051758\n",
      "Step 286/100000 MLoss: 1.097 GLoss: 1.0007 Sum: 2.0976999999999997\n",
      "Time:  0.06979107856750488\n",
      "Step 287/100000 MLoss: 1.1309 GLoss: 1.0003 Sum: 2.1311999999999998\n",
      "Time:  0.07224345207214355\n",
      "Step 288/100000 MLoss: 1.107 GLoss: 1.0112 Sum: 2.1182\n",
      "Time:  0.06911826133728027\n",
      "Step 289/100000 MLoss: 1.127 GLoss: 1.006 Sum: 2.133\n",
      "Time:  0.07707071304321289\n",
      "Step 290/100000 MLoss: 1.1302 GLoss: 1.0057 Sum: 2.1359000000000004\n",
      "Time:  0.07166242599487305\n",
      "Step 291/100000 MLoss: 1.1613 GLoss: 0.9959 Sum: 2.1572\n",
      "Time:  0.07390952110290527\n",
      "Step 292/100000 MLoss: 1.0674 GLoss: 1.0105 Sum: 2.0778999999999996\n",
      "Time:  0.07138228416442871\n",
      "Step 293/100000 MLoss: 1.0578 GLoss: 0.9886 Sum: 2.0464\n",
      "Time:  0.15006709098815918\n",
      "Step 294/100000 MLoss: 1.0527 GLoss: 1.0072 Sum: 2.0599\n",
      "Time:  0.0713355541229248\n",
      "Step 295/100000 MLoss: 1.1198 GLoss: 0.9905 Sum: 2.1103\n",
      "Time:  0.07388663291931152\n",
      "Step 296/100000 MLoss: 1.1354 GLoss: 0.994 Sum: 2.1294\n",
      "Time:  0.07646751403808594\n",
      "Step 297/100000 MLoss: 1.1413 GLoss: 1.0016 Sum: 2.1429\n",
      "Time:  0.07751893997192383\n",
      "Step 298/100000 MLoss: 1.1243 GLoss: 0.9945 Sum: 2.1188000000000002\n",
      "Time:  0.07534289360046387\n",
      "Step 299/100000 MLoss: 1.1007 GLoss: 1.0048 Sum: 2.1055\n",
      "Time:  0.07619094848632812\n",
      "Step 300/100000 MLoss: 1.117 GLoss: 1.0127 Sum: 2.1296999999999997\n",
      "Time:  0.07239317893981934\n",
      "Step 301/100000 MLoss: 1.0719 GLoss: 1.004 Sum: 2.0759\n",
      "Time:  0.07033443450927734\n",
      "Step 302/100000 MLoss: 1.0599 GLoss: 1.002 Sum: 2.0619\n",
      "Time:  0.07338547706604004\n",
      "Step 303/100000 MLoss: 1.1013 GLoss: 1.0072 Sum: 2.1085000000000003\n",
      "Time:  0.07108068466186523\n",
      "Step 304/100000 MLoss: 1.0853 GLoss: 1.0072 Sum: 2.0925000000000002\n",
      "Time:  0.0724630355834961\n",
      "Step 305/100000 MLoss: 1.1559 GLoss: 1.0015 Sum: 2.1574\n",
      "Time:  0.07403826713562012\n",
      "Step 306/100000 MLoss: 1.0244 GLoss: 1.0134 Sum: 2.0378\n",
      "Time:  0.14717984199523926\n",
      "Step 307/100000 MLoss: 1.074 GLoss: 1.0001 Sum: 2.0741\n",
      "Time:  0.07049870491027832\n",
      "Step 308/100000 MLoss: 1.1721 GLoss: 1.01 Sum: 2.1821\n",
      "Time:  0.07768630981445312\n",
      "Step 309/100000 MLoss: 1.086 GLoss: 0.9883 Sum: 2.0743\n",
      "Time:  0.07244324684143066\n",
      "Step 310/100000 MLoss: 1.076 GLoss: 0.9885 Sum: 2.0645000000000002\n",
      "Time:  0.07497048377990723\n",
      "Step 311/100000 MLoss: 1.1616 GLoss: 0.995 Sum: 2.1566\n",
      "Time:  0.07665061950683594\n",
      "Step 312/100000 MLoss: 1.0772 GLoss: 1.0078 Sum: 2.085\n",
      "Time:  0.07619524002075195\n",
      "Step 313/100000 MLoss: 1.1346 GLoss: 0.9974 Sum: 2.132\n",
      "Time:  0.07762837409973145\n",
      "Step 314/100000 MLoss: 1.0839 GLoss: 0.9965 Sum: 2.0804\n",
      "Time:  0.07358193397521973\n",
      "Step 315/100000 MLoss: 1.1467 GLoss: 1.0061 Sum: 2.1528\n",
      "Time:  0.07587265968322754\n",
      "Step 316/100000 MLoss: 1.094 GLoss: 1.0071 Sum: 2.1011\n",
      "Time:  0.07615160942077637\n",
      "Step 317/100000 MLoss: 1.1027 GLoss: 0.9993 Sum: 2.102\n",
      "Time:  0.07551026344299316\n",
      "Step 318/100000 MLoss: 1.0908 GLoss: 0.9866 Sum: 2.0774\n",
      "Time:  0.07662296295166016\n",
      "Step 319/100000 MLoss: 1.1631 GLoss: 0.9941 Sum: 2.1572\n",
      "Time:  0.07787895202636719\n",
      "Step 320/100000 MLoss: 1.2025 GLoss: 0.9927 Sum: 2.1952\n",
      "Time:  0.07528829574584961\n",
      "Step 321/100000 MLoss: 1.1542 GLoss: 1.0087 Sum: 2.1628999999999996\n",
      "Time:  0.07334756851196289\n",
      "Step 322/100000 MLoss: 1.076 GLoss: 1.0209 Sum: 2.0968999999999998\n",
      "Time:  0.07026815414428711\n",
      "Step 323/100000 MLoss: 1.1558 GLoss: 1.0017 Sum: 2.1574999999999998\n",
      "Time:  0.07157182693481445\n",
      "Step 324/100000 MLoss: 1.0956 GLoss: 1.0045 Sum: 2.1001\n",
      "Time:  0.07058382034301758\n",
      "Step 325/100000 MLoss: 1.1727 GLoss: 0.9948 Sum: 2.1675\n",
      "Time:  0.07289457321166992\n",
      "Step 326/100000 MLoss: 1.1183 GLoss: 0.9955 Sum: 2.1138000000000003\n",
      "Time:  0.07069754600524902\n",
      "Step 327/100000 MLoss: 1.0865 GLoss: 0.9851 Sum: 2.0716\n",
      "Time:  0.07353806495666504\n",
      "Step 328/100000 MLoss: 1.1261 GLoss: 1.0145 Sum: 2.1406\n",
      "Time:  0.07011795043945312\n",
      "Step 329/100000 MLoss: 1.138 GLoss: 1.0012 Sum: 2.1391999999999998\n",
      "Time:  0.0751793384552002\n",
      "Step 330/100000 MLoss: 1.139 GLoss: 0.9874 Sum: 2.1264000000000003\n",
      "Time:  0.07358694076538086\n",
      "Step 331/100000 MLoss: 1.0723 GLoss: 0.9874 Sum: 2.0597000000000003\n",
      "Time:  0.0709381103515625\n",
      "Step 332/100000 MLoss: 1.0741 GLoss: 1.0037 Sum: 2.0778\n",
      "Time:  0.0724642276763916\n",
      "Step 333/100000 MLoss: 1.1338 GLoss: 1.0117 Sum: 2.1455\n",
      "Time:  0.07074189186096191\n",
      "Step 334/100000 MLoss: 1.1649 GLoss: 1.0095 Sum: 2.1744000000000003\n",
      "Time:  0.07289886474609375\n",
      "Step 335/100000 MLoss: 1.1004 GLoss: 1.0001 Sum: 2.1005000000000003\n",
      "Time:  0.06902289390563965\n",
      "Step 336/100000 MLoss: 1.1046 GLoss: 0.9925 Sum: 2.0971\n",
      "Time:  0.07259011268615723\n",
      "Step 337/100000 MLoss: 1.1086 GLoss: 0.9928 Sum: 2.1014\n",
      "Time:  0.07273292541503906\n",
      "Step 338/100000 MLoss: 1.0446 GLoss: 0.9962 Sum: 2.0408\n",
      "Time:  0.07324457168579102\n",
      "Step 339/100000 MLoss: 1.1115 GLoss: 1.0028 Sum: 2.1143\n",
      "Time:  0.07230234146118164\n",
      "Step 340/100000 MLoss: 1.2173 GLoss: 1.0035 Sum: 2.2208\n",
      "Time:  0.07112526893615723\n",
      "Step 341/100000 MLoss: 1.0377 GLoss: 0.9944 Sum: 2.0321\n",
      "Time:  0.14348578453063965\n",
      "Step 342/100000 MLoss: 1.1422 GLoss: 0.9924 Sum: 2.1346\n",
      "Time:  0.06543517112731934\n",
      "Step 343/100000 MLoss: 1.0356 GLoss: 1.0009 Sum: 2.0365\n",
      "Time:  0.0729374885559082\n",
      "Step 344/100000 MLoss: 1.0809 GLoss: 1.0112 Sum: 2.0921000000000003\n",
      "Time:  0.0728311538696289\n",
      "Step 345/100000 MLoss: 1.0747 GLoss: 0.984 Sum: 2.0587\n",
      "Time:  0.0719759464263916\n",
      "Step 346/100000 MLoss: 1.0973 GLoss: 0.9995 Sum: 2.0968\n",
      "Time:  0.07297945022583008\n",
      "Step 347/100000 MLoss: 1.0701 GLoss: 0.9868 Sum: 2.0569\n",
      "Time:  0.0710606575012207\n",
      "Step 348/100000 MLoss: 1.0744 GLoss: 0.994 Sum: 2.0684\n",
      "Time:  0.07355999946594238\n",
      "Step 349/100000 MLoss: 1.0972 GLoss: 1.0121 Sum: 2.1093\n",
      "Time:  0.07158470153808594\n",
      "Step 350/100000 MLoss: 1.112 GLoss: 0.9979 Sum: 2.1099\n",
      "Time:  0.07216835021972656\n",
      "Step 351/100000 MLoss: 1.1013 GLoss: 1.0074 Sum: 2.1087\n",
      "Time:  0.07170844078063965\n",
      "Step 352/100000 MLoss: 1.074 GLoss: 0.999 Sum: 2.073\n",
      "Time:  0.06920170783996582\n",
      "Step 353/100000 MLoss: 1.1959 GLoss: 0.9999 Sum: 2.1958\n",
      "Time:  0.07246065139770508\n",
      "Step 354/100000 MLoss: 1.1054 GLoss: 1.0218 Sum: 2.1272\n",
      "Time:  0.08056950569152832\n",
      "Step 355/100000 MLoss: 1.1015 GLoss: 0.995 Sum: 2.0965\n",
      "Time:  0.07587957382202148\n",
      "Step 356/100000 MLoss: 1.0868 GLoss: 1.0012 Sum: 2.088\n",
      "Time:  0.07661581039428711\n",
      "Step 357/100000 MLoss: 1.0668 GLoss: 0.9929 Sum: 2.0597\n",
      "Time:  0.07751822471618652\n",
      "Step 358/100000 MLoss: 1.0787 GLoss: 0.9939 Sum: 2.0726\n",
      "Time:  0.0751953125\n",
      "Step 359/100000 MLoss: 1.1453 GLoss: 0.9878 Sum: 2.1330999999999998\n",
      "Time:  0.07602548599243164\n",
      "Step 360/100000 MLoss: 1.0851 GLoss: 1.0322 Sum: 2.1173\n",
      "Time:  0.07633829116821289\n",
      "Step 361/100000 MLoss: 1.0755 GLoss: 1.0085 Sum: 2.0839999999999996\n",
      "Time:  0.07702851295471191\n",
      "Step 362/100000 MLoss: 1.1096 GLoss: 0.9987 Sum: 2.1083\n",
      "Time:  0.07646346092224121\n",
      "Step 363/100000 MLoss: 1.0851 GLoss: 0.9959 Sum: 2.081\n",
      "Time:  0.0733480453491211\n",
      "Step 364/100000 MLoss: 1.0994 GLoss: 1.0042 Sum: 2.1036\n",
      "Time:  0.07410216331481934\n",
      "Step 365/100000 MLoss: 1.0876 GLoss: 0.9945 Sum: 2.0821\n",
      "Time:  0.07271385192871094\n",
      "Step 366/100000 MLoss: 1.1004 GLoss: 1.0 Sum: 2.1004\n",
      "Time:  0.07152295112609863\n",
      "Step 367/100000 MLoss: 1.1203 GLoss: 0.983 Sum: 2.1033\n",
      "Time:  0.07322096824645996\n",
      "Step 368/100000 MLoss: 1.0923 GLoss: 0.989 Sum: 2.0813\n",
      "Time:  0.06879568099975586\n",
      "Step 369/100000 MLoss: 1.1693 GLoss: 0.993 Sum: 2.1623\n",
      "Time:  0.07322502136230469\n",
      "Step 370/100000 MLoss: 1.1248 GLoss: 1.0039 Sum: 2.1287000000000003\n",
      "Time:  0.06922674179077148\n",
      "Step 371/100000 MLoss: 1.1642 GLoss: 1.0056 Sum: 2.1698\n",
      "Time:  0.07334637641906738\n",
      "Step 372/100000 MLoss: 1.0911 GLoss: 0.9914 Sum: 2.0825\n",
      "Time:  0.07006263732910156\n",
      "Step 373/100000 MLoss: 1.136 GLoss: 0.9962 Sum: 2.1322\n",
      "Time:  0.07261466979980469\n",
      "Step 374/100000 MLoss: 1.0914 GLoss: 1.0172 Sum: 2.1086\n",
      "Time:  0.07070040702819824\n",
      "Step 375/100000 MLoss: 1.0999 GLoss: 0.9972 Sum: 2.0971\n",
      "Time:  0.07482337951660156\n",
      "Step 376/100000 MLoss: 1.0745 GLoss: 0.995 Sum: 2.0695\n",
      "Time:  0.07217168807983398\n",
      "Step 377/100000 MLoss: 1.1193 GLoss: 0.9866 Sum: 2.1059\n",
      "Time:  0.07283616065979004\n",
      "Step 378/100000 MLoss: 1.1414 GLoss: 0.9974 Sum: 2.1388\n",
      "Time:  0.07315683364868164\n",
      "Step 379/100000 MLoss: 1.1552 GLoss: 1.0038 Sum: 2.159\n",
      "Time:  0.06998205184936523\n",
      "Step 380/100000 MLoss: 1.1985 GLoss: 0.9928 Sum: 2.1913\n",
      "Time:  0.07391691207885742\n",
      "Step 381/100000 MLoss: 1.1644 GLoss: 0.9894 Sum: 2.1538\n",
      "Time:  0.07210469245910645\n",
      "Step 382/100000 MLoss: 1.0506 GLoss: 1.0052 Sum: 2.0558\n",
      "Time:  0.07256460189819336\n",
      "Step 383/100000 MLoss: 1.1564 GLoss: 0.9916 Sum: 2.148\n",
      "Time:  0.07288026809692383\n",
      "Step 384/100000 MLoss: 1.2026 GLoss: 1.0025 Sum: 2.2051\n",
      "Time:  0.06904268264770508\n",
      "Step 385/100000 MLoss: 1.0901 GLoss: 0.994 Sum: 2.0841000000000003\n",
      "Time:  0.07460284233093262\n",
      "Step 386/100000 MLoss: 1.0631 GLoss: 0.9959 Sum: 2.059\n",
      "Time:  0.06962060928344727\n",
      "Step 387/100000 MLoss: 1.1244 GLoss: 1.0134 Sum: 2.1378000000000004\n",
      "Time:  0.07273101806640625\n",
      "Step 388/100000 MLoss: 1.111 GLoss: 1.0044 Sum: 2.1154\n",
      "Time:  0.06933307647705078\n",
      "Step 389/100000 MLoss: 1.0926 GLoss: 1.0031 Sum: 2.0957\n",
      "Time:  0.07486653327941895\n",
      "Step 390/100000 MLoss: 1.1192 GLoss: 1.0007 Sum: 2.1199\n",
      "Time:  0.07207012176513672\n",
      "Step 391/100000 MLoss: 1.112 GLoss: 0.9926 Sum: 2.1046\n",
      "Time:  0.0727996826171875\n",
      "Step 392/100000 MLoss: 1.0659 GLoss: 0.9873 Sum: 2.0532\n",
      "Time:  0.07177734375\n",
      "Step 393/100000 MLoss: 1.0835 GLoss: 0.9981 Sum: 2.0816\n",
      "Time:  0.07249665260314941\n",
      "Step 394/100000 MLoss: 1.1028 GLoss: 0.9886 Sum: 2.0914\n",
      "Time:  0.07227110862731934\n",
      "Step 395/100000 MLoss: 1.1134 GLoss: 0.9938 Sum: 2.1071999999999997\n",
      "Time:  0.06952285766601562\n",
      "Step 396/100000 MLoss: 1.0837 GLoss: 1.0085 Sum: 2.0922\n",
      "Time:  0.07306504249572754\n",
      "Step 397/100000 MLoss: 1.0715 GLoss: 0.9887 Sum: 2.0602\n",
      "Time:  0.06973099708557129\n",
      "Step 398/100000 MLoss: 1.1221 GLoss: 0.9954 Sum: 2.1175\n",
      "Time:  0.0745995044708252\n",
      "Step 399/100000 MLoss: 1.105 GLoss: 0.9854 Sum: 2.0904\n",
      "Time:  0.07226896286010742\n",
      "Step 400/100000 MLoss: 1.1628 GLoss: 0.9875 Sum: 2.1503\n",
      "Time:  0.07053661346435547\n",
      "Step 401/100000 MLoss: 1.0985 GLoss: 0.9999 Sum: 2.0984\n",
      "Time:  0.07390809059143066\n",
      "Step 402/100000 MLoss: 1.1825 GLoss: 0.995 Sum: 2.1775\n",
      "Time:  0.07052278518676758\n",
      "Step 403/100000 MLoss: 1.1192 GLoss: 0.9961 Sum: 2.1153\n",
      "Time:  0.07294082641601562\n",
      "Step 404/100000 MLoss: 1.0432 GLoss: 1.012 Sum: 2.0552\n",
      "Time:  0.0701301097869873\n",
      "Step 405/100000 MLoss: 1.0762 GLoss: 0.985 Sum: 2.0612\n",
      "Time:  0.07313990592956543\n",
      "Step 406/100000 MLoss: 1.1058 GLoss: 1.0022 Sum: 2.1079999999999997\n",
      "Time:  0.06987667083740234\n",
      "Step 407/100000 MLoss: 1.057 GLoss: 0.9949 Sum: 2.0519\n",
      "Time:  0.07496976852416992\n",
      "Step 408/100000 MLoss: 1.0733 GLoss: 1.014 Sum: 2.0873\n",
      "Time:  0.07230830192565918\n",
      "Step 409/100000 MLoss: 1.1053 GLoss: 0.981 Sum: 2.0863\n",
      "Time:  0.07189416885375977\n",
      "Step 410/100000 MLoss: 1.0667 GLoss: 1.003 Sum: 2.0697\n",
      "Time:  0.0715947151184082\n",
      "Step 411/100000 MLoss: 1.0793 GLoss: 0.988 Sum: 2.0673\n",
      "Time:  0.07158184051513672\n",
      "Step 412/100000 MLoss: 1.0553 GLoss: 0.985 Sum: 2.0403\n",
      "Time:  0.07299065589904785\n",
      "Step 413/100000 MLoss: 1.1833 GLoss: 0.9953 Sum: 2.1786\n",
      "Time:  0.07086014747619629\n",
      "Step 414/100000 MLoss: 1.0921 GLoss: 1.0085 Sum: 2.1006\n",
      "Time:  0.07391905784606934\n",
      "Step 415/100000 MLoss: 1.1052 GLoss: 0.9908 Sum: 2.096\n",
      "Time:  0.07259988784790039\n",
      "Step 416/100000 MLoss: 1.1241 GLoss: 0.9761 Sum: 2.1002\n",
      "Time:  0.0712747573852539\n",
      "Step 417/100000 MLoss: 1.1478 GLoss: 1.0016 Sum: 2.1494\n",
      "Time:  0.0733952522277832\n",
      "Step 418/100000 MLoss: 1.0799 GLoss: 0.9804 Sum: 2.0603000000000002\n",
      "Time:  0.07157540321350098\n",
      "Step 419/100000 MLoss: 1.0988 GLoss: 0.9933 Sum: 2.0921\n",
      "Time:  0.07296490669250488\n",
      "Step 420/100000 MLoss: 1.0761 GLoss: 0.9994 Sum: 2.0755\n",
      "Time:  0.06959843635559082\n",
      "Step 421/100000 MLoss: 1.1194 GLoss: 0.9927 Sum: 2.1121\n",
      "Time:  0.0735161304473877\n",
      "Step 422/100000 MLoss: 1.0945 GLoss: 0.9984 Sum: 2.0929\n",
      "Time:  0.07127523422241211\n",
      "Step 423/100000 MLoss: 1.0903 GLoss: 0.9958 Sum: 2.0861\n",
      "Time:  0.07362842559814453\n",
      "Step 424/100000 MLoss: 1.0748 GLoss: 0.9841 Sum: 2.0589\n",
      "Time:  0.07266902923583984\n",
      "Step 425/100000 MLoss: 1.1058 GLoss: 0.9833 Sum: 2.0890999999999997\n",
      "Time:  0.07355308532714844\n",
      "Step 426/100000 MLoss: 1.1049 GLoss: 0.9983 Sum: 2.1032\n",
      "Time:  0.07322978973388672\n",
      "Step 427/100000 MLoss: 1.0822 GLoss: 0.9968 Sum: 2.079\n",
      "Time:  0.07032918930053711\n",
      "Step 428/100000 MLoss: 1.1044 GLoss: 0.9879 Sum: 2.0923\n",
      "Time:  0.07337188720703125\n",
      "Step 429/100000 MLoss: 1.1368 GLoss: 0.9696 Sum: 2.1064\n",
      "Time:  0.07261347770690918\n",
      "Step 430/100000 MLoss: 1.1317 GLoss: 0.9906 Sum: 2.1223\n",
      "Time:  0.07076406478881836\n",
      "Step 431/100000 MLoss: 1.1255 GLoss: 0.982 Sum: 2.1075\n",
      "Time:  0.07312774658203125\n",
      "Step 432/100000 MLoss: 1.1009 GLoss: 0.9879 Sum: 2.0888\n",
      "Time:  0.0724036693572998\n",
      "Step 433/100000 MLoss: 1.1108 GLoss: 0.9999 Sum: 2.1107\n",
      "Time:  0.0743875503540039\n",
      "Step 434/100000 MLoss: 1.0699 GLoss: 0.9952 Sum: 2.0651\n",
      "Time:  0.0694265365600586\n",
      "Step 435/100000 MLoss: 1.095 GLoss: 0.9832 Sum: 2.0782\n",
      "Time:  0.07295370101928711\n",
      "Step 436/100000 MLoss: 1.1365 GLoss: 0.9836 Sum: 2.1201\n",
      "Time:  0.06968832015991211\n",
      "Step 437/100000 MLoss: 1.1383 GLoss: 1.0057 Sum: 2.144\n",
      "Time:  0.07430362701416016\n",
      "Step 438/100000 MLoss: 1.0713 GLoss: 0.9865 Sum: 2.0578\n",
      "Time:  0.07596707344055176\n",
      "Step 439/100000 MLoss: 1.1367 GLoss: 0.9853 Sum: 2.122\n",
      "Time:  0.07612037658691406\n",
      "Step 440/100000 MLoss: 1.0787 GLoss: 0.9914 Sum: 2.0701\n",
      "Time:  0.07640790939331055\n",
      "Step 441/100000 MLoss: 1.1275 GLoss: 0.9964 Sum: 2.1239\n",
      "Time:  0.07831716537475586\n",
      "Step 442/100000 MLoss: 1.1934 GLoss: 0.9946 Sum: 2.188\n",
      "Time:  0.07646870613098145\n",
      "Step 443/100000 MLoss: 1.1041 GLoss: 0.9921 Sum: 2.0962\n",
      "Time:  0.07517242431640625\n",
      "Step 444/100000 MLoss: 1.0748 GLoss: 0.9846 Sum: 2.0594\n",
      "Time:  0.07654333114624023\n",
      "Step 445/100000 MLoss: 1.0855 GLoss: 1.0047 Sum: 2.0902\n",
      "Time:  0.07726454734802246\n",
      "Step 446/100000 MLoss: 1.1241 GLoss: 0.9909 Sum: 2.115\n",
      "Time:  0.07644462585449219\n",
      "Step 447/100000 MLoss: 1.0396 GLoss: 0.9855 Sum: 2.0251\n",
      "Time:  0.1465764045715332\n",
      "Step 448/100000 MLoss: 1.0644 GLoss: 0.9933 Sum: 2.0577\n",
      "Time:  0.07049679756164551\n",
      "Step 449/100000 MLoss: 1.0807 GLoss: 0.9928 Sum: 2.0735\n",
      "Time:  0.07684826850891113\n",
      "Step 450/100000 MLoss: 1.0569 GLoss: 0.997 Sum: 2.0539\n",
      "Time:  0.07335972785949707\n",
      "Step 451/100000 MLoss: 1.0695 GLoss: 0.9804 Sum: 2.0499\n",
      "Time:  0.07480764389038086\n",
      "Step 452/100000 MLoss: 1.1737 GLoss: 0.9929 Sum: 2.1666\n",
      "Time:  0.07016253471374512\n",
      "Step 453/100000 MLoss: 1.0898 GLoss: 0.9855 Sum: 2.0753000000000004\n",
      "Time:  0.0731511116027832\n",
      "Step 454/100000 MLoss: 1.0889 GLoss: 0.9883 Sum: 2.0772\n",
      "Time:  0.06967878341674805\n",
      "Step 455/100000 MLoss: 1.0546 GLoss: 0.9891 Sum: 2.0437\n",
      "Time:  0.07348990440368652\n",
      "Step 456/100000 MLoss: 1.1029 GLoss: 0.9811 Sum: 2.084\n",
      "Time:  0.0697164535522461\n",
      "Step 457/100000 MLoss: 1.0619 GLoss: 0.9881 Sum: 2.05\n",
      "Time:  0.07440876960754395\n",
      "Step 458/100000 MLoss: 1.0776 GLoss: 1.0003 Sum: 2.0778999999999996\n",
      "Time:  0.07306885719299316\n",
      "Step 459/100000 MLoss: 1.1268 GLoss: 1.0023 Sum: 2.1291\n",
      "Time:  0.07176041603088379\n",
      "Step 460/100000 MLoss: 1.1517 GLoss: 0.9903 Sum: 2.142\n",
      "Time:  0.07613229751586914\n",
      "Step 461/100000 MLoss: 1.0803 GLoss: 0.9927 Sum: 2.073\n",
      "Time:  0.07184433937072754\n",
      "Step 462/100000 MLoss: 1.0556 GLoss: 0.9894 Sum: 2.045\n",
      "Time:  0.07340621948242188\n",
      "Step 463/100000 MLoss: 1.0763 GLoss: 1.0016 Sum: 2.0779\n",
      "Time:  0.07237887382507324\n",
      "Step 464/100000 MLoss: 1.0748 GLoss: 0.9837 Sum: 2.0585\n",
      "Time:  0.06960129737854004\n",
      "Step 465/100000 MLoss: 1.0956 GLoss: 0.9676 Sum: 2.0632\n",
      "Time:  0.07372903823852539\n",
      "Step 466/100000 MLoss: 1.0978 GLoss: 0.9876 Sum: 2.0854\n",
      "Time:  0.07132792472839355\n",
      "Step 467/100000 MLoss: 1.1083 GLoss: 1.0 Sum: 2.1083\n",
      "Time:  0.07332420349121094\n",
      "Step 468/100000 MLoss: 1.0978 GLoss: 0.9839 Sum: 2.0817\n",
      "Time:  0.07042646408081055\n",
      "Step 469/100000 MLoss: 1.0228 GLoss: 0.977 Sum: 1.9998\n",
      "Time:  0.1479625701904297\n",
      "Step 470/100000 MLoss: 1.0759 GLoss: 0.9852 Sum: 2.0611\n",
      "Time:  0.06262683868408203\n",
      "Step 471/100000 MLoss: 1.0971 GLoss: 0.9859 Sum: 2.083\n",
      "Time:  0.072418212890625\n",
      "Step 472/100000 MLoss: 1.1195 GLoss: 0.9951 Sum: 2.1146\n",
      "Time:  0.07303690910339355\n",
      "Step 473/100000 MLoss: 1.067 GLoss: 1.0012 Sum: 2.0682\n",
      "Time:  0.07423925399780273\n",
      "Step 474/100000 MLoss: 1.1243 GLoss: 0.9863 Sum: 2.1106\n",
      "Time:  0.07202887535095215\n",
      "Step 475/100000 MLoss: 1.1192 GLoss: 0.9714 Sum: 2.0906000000000002\n",
      "Time:  0.07213377952575684\n",
      "Step 476/100000 MLoss: 1.1052 GLoss: 0.9742 Sum: 2.0793999999999997\n",
      "Time:  0.07118439674377441\n",
      "Step 477/100000 MLoss: 1.0815 GLoss: 0.962 Sum: 2.0435\n",
      "Time:  0.07439017295837402\n",
      "Step 478/100000 MLoss: 1.1192 GLoss: 0.9702 Sum: 2.0894\n",
      "Time:  0.07111454010009766\n",
      "Step 479/100000 MLoss: 1.0872 GLoss: 0.9901 Sum: 2.0773\n",
      "Time:  0.07360696792602539\n",
      "Step 480/100000 MLoss: 1.0741 GLoss: 0.9756 Sum: 2.0497\n",
      "Time:  0.07265138626098633\n",
      "Step 481/100000 MLoss: 1.09 GLoss: 0.9814 Sum: 2.0714\n",
      "Time:  0.0724174976348877\n",
      "Step 482/100000 MLoss: 1.0594 GLoss: 1.0051 Sum: 2.0645\n",
      "Time:  0.07932400703430176\n",
      "Step 483/100000 MLoss: 1.075 GLoss: 0.9663 Sum: 2.0413\n",
      "Time:  0.07288718223571777\n",
      "Step 484/100000 MLoss: 1.0465 GLoss: 1.0043 Sum: 2.0507999999999997\n",
      "Time:  0.07058477401733398\n",
      "Step 485/100000 MLoss: 1.0912 GLoss: 0.9847 Sum: 2.0759\n",
      "Time:  0.07364988327026367\n",
      "Step 486/100000 MLoss: 1.082 GLoss: 0.9997 Sum: 2.0817\n",
      "Time:  0.07082200050354004\n",
      "Step 487/100000 MLoss: 1.0709 GLoss: 0.9819 Sum: 2.0528\n",
      "Time:  0.07319903373718262\n",
      "Step 488/100000 MLoss: 1.0411 GLoss: 0.977 Sum: 2.0181\n",
      "Time:  0.07101106643676758\n",
      "Step 489/100000 MLoss: 1.0802 GLoss: 0.9745 Sum: 2.0547\n",
      "Time:  0.07549858093261719\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtabddpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mraw_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMODEL_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATA_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/yaspar/Documents/GitHub/diffusion_model_bootcamp/reference_implementations/tabular_reference_impelementation/single_table/src/baselines/tabddpm/pipeline.py:97\u001b[0m, in \u001b[0;36mTabDDPM.train\u001b[0;34m(self, model_save_path, steps, lr, weight_decay, batch_size)\u001b[0m\n\u001b[1;32m     84\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mprepare_fast_dataloader(\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     88\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion,\n\u001b[1;32m     90\u001b[0m     train_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m     96\u001b[0m )\n\u001b[0;32m---> 97\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_save_path):\n\u001b[1;32m    100\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(model_save_path)\n",
      "File \u001b[0;32m/fs01/home/yaspar/Documents/GitHub/diffusion_model_bootcamp/reference_implementations/tabular_reference_impelementation/single_table/src/baselines/tabddpm/train.py:76\u001b[0m, in \u001b[0;36mTrainer.run_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     74\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_iter)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 76\u001b[0m batch_loss_multi, batch_loss_gauss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_anneal_lr(step)\n\u001b[1;32m     80\u001b[0m curr_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x)\n",
      "File \u001b[0;32m/fs01/home/yaspar/Documents/GitHub/diffusion_model_bootcamp/reference_implementations/tabular_reference_impelementation/single_table/src/baselines/tabddpm/train.py:53\u001b[0m, in \u001b[0;36mTrainer._run_step\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 53\u001b[0m loss_multi, loss_gauss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixed_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_multi \u001b[38;5;241m+\u001b[39m loss_gauss\n\u001b[1;32m     56\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/fs01/home/yaspar/Documents/GitHub/diffusion_model_bootcamp/reference_implementations/tabular_reference_impelementation/single_table/src/baselines/tabddpm/model/gaussian_multinomial_diffusion.py:746\u001b[0m, in \u001b[0;36mGaussianMultinomialDiffusion.mixed_loss\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    744\u001b[0m     x_num_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgaussian_q_sample(x_num, t, noise\u001b[38;5;241m=\u001b[39mnoise)\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_cat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 746\u001b[0m     log_x_cat \u001b[38;5;241m=\u001b[39m \u001b[43mindex_to_log_onehot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_cat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m     log_x_cat_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_sample(log_x_start\u001b[38;5;241m=\u001b[39mlog_x_cat, t\u001b[38;5;241m=\u001b[39mt)\n\u001b[1;32m    749\u001b[0m x_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x_num_t, log_x_cat_t], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/fs01/home/yaspar/Documents/GitHub/diffusion_model_bootcamp/reference_implementations/tabular_reference_impelementation/single_table/src/baselines/tabddpm/model/utils.py:144\u001b[0m, in \u001b[0;36mindex_to_log_onehot\u001b[0;34m(x, num_classes)\u001b[0m\n\u001b[1;32m    142\u001b[0m onehots \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(num_classes)):\n\u001b[0;32m--> 144\u001b[0m     onehots\u001b[38;5;241m.\u001b[39mappend(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    146\u001b[0m x_onehot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(onehots, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    147\u001b[0m log_onehot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(x_onehot\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-30\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tabddpm.train(\n",
    "    **raw_config[\"train\"][\"main\"],\n",
    "    model_save_path=f\"{MODEL_PATH}/{DATA_NAME}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0624de8",
   "metadata": {},
   "source": [
    "## Load pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9def4df",
   "metadata": {},
   "source": [
    "Instead of training model from scratch, we can also load weights of pre-trained model from given checkpoint with `load_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29eb097c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from /projects/aieng/diffusion_bootcamp/models/tabular/tabddpm/adult/model_100000.pt\n"
     ]
    }
   ],
   "source": [
    "model_name= \"model_100000.pt\"\n",
    "\n",
    "tabddpm.load_model(\n",
    "    ckpt_path=f\"{MODEL_PATH}/{DATA_NAME}/{model_name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-smile",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6801c3",
   "metadata": {},
   "source": [
    "Now that we trained the model effectively, using `sample` function we can generate synthetic data starting from compelete noise. The input of this function is as follows:\n",
    "\n",
    "1. info_path: The path to the JSON file containing the information about the dataset.\n",
    "2. num_samples: The number of samples to generate.\n",
    "3. batch_size: The batch size for sampling the data.\n",
    "4. sample_save_path: The path to save the generated samples.\n",
    "5. ddim: Whether the sampling strategy is DDPM (Denoising Diffusion Probabilistic Models) and DDIM (Denoising Diffusion Implicit Models). DDPM produces high-quality samples but requires many computationally expensive sampling steps, making it slower. In contrast, DDIM (Denoising Diffusion Implicit Models) offers faster sampling with fewer steps by using a deterministic process, maintaining similar sample quality with improved efficiency. The choice between them depends on the need for speed versus the highest possible sample quality.\n",
    "6. steps: The number of steps for sampling the data. This is the number of diffusion steps to take when sampling the data. The higher the number of steps, the better the quality of the generated samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "stainless-ferry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample using DDIM.\n",
      "Sample timestep  999\n",
      "Sample timestep  999\n",
      "Sample timestep  187\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_100000.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtabddpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPROCESSED_DATA_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATA_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/info.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_samples\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_save_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mSYNTH_DATA_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATA_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/tabddpm.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mddim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/yaspar/Documents/GitHub/diffusion_model_bootcamp/reference_implementations/tabular_reference_impelementation/single_table/src/baselines/tabddpm/pipeline.py:134\u001b[0m, in \u001b[0;36mTabDDPM.sample\u001b[0;34m(self, info_path, sample_save_path, batch_size, num_samples, ddim, steps)\u001b[0m\n\u001b[1;32m    132\u001b[0m     x_gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion\u001b[38;5;241m.\u001b[39msample_all(num_samples, batch_size, ddim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 134\u001b[0m     x_gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_all\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape\u001b[39m\u001b[38;5;124m\"\u001b[39m, x_gen\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    140\u001b[0m syn_data \u001b[38;5;241m=\u001b[39m x_gen\n",
      "File \u001b[0;32m/fs01/home/yaspar/Documents/GitHub/diffusion_model_bootcamp/reference_implementations/tabular_reference_impelementation/single_table/src/baselines/tabddpm/model/gaussian_multinomial_diffusion.py:1090\u001b[0m, in \u001b[0;36mGaussianMultinomialDiffusion.sample_all\u001b[0;34m(self, num_samples, batch_size, ddim, steps)\u001b[0m\n\u001b[1;32m   1088\u001b[0m     sample \u001b[38;5;241m=\u001b[39m sample_fn(b)\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1090\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43msample_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1091\u001b[0m mask_nan \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39many(sample\u001b[38;5;241m.\u001b[39misnan(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1092\u001b[0m sample \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;241m~\u001b[39mmask_nan]\n",
      "File \u001b[0;32m/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/yaspar/Documents/GitHub/diffusion_model_bootcamp/reference_implementations/tabular_reference_impelementation/single_table/src/baselines/tabddpm/model/gaussian_multinomial_diffusion.py:1026\u001b[0m, in \u001b[0;36mGaussianMultinomialDiffusion.sample_ddim\u001b[0;34m(self, num_samples, steps)\u001b[0m\n\u001b[1;32m   1024\u001b[0m model_out_num \u001b[38;5;241m=\u001b[39m model_out[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_numerical_features]\n\u001b[1;32m   1025\u001b[0m model_out_cat \u001b[38;5;241m=\u001b[39m model_out[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_numerical_features :]\n\u001b[0;32m-> 1026\u001b[0m z_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgaussian_ddim_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_out_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1028\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_cat:\n\u001b[1;32m   1030\u001b[0m     log_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultinomial_ddim_step(model_out_cat, log_z, t, t_prev)\n",
      "File \u001b[0;32m/projects/aieng/diffusion_bootcamp/env/diffusion-models-bootcamp-z7DAirMd-py3.9/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/yaspar/Documents/GitHub/diffusion_model_bootcamp/reference_implementations/tabular_reference_impelementation/single_table/src/baselines/tabddpm/model/gaussian_multinomial_diffusion.py:866\u001b[0m, in \u001b[0;36mGaussianMultinomialDiffusion.gaussian_ddim_step\u001b[0;34m(self, model_out_num, x, t, t_prev, clip_denoised, denoised_fn, eta)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgaussian_ddim_step\u001b[39m(\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m     eta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[1;32m    865\u001b[0m ):\n\u001b[0;32m--> 866\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgaussian_p_mean_variance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_out_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_denoised\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdenoised_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdenoised_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m     eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_eps_from_xstart(x, t, out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_xstart\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    877\u001b[0m     alpha_bar \u001b[38;5;241m=\u001b[39m extract(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas_cumprod, t, x\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/fs01/home/yaspar/Documents/GitHub/diffusion_model_bootcamp/reference_implementations/tabular_reference_impelementation/single_table/src/baselines/tabddpm/model/gaussian_multinomial_diffusion.py:270\u001b[0m, in \u001b[0;36mGaussianMultinomialDiffusion.gaussian_p_mean_variance\u001b[0;34m(self, model_output, x, t, clip_denoised, denoised_fn, model_kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m B, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m t\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (B,)\n\u001b[1;32m    268\u001b[0m model_variance \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    269\u001b[0m     [\n\u001b[0;32m--> 270\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposterior_variance\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    271\u001b[0m         (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas)[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    272\u001b[0m     ],\n\u001b[1;32m    273\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    274\u001b[0m )\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# model_variance = self.posterior_variance.to(x.device)\u001b[39;00m\n\u001b[1;32m    276\u001b[0m model_log_variance \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(model_variance)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_name= \"model_100000.pt\"\n",
    "\n",
    "tabddpm.sample(\n",
    "    info_path=f\"{PROCESSED_DATA_DIR}/{DATA_NAME}/info.json\",\n",
    "    num_samples=raw_config[\"sample\"][\"num_samples\"],\n",
    "    batch_size=raw_config[\"sample\"][\"batch_size\"],\n",
    "    sample_save_path=f\"{SYNTH_DATA_DIR}/{DATA_NAME}/tabddpm.csv\",\n",
    "    ddim=True,\n",
    "    steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-encounter",
   "metadata": {},
   "source": [
    "## Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb748981",
   "metadata": {},
   "source": [
    "Finally here, we review the synthesized data. In the following `evaluate_synthetic_data.ipynb` notebook, we will evaluate this synthesized data with respect to various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "constitutional-price",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>62811.754</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>14192.9910</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>225005.230</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.0</td>\n",
       "      <td>?</td>\n",
       "      <td>192886.600</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.0</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>219193.360</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63.0</td>\n",
       "      <td>?</td>\n",
       "      <td>64411.824</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51.0</td>\n",
       "      <td>?</td>\n",
       "      <td>225035.330</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59.0</td>\n",
       "      <td>?</td>\n",
       "      <td>66558.990</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>Poland</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>36.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>282701.120</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>7298.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>119717.470</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.124638</td>\n",
       "      <td>Yugoslavia</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>199313.920</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>37.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>155541.250</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>?</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>41.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>227369.030</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>49.0</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>167075.250</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>40.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>231898.470</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>51.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>438906.100</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.158445</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>46.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>104957.164</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>27.0</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>228706.840</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>48.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>238609.440</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>1622.2196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>46.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>199455.940</td>\n",
       "      <td>Assoc-voc</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>41.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>392744.470</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>15024.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age   workclass      fnlwgt      education  education.num  \\\n",
       "0   62.0     Private   62811.754        HS-grad            9.0   \n",
       "1   49.0     Private  225005.230        HS-grad           10.0   \n",
       "2   28.0           ?  192886.600      Bachelors           13.0   \n",
       "3   45.0   State-gov  219193.360        Masters           14.0   \n",
       "4   63.0           ?   64411.824      Bachelors           13.0   \n",
       "5   51.0           ?  225035.330      Bachelors           13.0   \n",
       "6   59.0           ?   66558.990   Some-college           10.0   \n",
       "7   36.0     Private  282701.120        HS-grad            9.0   \n",
       "8   32.0     Private  119717.470   Some-college           10.0   \n",
       "9   21.0     Private  199313.920        HS-grad            9.0   \n",
       "10  37.0     Private  155541.250      Bachelors           13.0   \n",
       "11  41.0     Private  227369.030        HS-grad            9.0   \n",
       "12  49.0   Local-gov  167075.250        HS-grad            9.0   \n",
       "13  40.0     Private  231898.470        HS-grad            9.0   \n",
       "14  51.0     Private  438906.100        HS-grad            6.0   \n",
       "15  46.0     Private  104957.164        HS-grad            9.0   \n",
       "16  27.0   Local-gov  228706.840   Some-college           10.0   \n",
       "17  48.0     Private  238609.440   Some-college           10.0   \n",
       "18  46.0     Private  199455.940      Assoc-voc           10.0   \n",
       "19  41.0     Private  392744.470   Some-college           12.0   \n",
       "\n",
       "         marital.status         occupation    relationship    race      sex  \\\n",
       "0    Married-civ-spouse       Craft-repair         Husband   White     Male   \n",
       "1    Married-civ-spouse    Farming-fishing         Husband   White     Male   \n",
       "2         Never-married     Prof-specialty         Husband   White     Male   \n",
       "3         Never-married     Prof-specialty         Husband   Black   Female   \n",
       "4              Divorced    Exec-managerial         Husband   White     Male   \n",
       "5              Divorced    Farming-fishing         Husband   White     Male   \n",
       "6              Divorced      Other-service         Husband   White   Female   \n",
       "7         Never-married   Transport-moving         Husband   White     Male   \n",
       "8    Married-civ-spouse     Prof-specialty         Husband   White     Male   \n",
       "9    Married-civ-spouse      Other-service   Not-in-family   White     Male   \n",
       "10   Married-civ-spouse    Exec-managerial   Not-in-family   White     Male   \n",
       "11        Never-married   Transport-moving         Husband   White     Male   \n",
       "12              Widowed       Tech-support       Unmarried   White   Female   \n",
       "13        Never-married       Adm-clerical   Not-in-family   Black   Female   \n",
       "14             Divorced       Craft-repair         Husband   White   Female   \n",
       "15        Never-married       Craft-repair         Husband   White   Female   \n",
       "16   Married-civ-spouse    Farming-fishing   Not-in-family   White     Male   \n",
       "17             Divorced    Farming-fishing         Husband   White   Female   \n",
       "18   Married-civ-spouse       Adm-clerical   Not-in-family   White     Male   \n",
       "19   Married-civ-spouse     Prof-specialty         Husband   White     Male   \n",
       "\n",
       "    capital.gain  capital.loss  hours.per.week  native.country  income  \n",
       "0     14192.9910           0.0       40.000000   United-States    >50K  \n",
       "1         0.0000           0.0       40.000000   United-States   <=50K  \n",
       "2         0.0000           0.0       45.000000   United-States   <=50K  \n",
       "3         0.0000           0.0       40.000000   United-States   <=50K  \n",
       "4         0.0000           0.0       46.000000   United-States    >50K  \n",
       "5         0.0000           0.0       45.000000   United-States   <=50K  \n",
       "6         0.0000           0.0       20.000000          Poland   <=50K  \n",
       "7      7298.0000           0.0       48.000000   United-States    >50K  \n",
       "8         0.0000           0.0       52.124638      Yugoslavia   <=50K  \n",
       "9         0.0000           0.0       40.000000   United-States   <=50K  \n",
       "10        0.0000           0.0       40.000000               ?    >50K  \n",
       "11        0.0000           0.0       40.000000   United-States   <=50K  \n",
       "12        0.0000           0.0       40.000000   United-States   <=50K  \n",
       "13        0.0000           0.0       40.000000   United-States   <=50K  \n",
       "14        0.0000           0.0       18.158445   United-States   <=50K  \n",
       "15        0.0000           0.0       40.000000   United-States   <=50K  \n",
       "16        0.0000           0.0       40.000000   United-States   <=50K  \n",
       "17     1622.2196           0.0       40.000000   United-States    >50K  \n",
       "18        0.0000           0.0       40.000000   United-States   <=50K  \n",
       "19    15024.0000           0.0       40.000000   United-States    >50K  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{SYNTH_DATA_DIR}/{DATA_NAME}/tabddpm.csv\")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e82ee8",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b905aa",
   "metadata": {},
   "source": [
    "**Kotelnikov, Akim, et al.** \"TABDDPM: Modeling tabular data with diffusion models.\" *Advances in Neural Information Processing Systems* 36 (2021).\n",
    "\n",
    "**GitHub Repository:** [Amazon Science - Tabsyn](https://github.com/amazon-science/tabsyn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_boot_shared",
   "language": "python",
   "name": "diffusion_boot_shared"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
