{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ea1925-b528-4c96-875c-ba73f97cbd8b",
   "metadata": {},
   "source": [
    "<center>\n",
    "  \n",
    "## Predict, Refine, Synthesize: Self-Guiding Diffusion Models for Probabilistic Time Series Forecasting\n",
    "\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a777f9b5-46cd-43f0-8da7-21657f565c46",
   "metadata": {},
   "source": [
    "The paper proposes TSDiff, an unconditionally-trained diffusion model for time series. TSDiff utilizes a self-guidance mechanism that allows it to conditionally generate forecasts, refine predictions, and produce synthetic data without requiring auxiliary networks or altering the training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ebc70d-1c01-4e9f-8db1-93ef50d6b2f6",
   "metadata": {},
   "source": [
    "Time series forecasting is crucial for making informed decisions in various fields such as finance, energy, and healthcare. Traditional deep learning models approach this problem through conditional generative modeling. The paper introduces TSDiff, an unconditional diffusion model for time series, which can handle multiple downstream tasks. The self-guidance mechanism allows TSDiff to perform predictive tasks during inference without conditional training. The model's generative capabilities are also leveraged to improve the accuracy of base forecasters and generate high-quality synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0c8ea-11ed-4b7f-905f-841a07cbf00f",
   "metadata": {},
   "source": [
    "In the following, we will take a deeper dive to the implementation of this method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17887c36-2c62-4f8a-8ab6-d7cccdade296",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a08fdbe-afa8-4be4-b2b7-72c94d5eeca4",
   "metadata": {},
   "source": [
    "In this section, we import all necessary libraries and modules required for setting up the environment. This includes libraries for logging, parsing arguments, handling file paths, and loading configurations. Additionally, we import essential packages for data loading, model creation, and training such as PyTorch, PyTorch Lightning, and GluonTS. Custom modules specific to the time series diffusion model (TSDiff) are also imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20e1b95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back on slow Cauchy kernel. Install at least one of pykeops or the CUDA extension for efficiency.\n",
      "Falling back on slow Vandermonde kernel. Install pykeops for improved memory efficiency.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, RichProgressBar\n",
    "\n",
    "from gluonts.dataset.loader import TrainDataLoader\n",
    "from gluonts.dataset.split import OffsetSplitter\n",
    "from gluonts.itertools import Cached\n",
    "from gluonts.torch.batchify import batchify\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "\n",
    "import uncond_ts_diff.configs as diffusion_configs\n",
    "from uncond_ts_diff.dataset import get_gts_dataset\n",
    "from uncond_ts_diff.model.callback import EvaluateCallback\n",
    "from uncond_ts_diff.model import TSDiff\n",
    "from uncond_ts_diff.sampler import DDPMGuidance, DDIMGuidance\n",
    "from uncond_ts_diff.utils import (\n",
    "    create_transforms,\n",
    "    create_splitter,\n",
    "    add_config_to_argparser,\n",
    "    filter_metrics,\n",
    "    MaskInput,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb1aff-c084-42ee-8484-33a71a5f9521",
   "metadata": {},
   "source": [
    "# Load Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c1e8a-5b0c-450f-b4f3-9275111af9e1",
   "metadata": {},
   "source": [
    "Here, we set up the configuration for the model training. This involves loading the configuration file which contains parameters and settings needed for the training process. The configuration is read from a YAML file and parsed into a dictionary format. Logging is also configured in this section to record the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "486282cb-bb69-41a6-8ca9-71222afd9af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Logger\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Setup config\n",
    "config_path = \"configs/train_tsdiff/train_uber_tlc.yaml\"\n",
    "log_dir = \"./\"\n",
    "\n",
    "with open(config_path, \"r\") as fp:\n",
    "    config = yaml.safe_load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9964c4e-7bc6-4835-8226-2913aa4b5e8a",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaaa562-4950-4c75-b168-e295cd3e6359",
   "metadata": {},
   "source": [
    "**TSDiff Model Architecture**\n",
    "\n",
    "TSDiff is designed for univariate time series and uses S4 layers for temporal modeling. The architecture incorporates historical information by appending lagged time series along the channel dimension, allowing it to handle noisy inputs. The model's output dimensions match its input dimensions, making it suitable for unconditional generative tasks.\n",
    "\n",
    "![TSDiff Model Architecture](figures/model_figure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f902a2-39d4-4ac9-b0eb-1cf2cba32187",
   "metadata": {},
   "source": [
    "The following cells focuses on creating the TSDiff model based on the loaded configuration. A function create_model is defined which initializes the TSDiff model with parameters such as frequency, feature usage, normalization, context length, prediction length, and learning rate. The model is then moved to the specified device (CPU or GPU). Below is a detailed explanation of the input parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f54efa-b824-4c57-acde-70afd76d5189",
   "metadata": {},
   "source": [
    "**`diffusion_configs`**:\n",
    "   - This is a module or an object containing various diffusion configurations. The specific configuration to use is specified by `config[\"diffusion_config\"]`.\n",
    "\n",
    "**`config[\"diffusion_config\"]`**:\n",
    "   - A string that specifies which diffusion configuration to use from `diffusion_configs`. This string is used with `getattr` to dynamically fetch the appropriate configuration.\n",
    "\n",
    "**`freq`**:\n",
    "   - `config[\"freq\"]`: The frequency of the time series data (e.g., daily, hourly). This helps the model understand the time granularity of the input data.\n",
    "\n",
    "**`use_features`**:\n",
    "   - `config[\"use_features\"]`: A boolean indicating whether to use additional features apart from the time series data. These could be features like categorical variables or other covariates.\n",
    "\n",
    "**`use_lags`**:\n",
    "   - `config[\"use_lags\"]`: A boolean indicating whether to use lagged values of the time series as inputs. Lagged values can provide important temporal context for forecasting.\n",
    "\n",
    "**`normalization`**:\n",
    "   - `config[\"normalization\"]`: Specifies the normalization technique to be applied to the time series data. Normalization can help in stabilizing the training process.\n",
    "\n",
    "**`context_length`**:\n",
    "   - `config[\"context_length\"]`: The length of the historical context window used for making predictions. This determines how much past data the model considers when forecasting future values.\n",
    "\n",
    "**`prediction_length`**:\n",
    "   - `config[\"prediction_length\"]`: The length of the forecast horizon. This specifies how many time steps ahead the model is expected to predict.\n",
    "\n",
    "**`lr`**:\n",
    "   - `config[\"lr\"]`: The learning rate for the model's optimizer. This controls the step size during the gradient descent optimization process.\n",
    "\n",
    "**`init_skip`**:\n",
    "   - `config[\"init_skip\"]`: A parameter that may be used to control the initialization or skipping of certain model components during training or inference.\n",
    "\n",
    "**`device`**:\n",
    "   - `config[\"device\"]`: The device on which the model will be trained or run, typically \"cpu\" or \"cuda\" (for GPU acceleration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f00df3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config):\n",
    "    model = TSDiff(\n",
    "        **getattr(diffusion_configs, config[\"diffusion_config\"]),\n",
    "        freq=config[\"freq\"],\n",
    "        use_features=config[\"use_features\"],\n",
    "        use_lags=config[\"use_lags\"],\n",
    "        normalization=config[\"normalization\"],\n",
    "        context_length=config[\"context_length\"],\n",
    "        prediction_length=config[\"prediction_length\"],\n",
    "        lr=config[\"lr\"],\n",
    "        init_skip=config[\"init_skip\"],\n",
    "    )\n",
    "    model.to(config[\"device\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "120544cf-81bc-4f2f-b671-9ef969f341fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = create_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a654796c-1b28-46e8-b093-6a237534f99c",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77539cf-f542-4f52-9158-0f85360c72a7",
   "metadata": {},
   "source": [
    "## Uber TLC Dataset\n",
    "\n",
    "This dataset contains data on Uber pickups in New York City. The dataset is divided into several parts, each containing detailed trip-level data. It has 4.3 million Uber pickups from January to June 2015, contained in a single file.\n",
    "\n",
    "\n",
    "#### Uber Pickup Data (2015) Columns:\n",
    "\n",
    "- `Date/Time`: The date and time of the Uber pickup.\n",
    "- `Base`: The TLC base company code affiliated with the Uber pickup.\n",
    "- `locationID`: The pickup location ID affiliated with the Uber pickup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e6568-959e-4191-abb3-06448a89ccf4",
   "metadata": {},
   "source": [
    "In this section, the dataset is loaded and preprocessed based on the configuration settings. The dataset's metadata is validated to ensure consistency with the expected frequency and prediction length. Depending on the setup (forecasting or missing values), the appropriate data split is performed. Transformations and data loaders are also set up to facilitate the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1672dc3-ea0d-4982-a2fd-23dfaaee7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters\n",
    "dataset_name = config[\"dataset\"]\n",
    "freq = config[\"freq\"]\n",
    "context_length = config[\"context_length\"]\n",
    "prediction_length = config[\"prediction_length\"]\n",
    "total_length = context_length + prediction_length\n",
    "\n",
    "# Setup dataset and data loading\n",
    "dataset = get_gts_dataset(dataset_name)\n",
    "assert dataset.metadata.freq == freq\n",
    "assert dataset.metadata.prediction_length == prediction_length\n",
    "\n",
    "# These lines check if the setup is for forecasting or predicting the missing values.\n",
    "if config[\"setup\"] == \"forecasting\":\n",
    "    training_data = dataset.train\n",
    "elif config[\"setup\"] == \"missing_values\":\n",
    "    # This line creates an instance of OffsetSplitter with a specified offset. \n",
    "    # The offset determines where the training data ends. \n",
    "    # A negative offset means that the training data will exclude a certain number \n",
    "    # of observations from the end of each series.\n",
    "    missing_values_splitter = OffsetSplitter(offset=-total_length)\n",
    "    training_data, _ = missing_values_splitter.split(dataset.train)\n",
    "\n",
    "# The purpose of this line is to determine the number of rolling evaluations \n",
    "# that can be performed using the test and training datasets.\n",
    "# In the context of time series forecasting, rolling evaluation is a method \n",
    "# where multiple models are trained and evaluated on different segments of the data. \n",
    "# Each evaluation uses a different starting point in the time series \n",
    "# to test the model's performance over time.\n",
    "# By determining how many times the training dataset length fits into the test dataset length,\n",
    "# this calculation helps set up multiple evaluation windows or periods over the test data.\n",
    "num_rolling_evals = int(len(dataset.test) / len(dataset.train))\n",
    "\n",
    "transformation = create_transforms(\n",
    "    num_feat_dynamic_real=0, # number of dynamic real-valued features in the dataset\n",
    "    num_feat_static_cat=0, # number of static categorical features in the dataset\n",
    "    num_feat_static_real=0, # number of static real-valued features in the dataset\n",
    "    time_features=model.time_features, # time-related features used by the model, such as day of the week, month, etc.\n",
    "    prediction_length=config[\"prediction_length\"], # length of the prediction horizon\n",
    ")\n",
    "\n",
    "training_splitter = create_splitter(\n",
    "    past_length=config[\"context_length\"] + max(model.lags_seq),\n",
    "    future_length=config[\"prediction_length\"],\n",
    "    mode=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e0b804-82f4-474e-9fa5-53a299af7ee3",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fd0ffe-6fe8-41ab-94cc-9c6b1b521420",
   "metadata": {},
   "source": [
    "This section sets up the training process for the TSDiff model. Various callbacks are configured to monitor and save the model during training. The trainer is then defined using PyTorch Lightning, specifying parameters such as the number of epochs, devices, and callbacks. The training process is started, logging the progress and completing the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08eb6eb2-917a-42a1-a0e5-6196a894d4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "# Callbacks are objects that are used to customize and control the behavior of the training process.\n",
    "\n",
    "# Different types of callbacks:\n",
    "\n",
    "# EvaluateCallback: If a validation set is used, this callback is created to evaluate \n",
    "# the model on the validation data at regular intervals specified by config[\"eval_every\"].\n",
    "\n",
    "# ModelCheckpoint: This callback saves the top 3 models based on the \n",
    "# monitored metric (train_loss) and the last model's weights.\n",
    "\n",
    "# RichProgressBar: This callback adds a progress bar to visualize training progress.\n",
    "\n",
    "callbacks = []\n",
    "if config[\"use_validation_set\"]:\n",
    "    transformed_data = transformation.apply(training_data, is_train=True)\n",
    "    train_val_splitter = OffsetSplitter(\n",
    "        offset=-config[\"prediction_length\"] * num_rolling_evals\n",
    "    )\n",
    "    _, val_gen = train_val_splitter.split(training_data)\n",
    "    val_data = val_gen.generate_instances(\n",
    "        config[\"prediction_length\"], num_rolling_evals\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        EvaluateCallback(\n",
    "            context_length=config[\"context_length\"],\n",
    "            prediction_length=config[\"prediction_length\"],\n",
    "            sampler=config[\"sampler\"],\n",
    "            sampler_kwargs=config[\"sampler_params\"],\n",
    "            num_samples=config[\"num_samples\"],\n",
    "            model=model,\n",
    "            transformation=transformation,\n",
    "            test_dataset=dataset.test,\n",
    "            val_dataset=val_data,\n",
    "            eval_every=config[\"eval_every\"],\n",
    "        )\n",
    "    ]\n",
    "else:\n",
    "    transformed_data = transformation.apply(training_data, is_train=True)\n",
    "\n",
    "log_monitor = \"train_loss\"\n",
    "filename = dataset_name + \"-{epoch:03d}-{train_loss:.3f}\"\n",
    "\n",
    "data_loader = TrainDataLoader(\n",
    "    Cached(transformed_data),\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    stack_fn=batchify,\n",
    "    transform=training_splitter,\n",
    "    num_batches_per_epoch=config[\"num_batches_per_epoch\"],\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=3,\n",
    "    monitor=f\"{log_monitor}\",\n",
    "    mode=\"min\",\n",
    "    filename=filename,\n",
    "    save_last=True,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "callbacks.append(checkpoint_callback)\n",
    "callbacks.append(RichProgressBar())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b619b4f-132a-4cad-b45b-d15540366ece",
   "metadata": {},
   "source": [
    "### What is `trainer`?\n",
    "\n",
    "The `trainer` in this context refers to an instance of the `pl.Trainer` class from the PyTorch Lightning library. PyTorch Lightning is a high-level library built on top of PyTorch that provides a structured and flexible framework for training deep learning models. It abstracts much of the boilerplate code involved in setting up training loops, validation, logging, and checkpointing, allowing researchers and developers to focus on the model and data.\n",
    "\n",
    "### Key Features of `pl.Trainer`\n",
    "\n",
    "1. **Simplified Training Loop**:\n",
    "   - The `Trainer` handles the training loop, including forward and backward passes, gradient updates, and validation steps.\n",
    "\n",
    "2. **Hardware Acceleration**:\n",
    "   - Supports training on multiple GPUs, TPUs, or CPUs with minimal code changes. The `accelerator` and `devices` parameters make it easy to specify the hardware configuration.\n",
    "\n",
    "3. **Automatic Logging**:\n",
    "   - Integrates with logging frameworks like TensorBoard and Weights & Biases, enabling easy tracking of metrics and model performance.\n",
    "\n",
    "4. **Checkpointing**:\n",
    "   - Automatically saves model checkpoints based on specified criteria (e.g., best validation loss), making it easy to resume training or deploy the best model.\n",
    "\n",
    "5. **Callbacks**:\n",
    "   - Supports a wide range of callbacks for custom behavior during training, such as early stopping, learning rate scheduling, and custom evaluation metrics.\n",
    "\n",
    "6. **Scalability**:\n",
    "   - Designed to scale from single-device training to multi-device and multi-node training with minimal code changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d980d3a3-29c7-41a5-ad6c-a2728dcf47c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/vkhazaie/anaconda3/envs/tsdiff/lib/python3.8/site-packages/lightning_fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /h/vkhazaie/anaconda3/envs/tsdiff/lib/python3.8/site ...\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/h/vkhazaie/anaconda3/envs/tsdiff/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else None, #  specifies the hardware accelerator to be used for training\n",
    "    devices=[int(config[\"device\"].split(\":\")[-1])], # specifies which devices to use for training\n",
    "    max_epochs=config[\"max_epochs\"], # specifies the maximum number of epochs for training\n",
    "    enable_progress_bar=True, # enables or disables the progress bar during training\n",
    "    num_sanity_val_steps=0, # Number of validation steps to run before training to catch potential issues\n",
    "    callbacks=callbacks, # A list of callbacks to be used during training\n",
    "    default_root_dir=log_dir, # The directory where the model checkpoints and logs will be saved\n",
    "    gradient_clip_val=config.get(\"gradient_clip_val\", None), # The value for gradient clipping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a7c175-60e6-40bf-b034-82d7319a998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Logging to {trainer.logger.log_dir}\")\n",
    "trainer.fit(model, train_dataloaders=data_loader)\n",
    "logger.info(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60e8c5c-b53d-4a91-bc1f-c52372f3b5d1",
   "metadata": {},
   "source": [
    "In case you want to skip the training and load a pretrained model, execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc052b45-625a-4327-8f6e-c062f287d096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ckpt_path = os.path.join(\"/projects/aieng/diffusion_bootcamp/models/time-seris/tsdiff/lightning_logs/version_2\", \"best_checkpoint.ckpt\")\n",
    "best_state_dict = torch.load(best_ckpt_path)\n",
    "model.load_state_dict(best_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95b1cb0-55a5-43e4-82fa-60253c27ba5f",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8070abc6-fdc4-4379-915c-e7fcadf09456",
   "metadata": {},
   "source": [
    "In this section, we discuss the two options we can choose for guidance for the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e423e3da-d717-4e68-8def-c339a2dfd1d8",
   "metadata": {},
   "source": [
    "## DDPM (Denoising Diffusion Probabilistic Models)\n",
    "\n",
    "- DDPM is a type of generative model that learns to generate data by reversing a diffusion process.\n",
    "- The model involves two processes: a forward process (diffusion) and a reverse process (denoising).\n",
    "- During the forward process, noise is gradually added to the data over a series of steps until the data becomes pure noise.\n",
    "- During the reverse process, the model learns to progressively denoise the data, effectively generating new samples from noise.\n",
    "\n",
    "## DDIM (Denoising Diffusion Implicit Models)\n",
    "\n",
    "- DDIM is a variant of DDPM that modifies the sampling process to be more efficient and deterministic.\n",
    "- It introduces a non-Markovian forward process that allows for deterministic sampling without the need for auxiliary noise terms.\n",
    "\n",
    "In summary, both DDPM and DDIM are powerful generative models based on diffusion processes, with DDIM offering improvements in efficiency and speed through its deterministic sampling approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e67335c-1989-4e9d-9d4c-3a50d4c34405",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_map = {\"ddpm\": DDPMGuidance, \"ddim\": DDIMGuidance}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba9522-b8a4-41c1-a43e-7d9095119954",
   "metadata": {},
   "source": [
    "After the training is completed, the model is evaluated on the test dataset. A function evaluate_guidance is defined to assess the model's performance using different sampling techniques. This involves generating forecasts, applying transformations, and calculating metrics to evaluate the accuracy of the predictions. The best model checkpoint is loaded, and the evaluation results are saved for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae3609-3c3f-4e0f-9a47-88b4605803d0",
   "metadata": {},
   "source": [
    "Here we explain how the model works during inference:\n",
    "\n",
    "**Observation Self-Guidance:**\n",
    "This mechanism enables TSDiff to perform conditional forecasting during inference. By leveraging the learned probability density, TSDiff can guide its predictions based on observed data points. Two variants are proposed: mean square self-guidance, which uses Gaussian distribution, and quantile self-guidance, which uses asymmetric Laplace distribution for better quantile-based evaluation.\n",
    "\n",
    "**Prediction Refinement:**\n",
    "TSDiff can iteratively refine the predictions of base forecasters by interpreting the implicit probability density as a prior. This refinement is done directly in the data space, providing a computationally efficient way to improve forecast accuracy without modifying the core forecasting model. Two approaches are presented: energy-based sampling using Langevin Monte Carlo and maximum likelihood optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aad201d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate guidance\n",
    "def evaluate_guidance(\n",
    "    config, model, test_dataset, transformation, num_samples=100\n",
    "):\n",
    "    logger.info(f\"Evaluating with {num_samples} samples.\")\n",
    "    results = []\n",
    "    if config[\"setup\"] == \"forecasting\":\n",
    "        missing_data_kwargs_list = [\n",
    "            {\n",
    "                \"missing_scenario\": \"none\",\n",
    "                \"missing_values\": 0,\n",
    "            }\n",
    "        ]\n",
    "        config[\"missing_data_configs\"] = missing_data_kwargs_list\n",
    "    elif config[\"setup\"] == \"missing_values\":\n",
    "        missing_data_kwargs_list = config[\"missing_data_configs\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown setup {config['setup']}\")\n",
    "\n",
    "    # The guidance strategy is specified using the config[\"sampler\"] and\n",
    "    # config[\"sampler_params\"] keys within the configuration dictionary (config)\n",
    "    # Mean-Square Self Guidance: aims to minimize the mean squared error (MSE) during sampling.\n",
    "    # Quantile Self Guidance: aims to predict specific quantiles of the target distribution, providing a fuller picture of uncertainty.\n",
    "    Guidance = guidance_map[config[\"sampler\"]]\n",
    "    sampler_kwargs = config[\"sampler_params\"]\n",
    "    for missing_data_kwargs in missing_data_kwargs_list:\n",
    "        logger.info(\n",
    "            f\"Evaluating scenario '{missing_data_kwargs['missing_scenario']}' \"\n",
    "            f\"with {missing_data_kwargs['missing_values']:.1f} missing_values.\"\n",
    "        )\n",
    "        sampler = Guidance(\n",
    "            model=model,\n",
    "            prediction_length=config[\"prediction_length\"],\n",
    "            num_samples=num_samples,\n",
    "            **missing_data_kwargs,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "\n",
    "        transformed_testdata = transformation.apply(\n",
    "            test_dataset, is_train=False\n",
    "        )\n",
    "        test_splitter = create_splitter(\n",
    "            past_length=config[\"context_length\"] + max(model.lags_seq),\n",
    "            future_length=config[\"prediction_length\"],\n",
    "            mode=\"test\",\n",
    "        )\n",
    "\n",
    "        masking_transform = MaskInput(\n",
    "            FieldName.TARGET,\n",
    "            FieldName.OBSERVED_VALUES,\n",
    "            config[\"context_length\"],\n",
    "            missing_data_kwargs[\"missing_scenario\"],\n",
    "            missing_data_kwargs[\"missing_values\"],\n",
    "        )\n",
    "        test_transform = test_splitter + masking_transform\n",
    "\n",
    "        predictor = sampler.get_predictor(\n",
    "            test_transform,\n",
    "            batch_size=1280 // num_samples,\n",
    "            device=config[\"device\"],\n",
    "        )\n",
    "        forecast_it, ts_it = make_evaluation_predictions(\n",
    "            dataset=transformed_testdata,\n",
    "            predictor=predictor,\n",
    "            num_samples=num_samples,\n",
    "        )\n",
    "        forecasts = list(tqdm(forecast_it, total=len(transformed_testdata)))\n",
    "        tss = list(ts_it)\n",
    "        evaluator = Evaluator()\n",
    "        metrics, _ = evaluator(tss, forecasts)\n",
    "        metrics = filter_metrics(metrics)\n",
    "        results.append(dict(**missing_data_kwargs, **metrics))\n",
    "\n",
    "    return results, tss, forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f62682-936d-4301-83a5-4f8d773eb15f",
   "metadata": {},
   "source": [
    "### Metrics Explanation\n",
    "\n",
    "#### ND (Normalized Deviation)\n",
    "- **Definition**: Normalized Deviation (ND) measures the average absolute deviation of the forecast from the actual values, normalized by the sum of the actual values.\n",
    "- **Interpretation**: \n",
    "  - ND gives an idea of the relative scale of the forecast errors compared to the actual values. \n",
    "  - Lower values indicate better model performance.\n",
    "  - It is particularly useful when dealing with time series data that may have different scales.\n",
    "\n",
    "#### NRMSE (Normalized Root Mean Square Error)\n",
    "- **Definition**: Normalized Root Mean Square Error (NRMSE) measures the square root of the average squared differences between predicted and actual values, normalized by the range of the actual values.\n",
    "- **Interpretation**: \n",
    "  - NRMSE provides a normalized measure of the forecast accuracy that accounts for the variability in the actual values.\n",
    "  - Lower values indicate better model performance.\n",
    "  - It is useful for comparing the performance of models across different datasets or scales.\n",
    "\n",
    "#### mean_wQuantileLoss (Mean Weighted Quantile Loss)\n",
    "- **Definition**: Mean Weighted Quantile Loss (mean_wQuantileLoss) measures the average quantile loss weighted by the quantile level. It assesses the accuracy of quantile forecasts, which provide a range of possible future values.\n",
    "- **Interpretation**: \n",
    "  - Quantile Loss evaluates the accuracy of probabilistic forecasts by penalizing over- and under-predictions differently depending on the quantile level.\n",
    "  - Lower values indicate better model performance.\n",
    "  - It is especially useful for applications where it is important to predict a range of possible future outcomes rather than a single point estimate.\n",
    "\n",
    "These metrics help in assessing the robustness and performance of the forecasting model under different scenarios, including scenarios with missing values, which is crucial for improving and fine-tuning the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2714934-c07e-47ce-ab92-fe708dbbeff2",
   "metadata": {},
   "source": [
    "In case you didn't skip the training, you should uncomment the follwing line to get latest version of the pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b27d48-cf70-4aff-a7fc-4b4757d4ebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ckpt_path = Path(trainer.logger.log_dir) / \"best_checkpoint.ckpt\"\n",
    "\n",
    "if not best_ckpt_path.exists():\n",
    "    torch.save(\n",
    "        torch.load(checkpoint_callback.best_model_path)[\"state_dict\"],\n",
    "        best_ckpt_path,\n",
    "    )\n",
    "\n",
    "logger.info(f\"Loading {best_ckpt_path}.\")\n",
    "best_state_dict = torch.load(best_ckpt_path)\n",
    "model.load_state_dict(best_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ec0b3f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 15:45:58,387 - logger - INFO - Evaluating with 100 samples.\n",
      "2024-07-29 15:45:58,390 - logger - INFO - Evaluating scenario 'none' with 0.0 missing_values.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21bc89fe1e304037a996a4a0e8a0410e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running evaluation: 262it [00:04, 56.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and save results\n",
    "metrics, tss, forecasts = (\n",
    "    evaluate_guidance(config, model, dataset.test, transformation)\n",
    "    if config.get(\"do_final_eval\", True)\n",
    "    else \"Final eval not performed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c31b13-2a70-4bcc-a959-c0df2d69dbbc",
   "metadata": {},
   "source": [
    "In case you skipped the training, execute the following cell to see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dde99b08-17de-4ac2-bdc7-71799c3f53bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = os.path.join(\"/projects/aieng/diffusion_bootcamp/models/time-seris/tsdiff/lightning_logs/version_2\", \"results.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b0fb3f-dad8-4d51-b4c0-e69477ee2e20",
   "metadata": {},
   "source": [
    "Otherwise, execute the following cell to see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f78100-6f18-4735-8916-759413f969b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = Path(trainer.logger.log_dir) / \"results.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2eb91dab-61a1-446f-bfd1-33d3c494d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_path, \"w\") as fp:\n",
    "    yaml.dump(\n",
    "        {\n",
    "            \"config\": config,\n",
    "            \"version\": trainer.logger.version,\n",
    "            \"metrics\": metrics,\n",
    "        },\n",
    "        fp,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "300d99d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'missing_scenario': 'none',\n",
       "  'missing_values': 0,\n",
       "  'ND': 0.2109212917114393,\n",
       "  'NRMSE': 3.354814075187851,\n",
       "  'mean_wQuantileLoss': 0.17019186763220753}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d0d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming tss and forecasts are the lists containing your ground truths and predictions\n",
    "\n",
    "# Combine all ground truth data into a single DataFrame\n",
    "ground_truth = pd.concat(tss)\n",
    "\n",
    "# Initialize an empty list to store prediction data\n",
    "all_predictions = []\n",
    "\n",
    "# Iterate through each SampleForecast object in forecasts\n",
    "for forecast in forecasts:\n",
    "    # Calculate the mean of the prediction samples\n",
    "    pred_mean = forecast.samples.mean(axis=0)\n",
    "    # Convert start_date to a Timestamp\n",
    "    start_timestamp = forecast.start_date.to_timestamp()\n",
    "    # Create a date range for the prediction period\n",
    "    prediction_dates = pd.date_range(start=start_timestamp, periods=len(pred_mean), freq='H')\n",
    "    # Create a DataFrame for the predictions\n",
    "    pred_df = pd.DataFrame(pred_mean, index=prediction_dates, columns=['Prediction'])\n",
    "    # Append the prediction DataFrame to the list\n",
    "    all_predictions.append(pred_df)\n",
    "\n",
    "# Combine all prediction DataFrames into a single DataFrame\n",
    "predictions = pd.concat(all_predictions)\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(14, 7))\n",
    "preds = np.zeros(len(ground_truth.values))\n",
    "preds[-len(predictions.values):] = predictions.values.squeeze()\n",
    "\n",
    "# Number of points you want to select\n",
    "num_points = 100\n",
    "\n",
    "# Randomly select indices\n",
    "random_indices = np.random.choice(len(ground_truth.values), num_points, replace=False)\n",
    "\n",
    "# Sort indices to maintain the time order\n",
    "random_indices.sort()\n",
    "\n",
    "# Select the data points at those indices\n",
    "gt = ground_truth.values[random_indices]\n",
    "\n",
    "# Randomly select indices\n",
    "random_indices = np.random.choice(len(preds), num_points, replace=False)\n",
    "\n",
    "# Sort indices to maintain the time order\n",
    "random_indices.sort()\n",
    "\n",
    "# Select the data points at those indices\n",
    "p = preds[random_indices]\n",
    "\n",
    "plt.plot(gt, label='Ground Truth', color='blue')\n",
    "plt.plot(p, label='Prediction', color='red')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Time Series Ground Truth vs Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b448f-0292-482a-b58f-8f9fa4ed375b",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f77be91-f336-4dde-8e86-98733cc0ec6b",
   "metadata": {},
   "source": [
    "**Kollovieh, Marcel, et al.** \"Predict, refine, synthesize: Self-guiding diffusion models for probabilistic time series forecasting.\" *Advances in Neural Information Processing Systems* 36 (2024).\n",
    "\n",
    "**GitHub Repository:** [Amazon Science - Unconditional Time Series Diffusion](https://github.com/amazon-science/unconditional-time-series-diffusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsdiff",
   "language": "python",
   "name": "tsdiff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
