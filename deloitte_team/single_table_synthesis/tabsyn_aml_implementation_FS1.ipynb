{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  \n",
    "# TABSYN: Tabular Data Synthesis with Diffusion Models\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two challenges regarding the extention of diffusion models to tabular data are:\n",
    "1. **Diverse data types:** a single table can have different columns each containing data of different types, including numerical, categorical, text, etc.\n",
    "2. **Varied distributions:** the distribution of data under different columns in a single table varry widely from column to column.\n",
    "\n",
    "**TabSyn** addresses these challenges by introducing a latent space where tabular data of all columns are jointly represented. It then proceedes to train a diffusion model on the latent representations.\n",
    "This tactic allows TabSyn to:\n",
    "1. Train a single diffusion model for all data types in the dataset (i.e. Generality).\n",
    "2. Optimize the distribution of latent embeddings to facilitate training of the subsequent diffusion model, thus generating higher quality synthetic data (i.e. Quality).\n",
    "3. Require much fewer reverse steps during training of the diffusion model, and synthesize data faster (i.e. Speed).\n",
    "\n",
    "In this notebook, we review and implement the TabSyn model. The notebook is organized as follows:\n",
    "\n",
    "1. [Imports and Setup]()\n",
    "\n",
    "\n",
    "2. [Default Dataset]()\n",
    "    \n",
    "    \n",
    "3. [TabSyn Algorithm]()\n",
    "    \n",
    "    3.1. [Load Config]()\n",
    "    \n",
    "    3.2. [Make Dataset]()\n",
    "    \n",
    "    3.3. [Instantiate Model]()\n",
    "    \n",
    "    3.4. [Train Model]()\n",
    "        \n",
    "    3.5. [Load Pretrained Model]()\n",
    "    \n",
    "    3.6. [Sample Data]()\n",
    "    \n",
    "    3.7. [Review Synthetic Data]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we import all necessary libraries and modules required for setting up the environment.\n",
    "Most of the libraries we need to implement TabSyn are the same as TabDDPM.\n",
    "We also specify `NAME_URL_DICT_UCI`, `DATA_NAME`, `DATA_DIR` and other paths as in TabDDPM's implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/fs01/home/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('/fs01/home/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scripts.process_dataset import process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21941/4202322190.py:7: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, :29] = df.iloc[:, :29].astype(bool)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/h/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/data/raw_data/IBM_AML_FeatureEngineered_FS1/flattened_sample.csv')\n",
    "# Ignore the The Patern ID and The Source columns\n",
    "df=df.iloc[::,1::]   \n",
    "# Convert the first 29 Destination and Currency columns to boolean\n",
    "df.iloc[:, :29] = df.iloc[:, :29].astype(bool)\n",
    "\n",
    "# Convert the last column Is Fanout to boolean\n",
    "df.iloc[:, -1] = df.iloc[:, -1].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Destination_1</th>\n",
       "      <th>Destination_2</th>\n",
       "      <th>Destination_3</th>\n",
       "      <th>Destination_4</th>\n",
       "      <th>Destination_5</th>\n",
       "      <th>Destination_6</th>\n",
       "      <th>Destination_7</th>\n",
       "      <th>Destination_8</th>\n",
       "      <th>Destination_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Yen</th>\n",
       "      <th>UK Pound</th>\n",
       "      <th>number_transactions_above_9k_cad</th>\n",
       "      <th>avg_transaction_value_in_cad</th>\n",
       "      <th>transaction_value_variance_in_cad</th>\n",
       "      <th>variance_from_10k_cad</th>\n",
       "      <th>min_day_to_max_day_range</th>\n",
       "      <th>avg_transaction_frequency_in_days</th>\n",
       "      <th>transaction_frequency_variance_in_days</th>\n",
       "      <th>Is FanOut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5873.10725</td>\n",
       "      <td>5.063292e+05</td>\n",
       "      <td>3.456882e+07</td>\n",
       "      <td>6</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>648570.30320</td>\n",
       "      <td>7.507306e+11</td>\n",
       "      <td>1.894381e+13</td>\n",
       "      <td>5</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>649125.71180</td>\n",
       "      <td>8.189731e+11</td>\n",
       "      <td>1.391048e+13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>472.97624</td>\n",
       "      <td>4.133341e+05</td>\n",
       "      <td>1.819417e+08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.66720</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.968690e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Source  Destination_1  Destination_2  Destination_3  Destination_4  \\\n",
       "0    True           True           True          False          False   \n",
       "1    True           True           True           True           True   \n",
       "2    True           True           True           True           True   \n",
       "3    True           True           True          False          False   \n",
       "4    True           True          False          False          False   \n",
       "\n",
       "   Destination_5  Destination_6  Destination_7  Destination_8  Destination_9  \\\n",
       "0          False          False          False          False          False   \n",
       "1           True           True           True           True           True   \n",
       "2           True           True           True           True           True   \n",
       "3          False          False          False          False          False   \n",
       "4          False          False          False          False          False   \n",
       "\n",
       "   ...    Yen  UK Pound  number_transactions_above_9k_cad  \\\n",
       "0  ...  False         0                                 0   \n",
       "1  ...  False         0                                13   \n",
       "2  ...  False         0                                 9   \n",
       "3  ...  False         0                                 0   \n",
       "4  ...  False         0                                 0   \n",
       "\n",
       "   avg_transaction_value_in_cad  transaction_value_variance_in_cad  \\\n",
       "0                    5873.10725                       5.063292e+05   \n",
       "1                  648570.30320                       7.507306e+11   \n",
       "2                  649125.71180                       8.189731e+11   \n",
       "3                     472.97624                       4.133341e+05   \n",
       "4                      15.66720                                NaN   \n",
       "\n",
       "   variance_from_10k_cad  min_day_to_max_day_range  \\\n",
       "0           3.456882e+07                         6   \n",
       "1           1.894381e+13                         5   \n",
       "2           1.391048e+13                         1   \n",
       "3           1.819417e+08                         0   \n",
       "4           9.968690e+07                         0   \n",
       "\n",
       "   avg_transaction_frequency_in_days  transaction_frequency_variance_in_days  \\\n",
       "0                           3.000000                                 1.66884   \n",
       "1                           0.294118                                 1.66884   \n",
       "2                           0.083333                                 1.66884   \n",
       "3                           0.000000                                 1.66884   \n",
       "4                           0.000000                                 1.66884   \n",
       "\n",
       "   Is FanOut  \n",
       "0      False  \n",
       "1      False  \n",
       "2      False  \n",
       "3      False  \n",
       "4      False  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'column_info': {'Source': 'bool', 'Destination_1': 'bool', 'Destination_2': 'bool', 'Destination_3': 'bool', 'Destination_4': 'bool', 'Destination_5': 'bool', 'Destination_6': 'bool', 'Destination_7': 'bool', 'Destination_8': 'bool', 'Destination_9': 'bool', 'Destination_10': 'bool', 'Destination_11': 'bool', 'Destination_12': 'bool', 'Destination_13': 'bool', 'Destination_14': 'bool', 'Destination_15': 'bool', 'Destination_16': 'bool', 'US Dollar': 'bool', 'Euro': 'bool', 'Yuan': 'bool', 'Rupee': 'bool', 'Ruble': 'bool', 'Canadian Dollar': 'bool', 'Australian Dollar': 'bool', 'Brazil Real': 'bool', 'Swiss Franc': 'bool', 'Shekel': 'bool', 'Bitcoin': 'bool', 'Yen': 'bool', 'UK Pound': 'int', 'number_transactions_above_9k_cad': 'int', 'avg_transaction_value_in_cad': 'float', 'transaction_value_variance_in_cad': 'float', 'variance_from_10k_cad': 'float', 'min_day_to_max_day_range': 'int', 'avg_transaction_frequency_in_days': 'float', 'transaction_frequency_variance_in_days': 'float', 'Is FanOut': 'bool'}}\n"
     ]
    }
   ],
   "source": [
    "def get_column_info(df):\n",
    "    column_info = {col: str(df[col].dtype).replace('float64', 'float').replace('int64', 'int').replace('object', 'str') for col in df.columns}\n",
    "    return column_info\n",
    "\n",
    "# Get column info\n",
    "column_info = get_column_info(df)\n",
    "\n",
    "# Print in desired format\n",
    "print({\"column_info\": column_info})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def generate_json_config(df):\n",
    "    # Get column names\n",
    "    column_names = df.columns.tolist()\n",
    "    \n",
    "    # Identify the target column index (last column)\n",
    "    target_col_idx = [len(column_names) - 1]\n",
    "    \n",
    "    # Initialize lists for numerical and categorical column indices\n",
    "    num_col_idx = []\n",
    "    cat_col_idx = []\n",
    "    \n",
    "    # Initialize dictionary for column info\n",
    "    column_info = {}\n",
    "    \n",
    "    # Determine column types and indices\n",
    "    for idx, col in enumerate(df.columns):\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        # Fill in column_info with appropriate type\n",
    "        if pd.api.types.is_integer_dtype(col_type):\n",
    "            column_info[col] = \"int\"\n",
    "            if idx != target_col_idx[0]:  # Exclude target column from num_col_idx\n",
    "                num_col_idx.append(idx)\n",
    "        elif pd.api.types.is_float_dtype(col_type):\n",
    "            column_info[col] = \"float\"\n",
    "            num_col_idx.append(idx)\n",
    "        elif pd.api.types.is_bool_dtype(col_type):\n",
    "            column_info[col] = \"bool\"\n",
    "            cat_col_idx.append(idx)\n",
    "        else:\n",
    "            column_info[col] = \"str\"\n",
    "            cat_col_idx.append(idx)\n",
    "    \n",
    "    # Calculate train_num and test_num\n",
    "    total_rows = len(df)\n",
    "    train_num = int(total_rows * 0.8)\n",
    "    test_num = total_rows - train_num\n",
    "    \n",
    "    # Construct JSON configuration object\n",
    "    config = {\n",
    "        \"column_names\": column_names,\n",
    "        \"num_col_idx\": num_col_idx,\n",
    "        \"cat_col_idx\": cat_col_idx,\n",
    "        \"target_col_idx\": target_col_idx,\n",
    "        \"file_type\": \"csv\",\n",
    "        \"data_path\": \"/h/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/data/processed_data/flattened_sample.csv\", \n",
    "        \"column_info\": column_info,\n",
    "        \"train_num\": train_num,\n",
    "        \"test_num\": test_num\n",
    "    }\n",
    "    \n",
    "    # Print JSON object\n",
    "    print(json.dumps(config, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"column_names\": [\n",
      "        \"Source\",\n",
      "        \"Destination_1\",\n",
      "        \"Destination_2\",\n",
      "        \"Destination_3\",\n",
      "        \"Destination_4\",\n",
      "        \"Destination_5\",\n",
      "        \"Destination_6\",\n",
      "        \"Destination_7\",\n",
      "        \"Destination_8\",\n",
      "        \"Destination_9\",\n",
      "        \"Destination_10\",\n",
      "        \"Destination_11\",\n",
      "        \"Destination_12\",\n",
      "        \"Destination_13\",\n",
      "        \"Destination_14\",\n",
      "        \"Destination_15\",\n",
      "        \"Destination_16\",\n",
      "        \"US Dollar\",\n",
      "        \"Euro\",\n",
      "        \"Yuan\",\n",
      "        \"Rupee\",\n",
      "        \"Ruble\",\n",
      "        \"Canadian Dollar\",\n",
      "        \"Australian Dollar\",\n",
      "        \"Brazil Real\",\n",
      "        \"Swiss Franc\",\n",
      "        \"Shekel\",\n",
      "        \"Bitcoin\",\n",
      "        \"Yen\",\n",
      "        \"UK Pound\",\n",
      "        \"number_transactions_above_9k_cad\",\n",
      "        \"avg_transaction_value_in_cad\",\n",
      "        \"transaction_value_variance_in_cad\",\n",
      "        \"variance_from_10k_cad\",\n",
      "        \"min_day_to_max_day_range\",\n",
      "        \"avg_transaction_frequency_in_days\",\n",
      "        \"transaction_frequency_variance_in_days\",\n",
      "        \"Is FanOut\"\n",
      "    ],\n",
      "    \"num_col_idx\": [\n",
      "        29,\n",
      "        30,\n",
      "        31,\n",
      "        32,\n",
      "        33,\n",
      "        34,\n",
      "        35,\n",
      "        36\n",
      "    ],\n",
      "    \"cat_col_idx\": [\n",
      "        0,\n",
      "        1,\n",
      "        2,\n",
      "        3,\n",
      "        4,\n",
      "        5,\n",
      "        6,\n",
      "        7,\n",
      "        8,\n",
      "        9,\n",
      "        10,\n",
      "        11,\n",
      "        12,\n",
      "        13,\n",
      "        14,\n",
      "        15,\n",
      "        16,\n",
      "        17,\n",
      "        18,\n",
      "        19,\n",
      "        20,\n",
      "        21,\n",
      "        22,\n",
      "        23,\n",
      "        24,\n",
      "        25,\n",
      "        26,\n",
      "        27,\n",
      "        28,\n",
      "        37\n",
      "    ],\n",
      "    \"target_col_idx\": [\n",
      "        37\n",
      "    ],\n",
      "    \"file_type\": \"csv\",\n",
      "    \"data_path\": \"/h/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/data/processed_data/flattened_sample.csv\",\n",
      "    \"column_info\": {\n",
      "        \"Source\": \"bool\",\n",
      "        \"Destination_1\": \"bool\",\n",
      "        \"Destination_2\": \"bool\",\n",
      "        \"Destination_3\": \"bool\",\n",
      "        \"Destination_4\": \"bool\",\n",
      "        \"Destination_5\": \"bool\",\n",
      "        \"Destination_6\": \"bool\",\n",
      "        \"Destination_7\": \"bool\",\n",
      "        \"Destination_8\": \"bool\",\n",
      "        \"Destination_9\": \"bool\",\n",
      "        \"Destination_10\": \"bool\",\n",
      "        \"Destination_11\": \"bool\",\n",
      "        \"Destination_12\": \"bool\",\n",
      "        \"Destination_13\": \"bool\",\n",
      "        \"Destination_14\": \"bool\",\n",
      "        \"Destination_15\": \"bool\",\n",
      "        \"Destination_16\": \"bool\",\n",
      "        \"US Dollar\": \"bool\",\n",
      "        \"Euro\": \"bool\",\n",
      "        \"Yuan\": \"bool\",\n",
      "        \"Rupee\": \"bool\",\n",
      "        \"Ruble\": \"bool\",\n",
      "        \"Canadian Dollar\": \"bool\",\n",
      "        \"Australian Dollar\": \"bool\",\n",
      "        \"Brazil Real\": \"bool\",\n",
      "        \"Swiss Franc\": \"bool\",\n",
      "        \"Shekel\": \"bool\",\n",
      "        \"Bitcoin\": \"bool\",\n",
      "        \"Yen\": \"bool\",\n",
      "        \"UK Pound\": \"int\",\n",
      "        \"number_transactions_above_9k_cad\": \"int\",\n",
      "        \"avg_transaction_value_in_cad\": \"float\",\n",
      "        \"transaction_value_variance_in_cad\": \"float\",\n",
      "        \"variance_from_10k_cad\": \"float\",\n",
      "        \"min_day_to_max_day_range\": \"int\",\n",
      "        \"avg_transaction_frequency_in_days\": \"float\",\n",
      "        \"transaction_frequency_variance_in_days\": \"float\",\n",
      "        \"Is FanOut\": \"bool\"\n",
      "    },\n",
      "    \"train_num\": 104,\n",
      "    \"test_num\": 27\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "generate_json_config(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131, 38)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Dataset\n",
    "\n",
    "For more explanation of different steps in this section, please refer to TabDDPM's notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/\"\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw_data\")\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed_data\")\n",
    "SYNTH_DATA_DIR = os.path.join(DATA_DIR, \"synthetic_data\")\n",
    "DATA_NAME = \"adult\"\n",
    "\n",
    "MODEL_PATH = \"models/tabsyn\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXX(32561, 15)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/raw_data/adult/test.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# process data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m INFO_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_info\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mINFO_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/scripts/process_dataset.py:195\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(name, info_path, data_dir)\u001b[0m\n\u001b[1;32m    193\u001b[0m test_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_data_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/test.data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(test_save_path):\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_save_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f1:\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m    197\u001b[0m             save_line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/raw_data/adult/test.data'"
     ]
    }
   ],
   "source": [
    "# process data\n",
    "INFO_DIR = \"data_info\"\n",
    "process_data(DATA_NAME, INFO_DIR, DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFO_DIR = \"data_info\"\n",
    "# DONT NEED process_data(DATA_NAME, INFO_DIR, DATA_DIR)\n",
    "\n",
    "# review data\n",
    "df = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, DATA_NAME, \"train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review json file and its contents\n",
    "with open(f\"{PROCESSED_DATA_DIR}/{DATA_NAME}/info.json\", \"r\") as file:\n",
    "    data_info = json.load(file)\n",
    "# pprint(data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabSyn Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will describe the design of TabSyn as well as its main hyperparameters loaded through config, which affect the model’s effectiveness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TabSyn** consists of two parts:\n",
    "1. A *variational auto-encoder (VAE)* which learns a joint representation space for the given tabular data.\n",
    "2. A *Diffusion model* which learns the distribution of data in the joint representation space.\n",
    "\n",
    "The figure below shows a diagram of the TabSyn model.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn.jpg\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VAE**\n",
    "\n",
    "The left-side of the figure shows the VAE which operates in the original data space. The VAE itself consists of two parts: an encoder and a decoder. It also contains the corresponding tokenizer and detokenizer.\n",
    "Each row of the input tabular data ($\\pmb{x}$) is tokenized, then embedded by a transformer. Another transformer decodes the embeddings and a detokenizer reconstructs the table ($\\pmb{\\tilde{x}}$). The VAE is trained by minimizing the reconstruction loss between $\\pmb{x}$ and $\\pmb{\\tilde{x}}$.\n",
    "\n",
    "After the VAE is fully trained, the whole data ($\\pmb{x}$) is tokenized and embedded. The embedding of each row is flattened to form a 1-dimensional vector $\\pmb{z}$.\n",
    "These 1-dimensional embeddings for all rows are stored on disk, and will later be used to train the diffusion model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diffusion**\n",
    "\n",
    "The right-side of the figure shows the diffusion model which operates in the latent representation space; in other words, it only *sees* the embeddings obtained by the VAE, not the original tabular data.\n",
    "The diffusion model can be similarly divided into two parts: a forward process, and a reverse process.\n",
    "\n",
    "The forward process receives the embedded data points. A single data point is denoted by $\\pmb{z_0}$ in the figure. Gaussian noise is incrementally added to the embeddings in numerous incremental steps during the forward process. The number of the steps is denoted by $T$ in the figure. $T$ should be high enough that the distribution of embeddings at step $t=T$ is essentially a standard Gaussian distribution; in other words, the signal-to-noise ratio is practically zero.\n",
    "\n",
    "The reverse process, on the other hand, learns to *predict* an earlier-step embedding (e.g. $\\pmb{z_{t-\\Delta t}}$) from a later-step embedding (e.g. $\\pmb{z_t}$) via a neural network.\n",
    "\n",
    "After the diffusion model is fully trained, the reverse process can estimate the data distribution at step $t=0$ if it receives a standard Gaussian distribution at step $t=T$. New data points can be synthesized by sampling from this estimated distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will load the configuration file that contains the hyperparameters for the TabSyn model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(\"src/baselines/tabsyn/configs\", f\"{DATA_NAME}.toml\")\n",
    "raw_config = src.load_config(config_path)\n",
    "\n",
    "pprint(raw_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file is a TOML file that contains the following hyperparameters:\n",
    "\n",
    "1. **model_params:** specifies the structure of the transformers (both encoder and decoder) in the VAE model, including number of transformer layers, number of self-attnetion heads and token dimension.\n",
    "\n",
    "2. **transforms:** specifies the transformations and preprocessing of the data before tokenization, such as cleaning, normalization, and encoding.\n",
    "    - For preprocessing numerical features, we use the gaussian quantile transformation and replace the NaN values with mean of each row.\n",
    "    - For categorical features, we use the one-hot encoding method. NaN values are left unchanged, but we have the option to replace them. We have the option to drop the values that appear with less than a given minimum frequency under each column. Furthermore, we have the option to add an extra encoding step for categorical features during tokenization.\n",
    "\n",
    "3. **train.vae:** specifies training parameters of the VAE, including batch size, number of epochs, and number of dataset workers.\n",
    "\n",
    "4. **train.diffusion:** specifies the same training parameters as above for the diffusion model.\n",
    "\n",
    "5. **train.optim.vae:** specifies the parameters of the *Adam* optimizer and the `ReduceLROnPlateau` learning rate scheduler used to train the VAE. Optimizer parameters include initial learning rate and weight decay. LR scheduler parameters includer `factor` and `patience`.\n",
    "\n",
    "6. **train.optim.diffusion:** specifies the same parameters as above for the diffusion model.\n",
    "\n",
    "7. **loss_params:** specifies parameters of the loss function used to train the VAE including `max_beta`, `min_beta` and `lambd`.\n",
    "\n",
    "$\\beta$ is the coefficient of the KL divergence term in the VAE loss formula,\n",
    "\n",
    "$\\mathcal{L}_{vae} = \\mathcal{L}_{mse} + \\mathcal{L}_{ce} + \\beta \\mathcal{L}_{kl}$\n",
    ".\n",
    "\n",
    "Parameters `max_beta` and `min_beta` determine the range of $\\beta$. $\\beta$ is first set to `max_beta`. If the loss stops decreasing for a certain number of epochs (e.g. $10$ epochs), then at the end of each epoch after that (e.g. epoch $11$, $12$, etc.) $\\beta$ is decreased by a factor of `lambd`,\n",
    "$\\beta_{new} = \\lambda \\beta_{curr}$,\n",
    "until it reaches `beta_min`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we pre-process the data and make a dataset object.\n",
    "\n",
    "First, we determine transformations needed for the dataset, such as normalization and cleaning, in `transforms`. Next, using `preprocess` function we load the data from disk in arrays that contain both training and test data (`X_num` and `X_cat`), as well as the number of categories for each categorical feature (`categories`) and the number of numerical features (`d_numerical`).\n",
    "\n",
    "We then separate the train and test data in different arrays and convert them to Pytorch tensors.\n",
    "We create a dataset object (`TabularDataset`) with the train data. `TabularDataset` is a simple module which returns the tokens of a single row at a time. Each row constiutes a single data sample in TabSyn. Afterwards, we create a Dataloader for the train data using the `batch_size` and `num_workers` specified in config.\n",
    "\n",
    "In contrast, we keep the test data as tensors (`X_test_num` and `X_test_cat`). If a GPU is available, we move these tensors to GPU so that they can be accessed by the model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "X_num, X_cat, categories, d_numerical = preprocess(os.path.join(PROCESSED_DATA_DIR, DATA_NAME),\n",
    "                                                   transforms = raw_config[\"transforms\"],\n",
    "                                                   task_type = raw_config[\"task_type\"])\n",
    "\n",
    "# separate train and test data\n",
    "X_train_num, X_test_num = X_num\n",
    "X_train_cat, X_test_cat = X_cat\n",
    "\n",
    "# convert to float tensor\n",
    "X_train_num, X_test_num = torch.tensor(X_train_num).float(), torch.tensor(X_test_num).float()\n",
    "X_train_cat, X_test_cat =  torch.tensor(X_train_cat), torch.tensor(X_test_cat)\n",
    "\n",
    "# create dataset module\n",
    "train_data = TabularDataset(X_train_num.float(), X_train_cat)\n",
    "\n",
    "# move test data to gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_test_num = X_test_num.float().to(device)\n",
    "X_test_cat = X_test_cat.to(device)\n",
    "\n",
    "# create train dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size = raw_config[\"train\"][\"vae\"][\"batch_size\"],\n",
    "    shuffle = True,\n",
    "    num_workers = raw_config[\"train\"][\"vae\"][\"num_dataset_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the model using the `TabSyn` class. `TabSyn` class takes the following arguments:\n",
    "\n",
    "1. `train_loader`: dataloader for train data.\n",
    "2. `X_test_num`: numerical features of the test data.\n",
    "3. `X_test_cat`: categorical features of the train data.\n",
    "4. `num_numerical_features`: number of numerical features in the dataset.\n",
    "5. `num_classes`: number of classes (i.e. categories) of each categorical feature in the dataset.\n",
    "6. `device`: the device on which the model and data exist, either \"cpu\" or \"cuda\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabsyn = TabSyn(train_loader,\n",
    "                X_test_num, X_test_cat,\n",
    "                num_numerical_features = d_numerical,\n",
    "                num_classes = categories,\n",
    "                device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TabSyn` class has the tools to instantiate VAE and diffusion models, train both, and sample from the trained diffusion model.\n",
    "We will demonstrate how to use these tools in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE and the diffusion model are trained independently. The following subsections explain each training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### A. Train VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to instantiate the VAE using the `instantiate_vae` method. This method takes the VAE model hyperparameters, optimizer and lr scheduler parameters from config, and instantiates them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate VAE model for training\n",
    "tabsyn.instantiate_vae(**raw_config[\"model_params\"], optim_params = raw_config[\"train\"][\"optim\"][\"vae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have instantiated the VAE, we can train it using the `train_vae` function.\n",
    "This function receives the loss hyperparameters and number of epochs from the config.\n",
    "Moreover, it recieves `save_path` which is the directory where trained model checkpoints will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{MODEL_PATH}/{DATA_NAME}/vae\")\n",
    "tabsyn.train_vae(**raw_config[\"loss_params\"],\n",
    "                 num_epochs = raw_config[\"train\"][\"vae\"][\"num_epochs\"],\n",
    "                 save_path = os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the VAE, we embed the training data with the trained encoder and store the embeddings in a direcotry specified by `vae_ckpt_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed all inputs in the latent space\n",
    "tabsyn.save_vae_embeddings(X_train_num, X_train_cat,\n",
    "                           vae_ckpt_dir = os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Train Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have stored the training data embeddings, we need to load and prepare them for the diffusion model.\n",
    "We load the embeddings using `load_vae_embeddings`. We normalize the embeddings by subtracting the mean and dividing by the standard deviation. Then, we create a Dataloader with the specified `batch_size` and `num_workers` from the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load latent space embeddings\n",
    "train_z, _ = tabsyn.load_latent_embeddings(os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))  # train_z dim: B x in_dim\n",
    "\n",
    "# normalize embeddings\n",
    "mean, std = train_z.mean(0), train_z.std(0)\n",
    "train_z = (train_z - mean) / std\n",
    "latent_train_data = train_z\n",
    "\n",
    "# create data loader\n",
    "latent_train_loader = DataLoader(\n",
    "    latent_train_data,\n",
    "    batch_size = raw_config[\"train\"][\"diffusion\"][\"batch_size\"],\n",
    "    shuffle = True,\n",
    "    num_workers = raw_config[\"train\"][\"diffusion\"][\"num_dataset_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is ready, we instantiate the diffusion model with `instantiate_diffusion`. The input dimension and hidden dimention of the diffusion model is determined by the dimension of the embeddings. \n",
    "Moreover, we instantiate the optimizer and lr scheduler using hyperparameters from config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate diffusion model for training\n",
    "tabsyn.instantiate_diffusion(in_dim = train_z.shape[1], hid_dim = train_z.shape[1], optim_params = raw_config[\"train\"][\"optim\"][\"diffusion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the diffusion model with `train_diffusion` function.\n",
    "This function takes the following arguements:\n",
    "1. `latent_train_loader`: dataloader for the latent representations which are used to train the diffusion model.\n",
    "2. `num_epochs`: number of training epochs.\n",
    "3. `ckpt_path`: directory where the model checkpoints will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{MODEL_PATH}/{DATA_NAME}\")\n",
    "# train diffusion model\n",
    "tabsyn.train_diffusion(latent_train_loader,\n",
    "                       num_epochs = raw_config[\"train\"][\"diffusion\"][\"num_epochs\"],\n",
    "                       ckpt_path = os.path.join(MODEL_PATH, DATA_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training model from scratch, we can also load weights of a pre-trained model from a given checkpoint with `load_model_state` function.\n",
    "If we haven't instantiated the VAE and diffusion model beforehand, we need to instantiate them first using `instantiate_vae` and `instantiate_diffusion` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate VAE model\n",
    "tabsyn.instantiate_vae(**raw_config[\"model_params\"], optim_params = None)\n",
    "\n",
    "latent_embeddings_path = \"/projects/diffusion_bootcamp/models/tabular/tabsyn/default/vae\"\n",
    "# load latent embeddings of input data\n",
    "train_z, token_dim = tabsyn.load_latent_embeddings(latent_embeddings_path)\n",
    "\n",
    "# instantiate diffusion model\n",
    "tabsyn.instantiate_diffusion(in_dim = train_z.shape[1], hid_dim = train_z.shape[1], optim_params = None)\n",
    "\n",
    "pretrained_model_path = \"/projects/diffusion_bootcamp/models/tabular/tabsyn/default\"\n",
    "# load state from checkpoint\n",
    "tabsyn.load_model_state(ckpt_dir = pretrained_model_path,\n",
    "                        dif_ckpt_name = \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained the model effectively, using `sample` function we can generate synthetic data starting from compelete noise. The input of this function is as follows:\n",
    "\n",
    "1. `train_z`: latent embeddings of the training data.\n",
    "2. `info`: info about the data from the json file we reviewed at the beginning of this notebook.\n",
    "3. `num_inverse`: detokenizer for numerical features.\n",
    "4. `cat_inverse`: detokenizer for categorical features.\n",
    "5. `save_path`: file-path where the synthetic table will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data info file\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, DATA_NAME, \"info.json\"), \"r\") as file:\n",
    "    data_info = json.load(file)\n",
    "data_info[\"token_dim\"] = token_dim\n",
    "\n",
    "# get inverse tokenizers\n",
    "_, _, categories, d_numerical, num_inverse, cat_inverse = preprocess(os.path.join(PROCESSED_DATA_DIR, DATA_NAME),\n",
    "                                                                     transforms = raw_config[\"transforms\"],\n",
    "                                                                     task_type = raw_config[\"task_type\"],\n",
    "                                                                     inverse = True)\n",
    "\n",
    "# sample data\n",
    "num_samples = train_z.shape[0]\n",
    "in_dim = train_z.shape[1] \n",
    "mean_input_emb = train_z.mean(0)\n",
    "tabsyn.sample(num_samples,\n",
    "              in_dim,\n",
    "              mean_input_emb,\n",
    "              info = data_info,\n",
    "              num_inverse = num_inverse,\n",
    "              cat_inverse = cat_inverse,\n",
    "              save_path = os.path.join(SYNTH_DATA_DIR, DATA_NAME, \"tabsyn.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally here, we review the synthesized data. In the following `evaluate_synthetic_data.ipynb` notebook, we will evaluate this synthesized data with respect to various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(SYNTH_DATA_DIR, DATA_NAME, \"tabsyn.csv\"))\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "visualize_default(df).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zhang, Hengrui, et al.** \"Mixed-type tabular data synthesis with score-based diffusion in latent space.\" *International Conference on Learning Representations (ICLR)* (2023).\n",
    "\n",
    "**GitHub Repository:** [Amazon Science - Tabsyn](https://github.com/amazon-science/tabsyn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
