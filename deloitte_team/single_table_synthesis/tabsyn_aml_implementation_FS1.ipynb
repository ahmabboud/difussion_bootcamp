{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  \n",
    "# TABSYN: Tabular Data Synthesis with Diffusion Models\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two challenges regarding the extention of diffusion models to tabular data are:\n",
    "1. **Diverse data types:** a single table can have different columns each containing data of different types, including numerical, categorical, text, etc.\n",
    "2. **Varied distributions:** the distribution of data under different columns in a single table varry widely from column to column.\n",
    "\n",
    "**TabSyn** addresses these challenges by introducing a latent space where tabular data of all columns are jointly represented. It then proceedes to train a diffusion model on the latent representations.\n",
    "This tactic allows TabSyn to:\n",
    "1. Train a single diffusion model for all data types in the dataset (i.e. Generality).\n",
    "2. Optimize the distribution of latent embeddings to facilitate training of the subsequent diffusion model, thus generating higher quality synthetic data (i.e. Quality).\n",
    "3. Require much fewer reverse steps during training of the diffusion model, and synthesize data faster (i.e. Speed).\n",
    "\n",
    "In this notebook, we review and implement the TabSyn model. The notebook is organized as follows:\n",
    "\n",
    "1. [Imports and Setup]()\n",
    "\n",
    "\n",
    "2. [Default Dataset]()\n",
    "    \n",
    "    \n",
    "3. [TabSyn Algorithm]()\n",
    "    \n",
    "    3.1. [Load Config]()\n",
    "    \n",
    "    3.2. [Make Dataset]()\n",
    "    \n",
    "    3.3. [Instantiate Model]()\n",
    "    \n",
    "    3.4. [Train Model]()\n",
    "        \n",
    "    3.5. [Load Pretrained Model]()\n",
    "    \n",
    "    3.6. [Sample Data]()\n",
    "    \n",
    "    3.7. [Review Synthetic Data]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we import all necessary libraries and modules required for setting up the environment.\n",
    "Most of the libraries we need to implement TabSyn are the same as TabDDPM.\n",
    "We also specify `NAME_URL_DICT_UCI`, `DATA_NAME`, `DATA_DIR` and other paths as in TabDDPM's implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import src\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scripts.download_dataset import download_from_uci\n",
    "from scripts.process_dataset import process_data\n",
    "\n",
    "from src.data import preprocess, TabularDataset\n",
    "from src.baselines.tabsyn.pipeline import TabSyn\n",
    "\n",
    "from src.util import visualize_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/fs01/home/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('/fs01/home/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scripts.process_dataset import process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16561/1635508491.py:8: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, :16] = df.iloc[:, :16].astype(float)   # Destination Col\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/raw_data/IBM_AML_FeatureEngineered_FS1_raw.csv')\n",
    "df=df.dropna()\n",
    "# Ignore the The Patern ID and The Source columns\n",
    "df=df.iloc[::,2::]   \n",
    "# Convert the first Currency columns to boolean\n",
    "df.iloc[:, :16] = df.iloc[:, :16].astype(float)   # Destination Col\n",
    "df.iloc[:, 16:29] = df.iloc[:, 16:29].astype(bool).astype(str).replace({'True': 'Y', 'False': 'N'})\n",
    "# Convert the last column Is Fanout to boolean\n",
    "df.iloc[:, -1] = df.iloc[:, -1].astype(bool).astype(str).replace({'True': 'Y', 'False': 'N'})\n",
    "df.rename(columns={'Is FanOut': 'Is_FanOut'}, inplace=True)\n",
    "\n",
    "# df=df.iloc[:, ~df.columns.isin(df.columns[16:29])]  # Exclude Currency Columns for test purposes\n",
    "# df=df.iloc[:, ~df.columns.isin(df.columns[17:20])]  # Exclude erronous columns Columns for test purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6794 entries, 1 to 7393\n",
      "Data columns (total 37 columns):\n",
      " #   Column                                  Non-Null Count  Dtype  \n",
      "---  ------                                  --------------  -----  \n",
      " 0   Destination_1                           6794 non-null   float64\n",
      " 1   Destination_2                           6794 non-null   float64\n",
      " 2   Destination_3                           6794 non-null   float64\n",
      " 3   Destination_4                           6794 non-null   float64\n",
      " 4   Destination_5                           6794 non-null   float64\n",
      " 5   Destination_6                           6794 non-null   float64\n",
      " 6   Destination_7                           6794 non-null   float64\n",
      " 7   Destination_8                           6794 non-null   float64\n",
      " 8   Destination_9                           6794 non-null   float64\n",
      " 9   Destination_10                          6794 non-null   float64\n",
      " 10  Destination_11                          6794 non-null   float64\n",
      " 11  Destination_12                          6794 non-null   float64\n",
      " 12  Destination_13                          6794 non-null   float64\n",
      " 13  Destination_14                          6794 non-null   float64\n",
      " 14  Destination_15                          6794 non-null   float64\n",
      " 15  Destination_16                          6794 non-null   float64\n",
      " 16  US Dollar                               6794 non-null   object \n",
      " 17  Euro                                    6794 non-null   object \n",
      " 18  Yuan                                    6794 non-null   object \n",
      " 19  Rupee                                   6794 non-null   object \n",
      " 20  Ruble                                   6794 non-null   object \n",
      " 21  Canadian Dollar                         6794 non-null   object \n",
      " 22  Australian Dollar                       6794 non-null   object \n",
      " 23  Brazil Real                             6794 non-null   object \n",
      " 24  Swiss Franc                             6794 non-null   object \n",
      " 25  Shekel                                  6794 non-null   object \n",
      " 26  Bitcoin                                 6794 non-null   object \n",
      " 27  Yen                                     6794 non-null   object \n",
      " 28  UK Pound                                6794 non-null   object \n",
      " 29  number_transactions_above_9k_cad        6794 non-null   int64  \n",
      " 30  avg_transaction_value_in_cad            6794 non-null   float64\n",
      " 31  transaction_value_variance_in_cad       6794 non-null   float64\n",
      " 32  variance_from_10k_cad                   6794 non-null   float64\n",
      " 33  min_day_to_max_day_range                6794 non-null   int64  \n",
      " 34  avg_transaction_frequency_in_days       6794 non-null   float64\n",
      " 35  transaction_frequency_variance_in_days  6794 non-null   float64\n",
      " 36  Is_FanOut                               6794 non-null   object \n",
      "dtypes: float64(21), int64(2), object(14)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 14:38:35,709 - INFO - Unique values in target column: ['Y' 'N']\n",
      "2024-10-01 14:38:38,466 - INFO - Number of columns before selection: 36\n",
      "2024-10-01 14:38:38,468 - INFO - Number of columns after selection: 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance (sorted):\n",
      "transaction_frequency_variance_in_days: 0.6891\n",
      "Destination_1: 0.4032\n",
      "avg_transaction_frequency_in_days: 0.3777\n",
      "Destination_2: 0.3687\n",
      "min_day_to_max_day_range: 0.3426\n",
      "avg_transaction_value_in_cad: 0.3171\n",
      "Destination_3: 0.2376\n",
      "number_transactions_above_9k_cad: 0.1812\n",
      "Destination_4: 0.1666\n",
      "Destination_14: 0.1375\n",
      "Destination_15: 0.1340\n",
      "Destination_12: 0.1323\n",
      "Destination_5: 0.1266\n",
      "Destination_16: 0.1252\n",
      "Destination_11: 0.1251\n",
      "Destination_13: 0.1215\n",
      "Destination_10: 0.1192\n",
      "Destination_9: 0.1038\n",
      "Euro: 0.1022\n",
      "Destination_6: 0.1002\n",
      "Destination_8: 0.0932\n",
      "variance_from_10k_cad: 0.0925\n",
      "Destination_7: 0.0893\n",
      "US Dollar: 0.0882\n",
      "transaction_value_variance_in_cad: 0.0673\n",
      "Yuan: 0.0583\n",
      "Bitcoin: 0.0169\n",
      "Shekel: 0.0162\n",
      "Yen: 0.0132\n",
      "Rupee: 0.0082\n",
      "UK Pound: 0.0071\n",
      "Ruble: 0.0071\n",
      "Canadian Dollar: 0.0000\n",
      "Australian Dollar: 0.0000\n",
      "Brazil Real: 0.0000\n",
      "Swiss Franc: 0.0000\n",
      "Important columns: ['Destination_1', 'Destination_2', 'Destination_3', 'Destination_4', 'Destination_5', 'Destination_6', 'Destination_7', 'Destination_8', 'Destination_9', 'Destination_10', 'Destination_11', 'Destination_12', 'Destination_13', 'Destination_14', 'Destination_15', 'Destination_16', 'US Dollar', 'Euro', 'Yuan', 'number_transactions_above_9k_cad', 'avg_transaction_value_in_cad', 'transaction_value_variance_in_cad', 'variance_from_10k_cad', 'min_day_to_max_day_range', 'avg_transaction_frequency_in_days', 'transaction_frequency_variance_in_days']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import logging\n",
    "\n",
    "def select_important_columns(df, target_column, threshold, random_state=42):\n",
    "    \"\"\"\n",
    "    Selects the most important columns for binary classification based on mutual information.\n",
    "    Handles categorical features by encoding them and prints feature importance in sorted order.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame containing features and target variable.\n",
    "    target_column (str): The name of the target variable column in the DataFrame.\n",
    "    threshold (float): The minimum mutual information score for a feature to be considered important.\n",
    "    random_state (int, optional): Random state for reproducibility. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of column names that are deemed important for classification.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    unique_values = y.unique()\n",
    "    if len(unique_values) != 2:\n",
    "        logger.warning(f\"Target column '{target_column}' is not binary. It has {len(unique_values)} unique values.\")\n",
    "        return []\n",
    "\n",
    "    logger.info(f\"Unique values in target column: {unique_values}\")\n",
    "\n",
    "    # Apply label encoding to the target variable\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    # Preprocess features\n",
    "    X_processed = X.copy()\n",
    "    for column in X_processed.columns:\n",
    "        if X_processed[column].dtype == 'object':\n",
    "            # For categorical columns, use label encoding\n",
    "            X_processed[column] = LabelEncoder().fit_transform(X_processed[column].astype(str))\n",
    "        else:\n",
    "            # For numeric columns, fill NaN values with the mean\n",
    "            imputer = SimpleImputer(strategy='mean')\n",
    "            X_processed[column] = imputer.fit_transform(X_processed[[column]])\n",
    "\n",
    "    # Calculate mutual information scores\n",
    "    mi_scores = mutual_info_classif(X_processed, y_encoded, random_state=random_state)\n",
    "\n",
    "    # Create a dictionary of feature names and their mutual information scores\n",
    "    feature_scores = dict(zip(X.columns, mi_scores))\n",
    "\n",
    "    # Sort features by importance\n",
    "    sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print feature importance in sorted order\n",
    "    print(\"\\nFeature Importance (sorted):\")\n",
    "    for feature, score in sorted_features:\n",
    "        print(f\"{feature}: {score:.4f}\")\n",
    "\n",
    "    important_features = [feature for feature, score in feature_scores.items() if score > threshold]\n",
    "\n",
    "    logger.info(f\"Number of columns before selection: {len(X.columns)}\")\n",
    "    logger.info(f\"Number of columns after selection: {len(important_features)}\")\n",
    "\n",
    "    if not important_features:\n",
    "        logger.warning(\"No features met the threshold criteria. Consider lowering the threshold.\")\n",
    "\n",
    "    return important_features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "important_cols = select_important_columns(df=df, target_column='Is_FanOut', threshold=0.05)\n",
    "print(\"Important columns:\", important_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only important columns\n",
    "df= df[important_cols + ['Is_FanOut']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Destination_1</th>\n",
       "      <th>Destination_2</th>\n",
       "      <th>Destination_3</th>\n",
       "      <th>Destination_4</th>\n",
       "      <th>Destination_5</th>\n",
       "      <th>Destination_6</th>\n",
       "      <th>Destination_7</th>\n",
       "      <th>Destination_8</th>\n",
       "      <th>Destination_9</th>\n",
       "      <th>Destination_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Euro</th>\n",
       "      <th>Yuan</th>\n",
       "      <th>number_transactions_above_9k_cad</th>\n",
       "      <th>avg_transaction_value_in_cad</th>\n",
       "      <th>transaction_value_variance_in_cad</th>\n",
       "      <th>variance_from_10k_cad</th>\n",
       "      <th>min_day_to_max_day_range</th>\n",
       "      <th>avg_transaction_frequency_in_days</th>\n",
       "      <th>transaction_frequency_variance_in_days</th>\n",
       "      <th>Is_FanOut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12748.0</td>\n",
       "      <td>11218.0</td>\n",
       "      <td>11161.0</td>\n",
       "      <td>6452.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>3</td>\n",
       "      <td>1.039523e+04</td>\n",
       "      <td>7.449777e+06</td>\n",
       "      <td>2.297416e+07</td>\n",
       "      <td>13</td>\n",
       "      <td>3.25</td>\n",
       "      <td>7.849218</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>238562565.0</td>\n",
       "      <td>11577.0</td>\n",
       "      <td>8583.0</td>\n",
       "      <td>2453.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>2</td>\n",
       "      <td>5.964630e+07</td>\n",
       "      <td>1.422713e+16</td>\n",
       "      <td>5.690733e+16</td>\n",
       "      <td>12</td>\n",
       "      <td>3.00</td>\n",
       "      <td>7.849218</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23108.0</td>\n",
       "      <td>13930.0</td>\n",
       "      <td>7568.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>2</td>\n",
       "      <td>1.486891e+04</td>\n",
       "      <td>6.103391e+07</td>\n",
       "      <td>1.931867e+08</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.849218</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8692.0</td>\n",
       "      <td>4644.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>6.668545e+03</td>\n",
       "      <td>8.196547e+06</td>\n",
       "      <td>3.039373e+07</td>\n",
       "      <td>10</td>\n",
       "      <td>5.00</td>\n",
       "      <td>7.849218</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26771.0</td>\n",
       "      <td>19889.0</td>\n",
       "      <td>5768.0</td>\n",
       "      <td>1954.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>2</td>\n",
       "      <td>1.359592e+04</td>\n",
       "      <td>1.366655e+08</td>\n",
       "      <td>4.617190e+08</td>\n",
       "      <td>13</td>\n",
       "      <td>3.25</td>\n",
       "      <td>7.849218</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Destination_1  Destination_2  Destination_3  Destination_4  Destination_5  \\\n",
       "1        12748.0        11218.0        11161.0         6452.0            0.0   \n",
       "2    238562565.0        11577.0         8583.0         2453.0            0.0   \n",
       "3        23108.0        13930.0         7568.0            0.0            0.0   \n",
       "4         8692.0         4644.0            0.0            0.0            0.0   \n",
       "5        26771.0        19889.0         5768.0         1954.0            0.0   \n",
       "\n",
       "   Destination_6  Destination_7  Destination_8  Destination_9  Destination_10  \\\n",
       "1            0.0            0.0            0.0            0.0             0.0   \n",
       "2            0.0            0.0            0.0            0.0             0.0   \n",
       "3            0.0            0.0            0.0            0.0             0.0   \n",
       "4            0.0            0.0            0.0            0.0             0.0   \n",
       "5            0.0            0.0            0.0            0.0             0.0   \n",
       "\n",
       "   ...  Euro  Yuan  number_transactions_above_9k_cad  \\\n",
       "1  ...     Y     N                                 3   \n",
       "2  ...     Y     Y                                 2   \n",
       "3  ...     Y     Y                                 2   \n",
       "4  ...     Y     N                                 0   \n",
       "5  ...     Y     N                                 2   \n",
       "\n",
       "   avg_transaction_value_in_cad  transaction_value_variance_in_cad  \\\n",
       "1                  1.039523e+04                       7.449777e+06   \n",
       "2                  5.964630e+07                       1.422713e+16   \n",
       "3                  1.486891e+04                       6.103391e+07   \n",
       "4                  6.668545e+03                       8.196547e+06   \n",
       "5                  1.359592e+04                       1.366655e+08   \n",
       "\n",
       "   variance_from_10k_cad min_day_to_max_day_range  \\\n",
       "1           2.297416e+07                       13   \n",
       "2           5.690733e+16                       12   \n",
       "3           1.931867e+08                        3   \n",
       "4           3.039373e+07                       10   \n",
       "5           4.617190e+08                       13   \n",
       "\n",
       "  avg_transaction_frequency_in_days transaction_frequency_variance_in_days  \\\n",
       "1                              3.25                               7.849218   \n",
       "2                              3.00                               7.849218   \n",
       "3                              1.00                               7.849218   \n",
       "4                              5.00                               7.849218   \n",
       "5                              3.25                               7.849218   \n",
       "\n",
       "   Is_FanOut  \n",
       "1          Y  \n",
       "2          Y  \n",
       "3          Y  \n",
       "4          Y  \n",
       "5          Y  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Dataset\n",
    "df.to_csv('data/raw_data/IBM_AML_FeatureEngineered_FS1/IBM_AML_FeatureEngineered_FS1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'column_info': {'Destination_1': 'float', 'Destination_2': 'float', 'Destination_3': 'float', 'Destination_4': 'float', 'Destination_5': 'float', 'Destination_6': 'float', 'Destination_7': 'float', 'Destination_8': 'float', 'Destination_9': 'float', 'Destination_10': 'float', 'Destination_11': 'float', 'Destination_12': 'float', 'Destination_13': 'float', 'Destination_14': 'float', 'Destination_15': 'float', 'Destination_16': 'float', 'US Dollar': 'str', 'Euro': 'str', 'Yuan': 'str', 'number_transactions_above_9k_cad': 'int', 'avg_transaction_value_in_cad': 'float', 'transaction_value_variance_in_cad': 'float', 'variance_from_10k_cad': 'float', 'min_day_to_max_day_range': 'int', 'avg_transaction_frequency_in_days': 'float', 'transaction_frequency_variance_in_days': 'float', 'Is_FanOut': 'str'}}\n"
     ]
    }
   ],
   "source": [
    "def get_column_info(df):\n",
    "    column_info = {col: str(df[col].dtype).replace('float64', 'float').replace('int64', 'int').replace('object', 'str') for col in df.columns}\n",
    "    return column_info\n",
    "\n",
    "# Get column info\n",
    "column_info = get_column_info(df)\n",
    "\n",
    "# Print in desired format\n",
    "print({\"column_info\": column_info})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def generate_json_config(df):\n",
    "    # Get column names\n",
    "    column_names = df.columns.tolist()\n",
    "    \n",
    "    # Identify the target column index (last column)\n",
    "    target_col_idx = [len(column_names) - 1]\n",
    "    \n",
    "    # Initialize lists for numerical and categorical column indices\n",
    "    num_col_idx = []\n",
    "    cat_col_idx = []\n",
    "    \n",
    "    # Initialize dictionary for column info\n",
    "    column_info = {}\n",
    "    \n",
    "    # Determine column types and indices\n",
    "    for idx, col in enumerate(df.columns):\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        # Fill in column_info with appropriate type\n",
    "        if pd.api.types.is_integer_dtype(col_type):\n",
    "            column_info[col] = \"int\"\n",
    "            if idx != target_col_idx[0]:  # Exclude target column from num_col_idx\n",
    "                num_col_idx.append(idx)\n",
    "        elif pd.api.types.is_float_dtype(col_type):\n",
    "            column_info[col] = \"float\"\n",
    "            if idx != target_col_idx[0]:\n",
    "                num_col_idx.append(idx)\n",
    "        elif pd.api.types.is_bool_dtype(col_type):\n",
    "            column_info[col] = \"bool\"\n",
    "            if idx != target_col_idx[0]:\n",
    "                cat_col_idx.append(idx)\n",
    "        else:\n",
    "            column_info[col] = \"str\"\n",
    "            if idx != target_col_idx[0]: # Exclude target column from cat_col_idx_col_idx\n",
    "                cat_col_idx.append(idx)\n",
    "    \n",
    "    # Calculate train_num and test_num\n",
    "    total_rows = len(df)\n",
    "    train_num = int(total_rows * 1.0)\n",
    "    test_num = total_rows - train_num\n",
    "    \n",
    "    # Construct JSON configuration object\n",
    "    config = {\n",
    "        \"name\": \"IBM_AML_FeatureEngineered_FS1\",\n",
    "        \"task_type\": \"binclass\",\n",
    "        \"header\": \"infer\",\n",
    "        \"column_names\": column_names,\n",
    "        \"num_col_idx\": num_col_idx,\n",
    "        \"cat_col_idx\": cat_col_idx,\n",
    "        \"target_col_idx\": target_col_idx,\n",
    "        \"file_type\": \"csv\",\n",
    "        \"data_path\": \"data/raw_data/IBM_AML_FeatureEngineered_FS1/IBM_AML_FeatureEngineered_FS1.csv\",\n",
    "        \"test_path\":\"data/test_data/IBM_AML_FeatureEngineered_FS1/test.csv\", \n",
    "        \"column_info\": column_info,\n",
    "        \"train_num\": train_num,\n",
    "        \"test_num\": test_num\n",
    "    }\n",
    "    \n",
    "    # Print JSON object\n",
    "    print(json.dumps(config, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"IBM_AML_FeatureEngineered_FS1\",\n",
      "    \"task_type\": \"binclass\",\n",
      "    \"header\": \"infer\",\n",
      "    \"column_names\": [\n",
      "        \"Destination_1\",\n",
      "        \"Destination_2\",\n",
      "        \"Destination_3\",\n",
      "        \"Destination_4\",\n",
      "        \"Destination_5\",\n",
      "        \"Destination_6\",\n",
      "        \"Destination_7\",\n",
      "        \"Destination_8\",\n",
      "        \"Destination_9\",\n",
      "        \"Destination_10\",\n",
      "        \"Destination_11\",\n",
      "        \"Destination_12\",\n",
      "        \"Destination_13\",\n",
      "        \"Destination_14\",\n",
      "        \"Destination_15\",\n",
      "        \"Destination_16\",\n",
      "        \"US Dollar\",\n",
      "        \"Euro\",\n",
      "        \"Yuan\",\n",
      "        \"number_transactions_above_9k_cad\",\n",
      "        \"avg_transaction_value_in_cad\",\n",
      "        \"transaction_value_variance_in_cad\",\n",
      "        \"variance_from_10k_cad\",\n",
      "        \"min_day_to_max_day_range\",\n",
      "        \"avg_transaction_frequency_in_days\",\n",
      "        \"transaction_frequency_variance_in_days\",\n",
      "        \"Is_FanOut\"\n",
      "    ],\n",
      "    \"num_col_idx\": [\n",
      "        0,\n",
      "        1,\n",
      "        2,\n",
      "        3,\n",
      "        4,\n",
      "        5,\n",
      "        6,\n",
      "        7,\n",
      "        8,\n",
      "        9,\n",
      "        10,\n",
      "        11,\n",
      "        12,\n",
      "        13,\n",
      "        14,\n",
      "        15,\n",
      "        19,\n",
      "        20,\n",
      "        21,\n",
      "        22,\n",
      "        23,\n",
      "        24,\n",
      "        25\n",
      "    ],\n",
      "    \"cat_col_idx\": [\n",
      "        16,\n",
      "        17,\n",
      "        18\n",
      "    ],\n",
      "    \"target_col_idx\": [\n",
      "        26\n",
      "    ],\n",
      "    \"file_type\": \"csv\",\n",
      "    \"data_path\": \"data/raw_data/IBM_AML_FeatureEngineered_FS1/IBM_AML_FeatureEngineered_FS1.csv\",\n",
      "    \"test_path\": \"data/test_data/IBM_AML_FeatureEngineered_FS1/test.csv\",\n",
      "    \"column_info\": {\n",
      "        \"Destination_1\": \"float\",\n",
      "        \"Destination_2\": \"float\",\n",
      "        \"Destination_3\": \"float\",\n",
      "        \"Destination_4\": \"float\",\n",
      "        \"Destination_5\": \"float\",\n",
      "        \"Destination_6\": \"float\",\n",
      "        \"Destination_7\": \"float\",\n",
      "        \"Destination_8\": \"float\",\n",
      "        \"Destination_9\": \"float\",\n",
      "        \"Destination_10\": \"float\",\n",
      "        \"Destination_11\": \"float\",\n",
      "        \"Destination_12\": \"float\",\n",
      "        \"Destination_13\": \"float\",\n",
      "        \"Destination_14\": \"float\",\n",
      "        \"Destination_15\": \"float\",\n",
      "        \"Destination_16\": \"float\",\n",
      "        \"US Dollar\": \"str\",\n",
      "        \"Euro\": \"str\",\n",
      "        \"Yuan\": \"str\",\n",
      "        \"number_transactions_above_9k_cad\": \"int\",\n",
      "        \"avg_transaction_value_in_cad\": \"float\",\n",
      "        \"transaction_value_variance_in_cad\": \"float\",\n",
      "        \"variance_from_10k_cad\": \"float\",\n",
      "        \"min_day_to_max_day_range\": \"int\",\n",
      "        \"avg_transaction_frequency_in_days\": \"float\",\n",
      "        \"transaction_frequency_variance_in_days\": \"float\",\n",
      "        \"Is_FanOut\": \"str\"\n",
      "    },\n",
      "    \"train_num\": 6794,\n",
      "    \"test_num\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "generate_json_config(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Dataset\n",
    "\n",
    "For more explanation of different steps in this section, please refer to TabDDPM's notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/\"\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw_data\")\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed_data\")\n",
    "SYNTH_DATA_DIR = os.path.join(DATA_DIR, \"synthetic_data\")\n",
    "DATA_NAME = \"IBM_AML_FeatureEngineered_FS1\"\n",
    "\n",
    "MODEL_PATH = \"models/tabsyn\"\n",
    "# process data\n",
    "INFO_DIR = \"data_info\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (90, 37)\n",
      "Train dataset shape: (81, 37)\n",
      "Test dataset shape: (9, 37)\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# def split_and_save_dataset(df, train_size=0.9, random_state=42):\n",
    "#     # Split the dataset\n",
    "#     train_df, test_df = train_test_split(df, train_size=train_size, random_state=random_state)\n",
    "    \n",
    "#     # Save to CSV files\n",
    "#     train_df.to_csv('data/raw_data/IBM_AML_FeatureEngineered_FS1/train.csv', index=False)\n",
    "#     test_df.to_csv('data/test_data/IBM_AML_FeatureEngineered_FS1/test.csv', index=False)\n",
    "    \n",
    "#     # Print the shapes of the resulting datasets\n",
    "#     print(f\"Original dataset shape: {df.shape}\")\n",
    "#     print(f\"Train dataset shape: {train_df.shape}\")\n",
    "#     print(f\"Test dataset shape: {test_df.shape}\")\n",
    "\n",
    "# # Assuming you have a DataFrame called 'df'\n",
    "# split_and_save_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME_URL_DICT_UCI = {\n",
    "#     \"adult\": \"https://archive.ics.uci.edu/static/public/2/adult.zip\",\n",
    "#     \"default\": \"https://archive.ics.uci.edu/static/public/350/default+of+credit+card+clients.zip\",\n",
    "#     \"magic\": \"https://archive.ics.uci.edu/static/public/159/magic+gamma+telescope.zip\",\n",
    "#     \"shoppers\": \"https://archive.ics.uci.edu/static/public/468/online+shoppers+purchasing+intention+dataset.zip\",\n",
    "#     \"beijing\": \"https://archive.ics.uci.edu/static/public/381/beijing+pm2+5+data.zip\",\n",
    "#     \"news\": \"https://archive.ics.uci.edu/static/public/332/online+news+popularity.zip\",\n",
    "# }\n",
    "\n",
    "# # For shared directory you can change it to \"/projects/diffusion_bootcamp/data/tabular\"\n",
    "# DATA_DIR = \"data/\"\n",
    "# RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw_data\")\n",
    "# PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed_data\")\n",
    "# SYNTH_DATA_DIR = os.path.join(DATA_DIR, \"synthetic_data\")\n",
    "# # DATA_NAME = \"adult\"\n",
    "\n",
    "# MODEL_PATH = \"models/tabsyn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_NAME = \"IBM_AML_FeatureEngineered_FS1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXX(6794, 27)\n",
      "num Col index:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 19, 20, 21, 22, 23, 24, 25], Cat col index: [16, 17, 18]\n",
      "Processing and Saving IBM_AML_FeatureEngineered_FS1 Successfully!\n",
      "Dataset Name: IBM_AML_FeatureEngineered_FS1\n",
      "Total Size: 6794\n",
      "Train Size: 6114\n",
      "Test Size: 680\n",
      "Number of Numerical Columns: 23\n",
      "Number of Categorical Columns: 4\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "# download_from_uci(DATA_NAME, RAW_DATA_DIR, NAME_URL_DICT_UCI)\n",
    "\n",
    "# process data\n",
    "INFO_DIR = \"data_info\"\n",
    "process_data(DATA_NAME, INFO_DIR, DATA_DIR)\n",
    "\n",
    "# review data\n",
    "df = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, DATA_NAME, \"train.csv\"))\n",
    "# visualize_default(df).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat_col_idx': [16, 17, 18],\n",
      " 'column_info': {'0': {},\n",
      "                 '1': {},\n",
      "                 '10': {},\n",
      "                 '11': {},\n",
      "                 '12': {},\n",
      "                 '13': {},\n",
      "                 '14': {},\n",
      "                 '15': {},\n",
      "                 '16': {},\n",
      "                 '17': {},\n",
      "                 '18': {},\n",
      "                 '19': {},\n",
      "                 '2': {},\n",
      "                 '20': {},\n",
      "                 '21': {},\n",
      "                 '22': {},\n",
      "                 '23': {},\n",
      "                 '24': {},\n",
      "                 '25': {},\n",
      "                 '26': {},\n",
      "                 '3': {},\n",
      "                 '4': {},\n",
      "                 '5': {},\n",
      "                 '6': {},\n",
      "                 '7': {},\n",
      "                 '8': {},\n",
      "                 '9': {},\n",
      "                 'categorizes': ['Y', 'N'],\n",
      "                 'max': 7.849218007657018,\n",
      "                 'min': 1.6115712240084643,\n",
      "                 'type': 'categorical'},\n",
      " 'column_names': ['Destination_1',\n",
      "                  'Destination_2',\n",
      "                  'Destination_3',\n",
      "                  'Destination_4',\n",
      "                  'Destination_5',\n",
      "                  'Destination_6',\n",
      "                  'Destination_7',\n",
      "                  'Destination_8',\n",
      "                  'Destination_9',\n",
      "                  'Destination_10',\n",
      "                  'Destination_11',\n",
      "                  'Destination_12',\n",
      "                  'Destination_13',\n",
      "                  'Destination_14',\n",
      "                  'Destination_15',\n",
      "                  'Destination_16',\n",
      "                  'US Dollar',\n",
      "                  'Euro',\n",
      "                  'Yuan',\n",
      "                  'number_transactions_above_9k_cad',\n",
      "                  'avg_transaction_value_in_cad',\n",
      "                  'transaction_value_variance_in_cad',\n",
      "                  'variance_from_10k_cad',\n",
      "                  'min_day_to_max_day_range',\n",
      "                  'avg_transaction_frequency_in_days',\n",
      "                  'transaction_frequency_variance_in_days',\n",
      "                  'Is_FanOut'],\n",
      " 'data_path': 'data/raw_data/IBM_AML_FeatureEngineered_FS1/IBM_AML_FeatureEngineered_FS1.csv',\n",
      " 'file_type': 'csv',\n",
      " 'header': 'infer',\n",
      " 'idx_mapping': {'0': 0,\n",
      "                 '1': 1,\n",
      "                 '10': 10,\n",
      "                 '11': 11,\n",
      "                 '12': 12,\n",
      "                 '13': 13,\n",
      "                 '14': 14,\n",
      "                 '15': 15,\n",
      "                 '16': 23,\n",
      "                 '17': 24,\n",
      "                 '18': 25,\n",
      "                 '19': 16,\n",
      "                 '2': 2,\n",
      "                 '20': 17,\n",
      "                 '21': 18,\n",
      "                 '22': 19,\n",
      "                 '23': 20,\n",
      "                 '24': 21,\n",
      "                 '25': 22,\n",
      "                 '26': 26,\n",
      "                 '3': 3,\n",
      "                 '4': 4,\n",
      "                 '5': 5,\n",
      "                 '6': 6,\n",
      "                 '7': 7,\n",
      "                 '8': 8,\n",
      "                 '9': 9},\n",
      " 'idx_name_mapping': {'0': 'Destination_1',\n",
      "                      '1': 'Destination_2',\n",
      "                      '10': 'Destination_11',\n",
      "                      '11': 'Destination_12',\n",
      "                      '12': 'Destination_13',\n",
      "                      '13': 'Destination_14',\n",
      "                      '14': 'Destination_15',\n",
      "                      '15': 'Destination_16',\n",
      "                      '16': 'US Dollar',\n",
      "                      '17': 'Euro',\n",
      "                      '18': 'Yuan',\n",
      "                      '19': 'number_transactions_above_9k_cad',\n",
      "                      '2': 'Destination_3',\n",
      "                      '20': 'avg_transaction_value_in_cad',\n",
      "                      '21': 'transaction_value_variance_in_cad',\n",
      "                      '22': 'variance_from_10k_cad',\n",
      "                      '23': 'min_day_to_max_day_range',\n",
      "                      '24': 'avg_transaction_frequency_in_days',\n",
      "                      '25': 'transaction_frequency_variance_in_days',\n",
      "                      '26': 'Is_FanOut',\n",
      "                      '3': 'Destination_4',\n",
      "                      '4': 'Destination_5',\n",
      "                      '5': 'Destination_6',\n",
      "                      '6': 'Destination_7',\n",
      "                      '7': 'Destination_8',\n",
      "                      '8': 'Destination_9',\n",
      "                      '9': 'Destination_10'},\n",
      " 'inverse_idx_mapping': {'0': 0,\n",
      "                         '1': 1,\n",
      "                         '10': 10,\n",
      "                         '11': 11,\n",
      "                         '12': 12,\n",
      "                         '13': 13,\n",
      "                         '14': 14,\n",
      "                         '15': 15,\n",
      "                         '16': 19,\n",
      "                         '17': 20,\n",
      "                         '18': 21,\n",
      "                         '19': 22,\n",
      "                         '2': 2,\n",
      "                         '20': 23,\n",
      "                         '21': 24,\n",
      "                         '22': 25,\n",
      "                         '23': 16,\n",
      "                         '24': 17,\n",
      "                         '25': 18,\n",
      "                         '26': 26,\n",
      "                         '3': 3,\n",
      "                         '4': 4,\n",
      "                         '5': 5,\n",
      "                         '6': 6,\n",
      "                         '7': 7,\n",
      "                         '8': 8,\n",
      "                         '9': 9},\n",
      " 'metadata': {'columns': {'0': {'computer_representation': 'Float',\n",
      "                                'sdtype': 'numerical'},\n",
      "                          '1': {'computer_representation': 'Float',\n",
      "                                'sdtype': 'numerical'},\n",
      "                          '10': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '11': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '12': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '13': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '14': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '15': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '16': {'sdtype': 'categorical'},\n",
      "                          '17': {'sdtype': 'categorical'},\n",
      "                          '18': {'sdtype': 'categorical'},\n",
      "                          '19': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '2': {'computer_representation': 'Float',\n",
      "                                'sdtype': 'numerical'},\n",
      "                          '20': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '21': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '22': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '23': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '24': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '25': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '26': {'sdtype': 'categorical'},\n",
      "                          '3': {'computer_representation': 'Float',\n",
      "                                'sdtype': 'numerical'},\n",
      "                          '4': {'computer_representation': 'Float',\n",
      "                                'sdtype': 'numerical'},\n",
      "                          '5': {'computer_representation': 'Float',\n",
      "                                'sdtype': 'numerical'},\n",
      "                          '6': {'computer_representation': 'Float',\n",
      "                                'sdtype': 'numerical'},\n",
      "                          '7': {'computer_representation': 'Float',\n",
      "                                'sdtype': 'numerical'},\n",
      "                          '8': {'computer_representation': 'Float',\n",
      "                                'sdtype': 'numerical'},\n",
      "                          '9': {'computer_representation': 'Float',\n",
      "                                'sdtype': 'numerical'}}},\n",
      " 'name': 'IBM_AML_FeatureEngineered_FS1',\n",
      " 'num_col_idx': [0,\n",
      "                 1,\n",
      "                 2,\n",
      "                 3,\n",
      "                 4,\n",
      "                 5,\n",
      "                 6,\n",
      "                 7,\n",
      "                 8,\n",
      "                 9,\n",
      "                 10,\n",
      "                 11,\n",
      "                 12,\n",
      "                 13,\n",
      "                 14,\n",
      "                 15,\n",
      "                 19,\n",
      "                 20,\n",
      "                 21,\n",
      "                 22,\n",
      "                 23,\n",
      "                 24,\n",
      "                 25],\n",
      " 'target_col_idx': [26],\n",
      " 'task_type': 'binclass',\n",
      " 'test_num': 680,\n",
      " 'test_path': None,\n",
      " 'train_num': 6114}\n"
     ]
    }
   ],
   "source": [
    "# review json file and its contents\n",
    "with open(f\"{PROCESSED_DATA_DIR}/{DATA_NAME}/info.json\", \"r\") as file:\n",
    "    data_info = json.load(file)\n",
    "pprint(data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabSyn Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will describe the design of TabSyn as well as its main hyperparameters loaded through config, which affect the modelâ€™s effectiveness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TabSyn** consists of two parts:\n",
    "1. A *variational auto-encoder (VAE)* which learns a joint representation space for the given tabular data.\n",
    "2. A *Diffusion model* which learns the distribution of data in the joint representation space.\n",
    "\n",
    "The figure below shows a diagram of the TabSyn model.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn.jpg\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VAE**\n",
    "\n",
    "The left-side of the figure shows the VAE which operates in the original data space. The VAE itself consists of two parts: an encoder and a decoder. It also contains the corresponding tokenizer and detokenizer.\n",
    "Each row of the input tabular data ($\\pmb{x}$) is tokenized, then embedded by a transformer. Another transformer decodes the embeddings and a detokenizer reconstructs the table ($\\pmb{\\tilde{x}}$). The VAE is trained by minimizing the reconstruction loss between $\\pmb{x}$ and $\\pmb{\\tilde{x}}$.\n",
    "\n",
    "After the VAE is fully trained, the whole data ($\\pmb{x}$) is tokenized and embedded. The embedding of each row is flattened to form a 1-dimensional vector $\\pmb{z}$.\n",
    "These 1-dimensional embeddings for all rows are stored on disk, and will later be used to train the diffusion model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diffusion**\n",
    "\n",
    "The right-side of the figure shows the diffusion model which operates in the latent representation space; in other words, it only *sees* the embeddings obtained by the VAE, not the original tabular data.\n",
    "The diffusion model can be similarly divided into two parts: a forward process, and a reverse process.\n",
    "\n",
    "The forward process receives the embedded data points. A single data point is denoted by $\\pmb{z_0}$ in the figure. Gaussian noise is incrementally added to the embeddings in numerous incremental steps during the forward process. The number of the steps is denoted by $T$ in the figure. $T$ should be high enough that the distribution of embeddings at step $t=T$ is essentially a standard Gaussian distribution; in other words, the signal-to-noise ratio is practically zero.\n",
    "\n",
    "The reverse process, on the other hand, learns to *predict* an earlier-step embedding (e.g. $\\pmb{z_{t-\\Delta t}}$) from a later-step embedding (e.g. $\\pmb{z_t}$) via a neural network.\n",
    "\n",
    "After the diffusion model is fully trained, the reverse process can estimate the data distribution at step $t=0$ if it receives a standard Gaussian distribution at step $t=T$. New data points can be synthesized by sampling from this estimated distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will load the configuration file that contains the hyperparameters for the TabSyn model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'impute': {'N': 20,\n",
      "            'SIGMA_MAX': 80,\n",
      "            'SIGMA_MIN': 0.002,\n",
      "            'S_churn': 1,\n",
      "            'S_max': inf,\n",
      "            'S_min': 0,\n",
      "            'S_noise': 1,\n",
      "            'num_steps': 30,\n",
      "            'num_trials': 30,\n",
      "            'rho': 7},\n",
      " 'loss_params': {'lambd': 0.7, 'max_beta': 0.01, 'min_beta': 1e-05},\n",
      " 'model_params': {'d_token': 4, 'factor': 32, 'n_head': 1, 'num_layers': 2},\n",
      " 'task_type': 'binclass',\n",
      " 'train': {'diffusion': {'batch_size': 4096,\n",
      "                         'num_dataset_workers': 4,\n",
      "                         'num_epochs': 9},\n",
      "           'optim': {'diffusion': {'factor': 0.9,\n",
      "                                   'lr': 0.001,\n",
      "                                   'patience': 20,\n",
      "                                   'weight_decay': 0},\n",
      "                     'vae': {'factor': 0.95,\n",
      "                             'lr': 0.001,\n",
      "                             'patience': 10,\n",
      "                             'weight_decay': 0}},\n",
      "           'vae': {'batch_size': 4096,\n",
      "                   'num_dataset_workers': 4,\n",
      "                   'num_epochs': 10}},\n",
      " 'transforms': {'cat_encoding': None,\n",
      "                'cat_min_frequency': None,\n",
      "                'cat_nan_policy': None,\n",
      "                'normalization': 'quantile',\n",
      "                'num_nan_policy': 'mean',\n",
      "                'y_policy': 'default'}}\n"
     ]
    }
   ],
   "source": [
    "config_path = os.path.join(\"src/baselines/tabsyn/configs\", f\"{DATA_NAME}.toml\")\n",
    "raw_config = src.load_config(config_path)\n",
    "\n",
    "pprint(raw_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file is a TOML file that contains the following hyperparameters:\n",
    "\n",
    "1. **model_params:** specifies the structure of the transformers (both encoder and decoder) in the VAE model, including number of transformer layers, number of self-attnetion heads and token dimension.\n",
    "\n",
    "2. **transforms:** specifies the transformations and preprocessing of the data before tokenization, such as cleaning, normalization, and encoding.\n",
    "    - For preprocessing numerical features, we use the gaussian quantile transformation and replace the NaN values with mean of each row.\n",
    "    - For categorical features, we use the one-hot encoding method. NaN values are left unchanged, but we have the option to replace them. We have the option to drop the values that appear with less than a given minimum frequency under each column. Furthermore, we have the option to add an extra encoding step for categorical features during tokenization.\n",
    "\n",
    "3. **train.vae:** specifies training parameters of the VAE, including batch size, number of epochs, and number of dataset workers.\n",
    "\n",
    "4. **train.diffusion:** specifies the same training parameters as above for the diffusion model.\n",
    "\n",
    "5. **train.optim.vae:** specifies the parameters of the *Adam* optimizer and the `ReduceLROnPlateau` learning rate scheduler used to train the VAE. Optimizer parameters include initial learning rate and weight decay. LR scheduler parameters includer `factor` and `patience`.\n",
    "\n",
    "6. **train.optim.diffusion:** specifies the same parameters as above for the diffusion model.\n",
    "\n",
    "7. **loss_params:** specifies parameters of the loss function used to train the VAE including `max_beta`, `min_beta` and `lambd`.\n",
    "\n",
    "$\\beta$ is the coefficient of the KL divergence term in the VAE loss formula,\n",
    "\n",
    "$\\mathcal{L}_{vae} = \\mathcal{L}_{mse} + \\mathcal{L}_{ce} + \\beta \\mathcal{L}_{kl}$\n",
    ".\n",
    "\n",
    "Parameters `max_beta` and `min_beta` determine the range of $\\beta$. $\\beta$ is first set to `max_beta`. If the loss stops decreasing for a certain number of epochs (e.g. $10$ epochs), then at the end of each epoch after that (e.g. epoch $11$, $12$, etc.) $\\beta$ is decreased by a factor of `lambd`,\n",
    "$\\beta_{new} = \\lambda \\beta_{curr}$,\n",
    "until it reaches `beta_min`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we pre-process the data and make a dataset object.\n",
    "\n",
    "First, we determine transformations needed for the dataset, such as normalization and cleaning, in `transforms`. Next, using `preprocess` function we load the data from disk in arrays that contain both training and test data (`X_num` and `X_cat`), as well as the number of categories for each categorical feature (`categories`) and the number of numerical features (`d_numerical`).\n",
    "\n",
    "We then separate the train and test data in different arrays and convert them to Pytorch tensors.\n",
    "We create a dataset object (`TabularDataset`) with the train data. `TabularDataset` is a simple module which returns the tokens of a single row at a time. Each row constiutes a single data sample in TabSyn. Afterwards, we create a Dataloader for the train data using the `batch_size` and `num_workers` specified in config.\n",
    "\n",
    "In contrast, we keep the test data as tensors (`X_test_num` and `X_test_cat`). If a GPU is available, we move these tensors to GPU so that they can be accessed by the model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaNs in numerical features, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/ws_aabboud/diffusion_model_bootcamp/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# preprocess data\n",
    "X_num, X_cat, categories, d_numerical = preprocess(os.path.join(PROCESSED_DATA_DIR, DATA_NAME),\n",
    "                                                   transforms = raw_config[\"transforms\"],\n",
    "                                                   task_type = raw_config[\"task_type\"])\n",
    "\n",
    "# separate train and test data\n",
    "X_train_num, X_test_num = X_num\n",
    "X_train_cat, X_test_cat = X_cat\n",
    "\n",
    "# convert to float tensor\n",
    "X_train_num, X_test_num = torch.tensor(X_train_num).float(), torch.tensor(X_test_num).float()\n",
    "X_train_cat, X_test_cat =  torch.tensor(X_train_cat), torch.tensor(X_test_cat)\n",
    "\n",
    "# create dataset module\n",
    "train_data = TabularDataset(X_train_num.float(), X_train_cat)\n",
    "\n",
    "# move test data to gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_test_num = X_test_num.float().to(device)\n",
    "X_test_cat = X_test_cat.to(device)\n",
    "\n",
    "# create train dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size = raw_config[\"train\"][\"vae\"][\"batch_size\"],\n",
    "    shuffle = True,\n",
    "    num_workers = raw_config[\"train\"][\"vae\"][\"num_dataset_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the model using the `TabSyn` class. `TabSyn` class takes the following arguments:\n",
    "\n",
    "1. `train_loader`: dataloader for train data.\n",
    "2. `X_test_num`: numerical features of the test data.\n",
    "3. `X_test_cat`: categorical features of the train data.\n",
    "4. `num_numerical_features`: number of numerical features in the dataset.\n",
    "5. `num_classes`: number of classes (i.e. categories) of each categorical feature in the dataset.\n",
    "6. `device`: the device on which the model and data exist, either \"cpu\" or \"cuda\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabsyn = TabSyn(train_loader,\n",
    "                X_test_num, X_test_cat,\n",
    "                num_numerical_features = d_numerical,\n",
    "                num_classes = categories,\n",
    "                device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TabSyn` class has the tools to instantiate VAE and diffusion models, train both, and sample from the trained diffusion model.\n",
    "We will demonstrate how to use these tools in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE and the diffusion model are trained independently. The following subsections explain each training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### A. Train VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to instantiate the VAE using the `instantiate_vae` method. This method takes the VAE model hyperparameters, optimizer and lr scheduler parameters from config, and instantiates them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully instantiated VAE model.\n"
     ]
    }
   ],
   "source": [
    "# instantiate VAE model for training\n",
    "tabsyn.instantiate_vae(**raw_config[\"model_params\"], optim_params = raw_config[\"train\"][\"optim\"][\"vae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have instantiated the VAE, we can train it using the `train_vae` function.\n",
    "This function receives the loss hyperparameters and number of epochs from the config.\n",
    "Moreover, it recieves `save_path` which is the directory where trained model checkpoints will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(f\"{MODEL_PATH}/{DATA_NAME}/vae\")\n",
    "# Define your paths\n",
    "directory_path = f\"{MODEL_PATH}/{DATA_NAME}/vae\"\n",
    "\n",
    "# Check if directory exists\n",
    "if os.path.exists(directory_path):\n",
    "    # Remove the existing directory and its contents\n",
    "    shutil.rmtree(directory_path)\n",
    "\n",
    "# Create the directory\n",
    "os.makedirs(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                                                                                                                                                                             | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, beta = 0.010000, Train MSE: 13.445180, Train CE:1.044495, Train KL:1.004395, Val MSE:12.604588, Val CE:1.001523, Train ACC:0.487116, Val ACC:0.482353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, beta = 0.010000, Train MSE: 12.738208, Train CE:0.992967, Train KL:0.940369, Val MSE:12.074593, Val CE:0.960849, Train ACC:0.459985, Val ACC:0.467279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, beta = 0.010000, Train MSE: 12.173989, Train CE:0.957056, Train KL:0.926207, Val MSE:11.531956, Val CE:0.957460, Train ACC:0.457136, Val ACC:0.444118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, beta = 0.010000, Train MSE: 11.703219, Train CE:0.954443, Train KL:0.950759, Val MSE:11.155245, Val CE:0.934099, Train ACC:0.433598, Val ACC:0.445588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, beta = 0.010000, Train MSE: 11.284771, Train CE:0.951732, Train KL:1.000865, Val MSE:10.822515, Val CE:0.940759, Train ACC:0.428271, Val ACC:0.442279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, beta = 0.010000, Train MSE: 10.939717, Train CE:0.944744, Train KL:1.063959, Val MSE:10.521458, Val CE:0.931749, Train ACC:0.431739, Val ACC:0.448529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, beta = 0.010000, Train MSE: 10.629172, Train CE:0.930393, Train KL:1.129864, Val MSE:10.205504, Val CE:0.911699, Train ACC:0.433598, Val ACC:0.454779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, beta = 0.010000, Train MSE: 10.347938, Train CE:0.917326, Train KL:1.193349, Val MSE:9.910804, Val CE:0.888954, Train ACC:0.442517, Val ACC:0.454044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, beta = 0.010000, Train MSE: 10.040994, Train CE:0.899176, Train KL:1.251430, Val MSE:9.653733, Val CE:0.875695, Train ACC:0.445738, Val ACC:0.455515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, beta = 0.010000, Train MSE: 9.760778, Train CE:0.883426, Train KL:1.304872, Val MSE:9.344542, Val CE:0.872610, Train ACC:0.446110, Val ACC:0.464706\n",
      "Training time: 0.1392 mins\n",
      "Successfully trained and saved the VAE model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tabsyn.train_vae(**raw_config[\"loss_params\"],\n",
    "                 num_epochs = raw_config[\"train\"][\"vae\"][\"num_epochs\"],\n",
    "                 save_path = os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the VAE, we embed the training data with the trained encoder and store the embeddings in a direcotry specified by `vae_ckpt_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved pretrained embeddings on disk!\n"
     ]
    }
   ],
   "source": [
    "# embed all inputs in the latent space\n",
    "tabsyn.save_vae_embeddings(X_train_num, X_train_cat,\n",
    "                           vae_ckpt_dir = os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Train Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have stored the training data embeddings, we need to load and prepare them for the diffusion model.\n",
    "We load the embeddings using `load_vae_embeddings`. We normalize the embeddings by subtracting the mean and dividing by the standard deviation. Then, we create a Dataloader with the specified `batch_size` and `num_workers` from the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load latent space embeddings\n",
    "train_z, _ = tabsyn.load_latent_embeddings(os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))  # train_z dim: B x in_dim\n",
    "\n",
    "# normalize embeddings\n",
    "mean, std = train_z.mean(0), train_z.std(0)\n",
    "train_z = (train_z - mean) / std\n",
    "latent_train_data = train_z\n",
    "\n",
    "# create data loader\n",
    "latent_train_loader = DataLoader(\n",
    "    latent_train_data,\n",
    "    batch_size = raw_config[\"train\"][\"diffusion\"][\"batch_size\"],\n",
    "    shuffle = True,\n",
    "    num_workers = raw_config[\"train\"][\"diffusion\"][\"num_dataset_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is ready, we instantiate the diffusion model with `instantiate_diffusion`. The input dimension and hidden dimention of the diffusion model is determined by the dimension of the embeddings. \n",
    "Moreover, we instantiate the optimizer and lr scheduler using hyperparameters from config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPDiffusion(\n",
      "  (proj): Linear(in_features=108, out_features=1024, bias=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=1024, out_features=108, bias=True)\n",
      "  )\n",
      "  (map_noise): PositionalEmbedding()\n",
      "  (time_embed): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "The number of parameters: 10715244\n",
      "Successfully instantiated diffusion model.\n"
     ]
    }
   ],
   "source": [
    "# instantiate diffusion model for training\n",
    "tabsyn.instantiate_diffusion(in_dim = train_z.shape[1], hid_dim = train_z.shape[1], optim_params = raw_config[\"train\"][\"optim\"][\"diffusion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the diffusion model with `train_diffusion` function.\n",
    "This function takes the following arguements:\n",
    "1. `latent_train_loader`: dataloader for the latent representations which are used to train the diffusion model.\n",
    "2. `num_epochs`: number of training epochs.\n",
    "3. `ckpt_path`: directory where the model checkpoints will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.90it/s, Loss=1.9]\n",
      "Epoch 2/9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.78it/s, Loss=1.67]\n",
      "Epoch 3/9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.99it/s, Loss=1.79]\n",
      "Epoch 4/9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.67it/s, Loss=1.69]\n",
      "Epoch 5/9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.82it/s, Loss=1.54]\n",
      "Epoch 6/9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.92it/s, Loss=1.53]\n",
      "Epoch 7/9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.70it/s, Loss=1.35]\n",
      "Epoch 8/9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.74it/s, Loss=1.26]\n",
      "Epoch 9/9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  3.86it/s, Loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  5.429706335067749\n"
     ]
    }
   ],
   "source": [
    "# os.makedirs(f\"{MODEL_PATH}/{DATA_NAME}\")\n",
    "# train diffusion model\n",
    "tabsyn.train_diffusion(latent_train_loader,\n",
    "                       num_epochs = raw_config[\"train\"][\"diffusion\"][\"num_epochs\"],\n",
    "                       ckpt_path = os.path.join(MODEL_PATH, DATA_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training model from scratch, we can also load weights of a pre-trained model from a given checkpoint with `load_model_state` function.\n",
    "If we haven't instantiated the VAE and diffusion model beforehand, we need to instantiate them first using `instantiate_vae` and `instantiate_diffusion` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully instantiated VAE model.\n"
     ]
    }
   ],
   "source": [
    "# instantiate VAE model\n",
    "tabsyn.instantiate_vae(**raw_config[\"model_params\"], optim_params = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/fs01/home/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_embeddings_path = \"/projects/diffusion_bootcamp/models/tabular/tabsyn/default/vae\"\n",
    "latent_embeddings_path =os.path.join(MODEL_PATH, DATA_NAME, \"vae\")  # f\"{os.getcwd()}/models/tabsyn/{DATA_NAME}/vae\"  #f\"/models/tabsyn/{DATA_NAME}/vae\"\n",
    "# load latent embeddings of input data\n",
    "train_z, token_dim = tabsyn.load_latent_embeddings(latent_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPDiffusion(\n",
      "  (proj): Linear(in_features=108, out_features=1024, bias=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=1024, out_features=108, bias=True)\n",
      "  )\n",
      "  (map_noise): PositionalEmbedding()\n",
      "  (time_embed): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "The number of parameters: 10715244\n",
      "Successfully instantiated diffusion model.\n"
     ]
    }
   ],
   "source": [
    "# instantiate diffusion model\n",
    "tabsyn.instantiate_diffusion(in_dim = train_z.shape[1], hid_dim = train_z.shape[1], optim_params = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/tabsyn/IBM_AML_FeatureEngineered_FS1'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(MODEL_PATH, DATA_NAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model state from models/tabsyn/IBM_AML_FeatureEngineered_FS1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pretrained_model_path = os.path.join(MODEL_PATH, DATA_NAME)  #f\"{os.getcwd()}/models/tabsyn/{DATA_NAME}\"#\"/projects/diffusion_bootcamp/models/tabular/tabsyn/default\"\n",
    "# load state from checkpoint\n",
    "tabsyn.load_model_state(ckpt_dir = pretrained_model_path,\n",
    "                        dif_ckpt_name = \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained the model effectively, using `sample` function we can generate synthetic data starting from compelete noise. The input of this function is as follows:\n",
    "\n",
    "1. `train_z`: latent embeddings of the training data.\n",
    "2. `info`: info about the data from the json file we reviewed at the beginning of this notebook.\n",
    "3. `num_inverse`: detokenizer for numerical features.\n",
    "4. `cat_inverse`: detokenizer for categorical features.\n",
    "5. `save_path`: file-path where the synthetic table will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaNs in numerical features, skipping\n"
     ]
    }
   ],
   "source": [
    "# load data info file\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, DATA_NAME, \"info.json\"), \"r\") as file:\n",
    "    data_info = json.load(file)\n",
    "data_info[\"token_dim\"] = token_dim\n",
    "\n",
    "# get inverse tokenizers\n",
    "_, _, categories, d_numerical, num_inverse, cat_inverse = preprocess(os.path.join(PROCESSED_DATA_DIR, DATA_NAME),\n",
    "                                                                     transforms = raw_config[\"transforms\"],\n",
    "                                                                     task_type = raw_config[\"task_type\"],\n",
    "                                                                     inverse = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6114, 4)\n",
      "Time: 3.793548345565796\n",
      "Saving sampled data to data/synthetic_data/IBM_AML_FeatureEngineered_FS1/tabsyn.csv\n"
     ]
    }
   ],
   "source": [
    "# sample data\n",
    "num_samples = train_z.shape[0]\n",
    "in_dim = train_z.shape[1] \n",
    "mean_input_emb = train_z.mean(0)\n",
    "tabsyn.sample(num_samples,\n",
    "              in_dim,\n",
    "              mean_input_emb,\n",
    "              info = data_info,\n",
    "              num_inverse = num_inverse,\n",
    "              cat_inverse = cat_inverse,\n",
    "              save_path = os.path.join(SYNTH_DATA_DIR, DATA_NAME, \"tabsyn.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6114, 108)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_z.shape[0],train_z.shape[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally here, we review the synthesized data. In the following `evaluate_synthetic_data.ipynb` notebook, we will evaluate this synthesized data with respect to various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Destination_1</th>\n",
       "      <th>Destination_2</th>\n",
       "      <th>Destination_3</th>\n",
       "      <th>Destination_4</th>\n",
       "      <th>Destination_5</th>\n",
       "      <th>Destination_6</th>\n",
       "      <th>Destination_7</th>\n",
       "      <th>Destination_8</th>\n",
       "      <th>Destination_9</th>\n",
       "      <th>Destination_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Euro</th>\n",
       "      <th>Yuan</th>\n",
       "      <th>number_transactions_above_9k_cad</th>\n",
       "      <th>avg_transaction_value_in_cad</th>\n",
       "      <th>transaction_value_variance_in_cad</th>\n",
       "      <th>variance_from_10k_cad</th>\n",
       "      <th>min_day_to_max_day_range</th>\n",
       "      <th>avg_transaction_frequency_in_days</th>\n",
       "      <th>transaction_frequency_variance_in_days</th>\n",
       "      <th>Is_FanOut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.390849e+07</td>\n",
       "      <td>8.217370e+06</td>\n",
       "      <td>6055.89900</td>\n",
       "      <td>8086.7256</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16482.22500</td>\n",
       "      <td>698.387</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2908.79350</td>\n",
       "      <td>8.801453e+09</td>\n",
       "      <td>5.600047e+08</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.611571</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.439617e+04</td>\n",
       "      <td>4.961993e+04</td>\n",
       "      <td>49703.95000</td>\n",
       "      <td>4790.0186</td>\n",
       "      <td>1204.3182</td>\n",
       "      <td>6830.72400</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>624.10065</td>\n",
       "      <td>1.005576e+09</td>\n",
       "      <td>3.648974e+08</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.611571</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.652771e+04</td>\n",
       "      <td>2.444280e+04</td>\n",
       "      <td>160.51692</td>\n",
       "      <td>16035.8300</td>\n",
       "      <td>5811.1150</td>\n",
       "      <td>795.93964</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14006.77050</td>\n",
       "      <td>3.758378e+07</td>\n",
       "      <td>2.363945e+14</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.249369</td>\n",
       "      <td>1.611571</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.602315e+04</td>\n",
       "      <td>1.585963e+03</td>\n",
       "      <td>17285.34800</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>945.8458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5265.31100</td>\n",
       "      <td>7.644311e+07</td>\n",
       "      <td>2.111755e+10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.449506</td>\n",
       "      <td>7.849218</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.361475e+04</td>\n",
       "      <td>1.919021e+04</td>\n",
       "      <td>108474.41000</td>\n",
       "      <td>2573.2070</td>\n",
       "      <td>7965.3657</td>\n",
       "      <td>176386.66000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1426.76450</td>\n",
       "      <td>3.632620e+10</td>\n",
       "      <td>4.298464e+08</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.611571</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Destination_1  Destination_2  Destination_3  Destination_4  Destination_5  \\\n",
       "0   1.390849e+07   8.217370e+06     6055.89900      8086.7256         0.0000   \n",
       "1   2.439617e+04   4.961993e+04    49703.95000      4790.0186      1204.3182   \n",
       "2   1.652771e+04   2.444280e+04      160.51692     16035.8300      5811.1150   \n",
       "3   2.602315e+04   1.585963e+03    17285.34800         0.0000         0.0000   \n",
       "4   2.361475e+04   1.919021e+04   108474.41000      2573.2070      7965.3657   \n",
       "\n",
       "   Destination_6  Destination_7  Destination_8  Destination_9  Destination_10  \\\n",
       "0    16482.22500        698.387         0.0000            0.0             0.0   \n",
       "1     6830.72400          0.000         0.0000            0.0             0.0   \n",
       "2      795.93964          0.000         0.0000            0.0             0.0   \n",
       "3        0.00000          0.000       945.8458            0.0             0.0   \n",
       "4   176386.66000          0.000         0.0000            0.0             0.0   \n",
       "\n",
       "   ...  Euro  Yuan  number_transactions_above_9k_cad  \\\n",
       "0  ...     N     Y                               3.0   \n",
       "1  ...     N     Y                               0.0   \n",
       "2  ...     N     Y                               0.0   \n",
       "3  ...     N     Y                               0.0   \n",
       "4  ...     N     Y                               0.0   \n",
       "\n",
       "   avg_transaction_value_in_cad  transaction_value_variance_in_cad  \\\n",
       "0                    2908.79350                       8.801453e+09   \n",
       "1                     624.10065                       1.005576e+09   \n",
       "2                   14006.77050                       3.758378e+07   \n",
       "3                    5265.31100                       7.644311e+07   \n",
       "4                    1426.76450                       3.632620e+10   \n",
       "\n",
       "   variance_from_10k_cad min_day_to_max_day_range  \\\n",
       "0           5.600047e+08                      7.0   \n",
       "1           3.648974e+08                      7.0   \n",
       "2           2.363945e+14                      7.0   \n",
       "3           2.111755e+10                      3.0   \n",
       "4           4.298464e+08                      7.0   \n",
       "\n",
       "  avg_transaction_frequency_in_days transaction_frequency_variance_in_days  \\\n",
       "0                          3.000000                               1.611571   \n",
       "1                          2.333333                               1.611571   \n",
       "2                          1.249369                               1.611571   \n",
       "3                          2.449506                               7.849218   \n",
       "4                          3.500000                               1.611571   \n",
       "\n",
       "   Is_FanOut  \n",
       "0          N  \n",
       "1          N  \n",
       "2          N  \n",
       "3          Y  \n",
       "4          Y  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(SYNTH_DATA_DIR, DATA_NAME, \"tabsyn.csv\"))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Value Imputation for the Target Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KeOps] Warning : Cuda libraries were not detected on the system or could not be loaded ; using cpu only mode\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "from scripts.impute import impute\n",
    "from scripts.eval.eval_impute import eval_impute\n",
    "\n",
    "from scripts.eval.eval_impute import eval_impute\n",
    "from scripts.eval.eval_density import eval_density\n",
    "from scripts.eval.eval_quality import eval_quality\n",
    "from scripts.eval.eval_mle import eval_mle\n",
    "from scripts.eval.eval_dcr import eval_dcr\n",
    "from scripts.eval.eval_detection import eval_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = \"IBM_AML_FeatureEngineered_FS1\"\n",
    "\n",
    "# For shared directory you can change it to \"/projects/diffusion_bootcamp/data/tabular\"\n",
    "DATA_DIR = \"data\"\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed_data\")\n",
    "\n",
    "TRAIN_DATA_PATH = f\"{DATA_DIR}/processed_data/{dataname}/train.csv\"\n",
    "TEST_DATA_PATH = f\"{DATA_DIR}/test_data/{dataname}/test.csv\"\n",
    "# TABDDPM_DATA_PATH = f\"{DATA_DIR}/synthetic_data/{dataname}/tabddpm.csv\"\n",
    "TABSYN_DATA_PATH = f\"{DATA_DIR}/synthetic_data/{dataname}/tabsyn.csv\"\n",
    "INFO_PATH = f\"{DATA_DIR}/processed_data/{dataname}/info.json\"\n",
    "# Change that path to your local modal path to impute\n",
    "MODEL_PATH = os.path.join('models','tabsyn') #\"/deloitte_team/single_table_synthesis/models/tabsyn/IBM_AML_FeatureEngineered_FS1\"\n",
    "IMPUTE_PATH = \"impute/tabsyn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/fs01/home/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 1 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 2 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 3 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 4 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 5 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 6 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 7 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 8 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 9 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 10 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 11 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 12 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 13 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 14 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 15 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 16 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 17 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 18 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 19 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 20 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 21 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 22 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 23 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 24 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 25 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 26 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 27 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 28 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n",
      "Trial 29 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(680, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "impute(dataname, PROCESSED_DATA_DIR, INFO_PATH, MODEL_PATH, IMPUTE_PATH, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_type': 'binclass',\n",
       " 'model_params': {'n_head': 1, 'factor': 32, 'num_layers': 2, 'd_token': 4},\n",
       " 'transforms': {'normalization': 'quantile',\n",
       "  'num_nan_policy': 'mean',\n",
       "  'cat_nan_policy': None,\n",
       "  'cat_min_frequency': None,\n",
       "  'cat_encoding': None,\n",
       "  'y_policy': 'default'},\n",
       " 'train': {'vae': {'num_epochs': 10,\n",
       "   'batch_size': 4096,\n",
       "   'num_dataset_workers': 4},\n",
       "  'diffusion': {'num_epochs': 9, 'batch_size': 4096, 'num_dataset_workers': 4},\n",
       "  'optim': {'vae': {'lr': 0.001,\n",
       "    'weight_decay': 0,\n",
       "    'factor': 0.95,\n",
       "    'patience': 10},\n",
       "   'diffusion': {'lr': 0.001,\n",
       "    'weight_decay': 0,\n",
       "    'factor': 0.9,\n",
       "    'patience': 20}}},\n",
       " 'loss_params': {'max_beta': 0.01, 'min_beta': 1e-05, 'lambd': 0.7},\n",
       " 'impute': {'num_trials': 30,\n",
       "  'SIGMA_MIN': 0.002,\n",
       "  'SIGMA_MAX': 80,\n",
       "  'rho': 7,\n",
       "  'S_churn': 1,\n",
       "  'S_min': 0,\n",
       "  'S_max': inf,\n",
       "  'S_noise': 1,\n",
       "  'num_steps': 30,\n",
       "  'N': 20}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-F1: 0.8632352941176471\n",
      "AUC: 0.9175778546712803\n",
      "\n",
      "Confusion Matrix:\n",
      "          Predicted N  Predicted Y\n",
      "Actual N          347           28\n",
      "Actual Y           65          240\n"
     ]
    }
   ],
   "source": [
    "# Uncomment below line to evaluate pre-imputed data\n",
    "# IMPUTE_PATH = \"/projects/diffusion_bootcamp/data/tabular/impute_data/tabsyn\"\n",
    "\n",
    "eval_impute(dataname, PROCESSED_DATA_DIR, IMPUTE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random\n",
    "# Micro-F1: 0.2222222222222222\n",
    "# AUC: 0.25925925925925924\n",
    "\n",
    "# Confusion Matrix:\n",
    "#           Predicted N  Predicted Y\n",
    "# Actual N            2            0\n",
    "# Actual Y            7            0\n",
    "\n",
    "# Micro-F1: 0.8632352941176471\n",
    "# AUC: 0.9175778546712803\n",
    "\n",
    "# Confusion Matrix:\n",
    "#           Predicted N  Predicted Y\n",
    "# Actual N          347           28\n",
    "# Actual Y           65          240"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zhang, Hengrui, et al.** \"Mixed-type tabular data synthesis with score-based diffusion in latent space.\" *International Conference on Learning Representations (ICLR)* (2023).\n",
    "\n",
    "**GitHub Repository:** [Amazon Science - Tabsyn](https://github.com/amazon-science/tabsyn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_models",
   "language": "python",
   "name": "diffusion_models"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
