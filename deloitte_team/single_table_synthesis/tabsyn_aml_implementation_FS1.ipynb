{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  \n",
    "# TABSYN: Tabular Data Synthesis with Diffusion Models\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two challenges regarding the extention of diffusion models to tabular data are:\n",
    "1. **Diverse data types:** a single table can have different columns each containing data of different types, including numerical, categorical, text, etc.\n",
    "2. **Varied distributions:** the distribution of data under different columns in a single table varry widely from column to column.\n",
    "\n",
    "**TabSyn** addresses these challenges by introducing a latent space where tabular data of all columns are jointly represented. It then proceedes to train a diffusion model on the latent representations.\n",
    "This tactic allows TabSyn to:\n",
    "1. Train a single diffusion model for all data types in the dataset (i.e. Generality).\n",
    "2. Optimize the distribution of latent embeddings to facilitate training of the subsequent diffusion model, thus generating higher quality synthetic data (i.e. Quality).\n",
    "3. Require much fewer reverse steps during training of the diffusion model, and synthesize data faster (i.e. Speed).\n",
    "\n",
    "In this notebook, we review and implement the TabSyn model. The notebook is organized as follows:\n",
    "\n",
    "1. [Imports and Setup]()\n",
    "\n",
    "\n",
    "2. [Default Dataset]()\n",
    "    \n",
    "    \n",
    "3. [TabSyn Algorithm]()\n",
    "    \n",
    "    3.1. [Load Config]()\n",
    "    \n",
    "    3.2. [Make Dataset]()\n",
    "    \n",
    "    3.3. [Instantiate Model]()\n",
    "    \n",
    "    3.4. [Train Model]()\n",
    "        \n",
    "    3.5. [Load Pretrained Model]()\n",
    "    \n",
    "    3.6. [Sample Data]()\n",
    "    \n",
    "    3.7. [Review Synthetic Data]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we import all necessary libraries and modules required for setting up the environment.\n",
    "Most of the libraries we need to implement TabSyn are the same as TabDDPM.\n",
    "We also specify `NAME_URL_DICT_UCI`, `DATA_NAME`, `DATA_DIR` and other paths as in TabDDPM's implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import src\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scripts.download_dataset import download_from_uci\n",
    "from scripts.process_dataset import process_data\n",
    "\n",
    "from src.data import preprocess, TabularDataset\n",
    "from src.baselines.tabsyn.pipeline import TabSyn\n",
    "\n",
    "from src.util import visualize_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/fs01/home/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('/fs01/home/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scripts.process_dataset import process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27161/3381011145.py:9: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  df.iloc[:, 29:] = df.iloc[:, 29:].astype(float)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/h/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/data/raw_data/IBM_AML_FeatureEngineered_FS1/flattened_sample.csv')\n",
    "df=df.dropna()\n",
    "# Ignore the The Patern ID and The Source columns\n",
    "df=df.iloc[::,2::]   \n",
    "# Convert the first 29 Destination and Currency columns to boolean\n",
    "df.iloc[:, :29] = df.iloc[:, :29].astype(str)\n",
    "df.iloc[:, 29:] = df.iloc[:, 29:].astype(float)\n",
    "# Convert the last column Is Fanout to boolean\n",
    "df.iloc[:, -1] = df.iloc[:, -1].astype(str)\n",
    "\n",
    "\n",
    "df=df.iloc[:, ~df.columns.isin(df.columns[16:29])]  # Eclude Currency Columns for test purposes\n",
    "df=df.iloc[:, ~df.columns.isin(df.columns[17:20])]  # Eclude erronous columns Columns for test purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Destination_1</th>\n",
       "      <th>Destination_2</th>\n",
       "      <th>Destination_3</th>\n",
       "      <th>Destination_4</th>\n",
       "      <th>Destination_5</th>\n",
       "      <th>Destination_6</th>\n",
       "      <th>Destination_7</th>\n",
       "      <th>Destination_8</th>\n",
       "      <th>Destination_9</th>\n",
       "      <th>Destination_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Destination_12</th>\n",
       "      <th>Destination_13</th>\n",
       "      <th>Destination_14</th>\n",
       "      <th>Destination_15</th>\n",
       "      <th>Destination_16</th>\n",
       "      <th>number_transactions_above_9k_cad</th>\n",
       "      <th>min_day_to_max_day_range</th>\n",
       "      <th>avg_transaction_frequency_in_days</th>\n",
       "      <th>transaction_frequency_variance_in_days</th>\n",
       "      <th>Is FanOut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6376</td>\n",
       "      <td>5369</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2118007</td>\n",
       "      <td>2118007</td>\n",
       "      <td>2118007</td>\n",
       "      <td>2118007</td>\n",
       "      <td>639683</td>\n",
       "      <td>449285</td>\n",
       "      <td>449285</td>\n",
       "      <td>449285</td>\n",
       "      <td>449285</td>\n",
       "      <td>22306</td>\n",
       "      <td>...</td>\n",
       "      <td>22306</td>\n",
       "      <td>22306</td>\n",
       "      <td>6902</td>\n",
       "      <td>6902</td>\n",
       "      <td>6902</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2118007</td>\n",
       "      <td>2118007</td>\n",
       "      <td>2118007</td>\n",
       "      <td>449285</td>\n",
       "      <td>449285</td>\n",
       "      <td>449285</td>\n",
       "      <td>22306</td>\n",
       "      <td>22306</td>\n",
       "      <td>22306</td>\n",
       "      <td>6902</td>\n",
       "      <td>...</td>\n",
       "      <td>6902</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>927</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2915</td>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28344</td>\n",
       "      <td>5501</td>\n",
       "      <td>5343</td>\n",
       "      <td>5343</td>\n",
       "      <td>4152</td>\n",
       "      <td>4039</td>\n",
       "      <td>2644</td>\n",
       "      <td>881</td>\n",
       "      <td>881</td>\n",
       "      <td>881</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "      <td>107</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5343</td>\n",
       "      <td>5343</td>\n",
       "      <td>5343</td>\n",
       "      <td>5343</td>\n",
       "      <td>881</td>\n",
       "      <td>881</td>\n",
       "      <td>235</td>\n",
       "      <td>235</td>\n",
       "      <td>235</td>\n",
       "      <td>235</td>\n",
       "      <td>...</td>\n",
       "      <td>173</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5343</td>\n",
       "      <td>2644</td>\n",
       "      <td>881</td>\n",
       "      <td>881</td>\n",
       "      <td>235</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>831</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19467696</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>114028</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5922</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>71626</td>\n",
       "      <td>25949</td>\n",
       "      <td>1073</td>\n",
       "      <td>278</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>164</td>\n",
       "      <td>70</td>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "      <td>60</td>\n",
       "      <td>44</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>150</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>249488</td>\n",
       "      <td>691</td>\n",
       "      <td>691</td>\n",
       "      <td>691</td>\n",
       "      <td>691</td>\n",
       "      <td>691</td>\n",
       "      <td>691</td>\n",
       "      <td>691</td>\n",
       "      <td>691</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>691</td>\n",
       "      <td>691</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15989</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "      <td>...</td>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>178</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.66884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Destination_1 Destination_2 Destination_3 Destination_4 Destination_5  \\\n",
       "0           6376          5369             0             0             0   \n",
       "1        2118007       2118007       2118007       2118007        639683   \n",
       "2        2118007       2118007       2118007        449285        449285   \n",
       "3            927            18             0             0             0   \n",
       "5           2915          1053             0             0             0   \n",
       "7          28344          5501          5343          5343          4152   \n",
       "8           5343          5343          5343          5343           881   \n",
       "9           5343          2644           881           881           235   \n",
       "10           831            21             0             0             0   \n",
       "11      19467696            25             0             0             0   \n",
       "13        114028            21             0             0             0   \n",
       "14            18             3             0             0             0   \n",
       "15          5922            22             0             0             0   \n",
       "16         71626         25949          1073           278             0   \n",
       "17           164            70            66            66            60   \n",
       "21           150            22             0             0             0   \n",
       "24        249488           691           691           691           691   \n",
       "25           691           691            46            46             0   \n",
       "28         15989           178           178           178           178   \n",
       "29           178           106             0             0             0   \n",
       "\n",
       "   Destination_6 Destination_7 Destination_8 Destination_9 Destination_10  \\\n",
       "0              0             0             0             0              0   \n",
       "1         449285        449285        449285        449285          22306   \n",
       "2         449285         22306         22306         22306           6902   \n",
       "3              0             0             0             0              0   \n",
       "5              0             0             0             0              0   \n",
       "7           4039          2644           881           881            881   \n",
       "8            881           235           235           235            235   \n",
       "9            173           173           107             0              0   \n",
       "10             0             0             0             0              0   \n",
       "11             0             0             0             0              0   \n",
       "13             0             0             0             0              0   \n",
       "14             0             0             0             0              0   \n",
       "15             0             0             0             0              0   \n",
       "16             0             0             0             0              0   \n",
       "17            44            25            24            11              0   \n",
       "21             0             0             0             0              0   \n",
       "24           691           691           691           691             49   \n",
       "25             0             0             0             0              0   \n",
       "28           178           178           106           106            106   \n",
       "29             0             0             0             0              0   \n",
       "\n",
       "    ... Destination_12 Destination_13 Destination_14 Destination_15  \\\n",
       "0   ...              0              0              0              0   \n",
       "1   ...          22306          22306           6902           6902   \n",
       "2   ...           6902              0              0              0   \n",
       "3   ...              0              0              0              0   \n",
       "5   ...              0              0              0              0   \n",
       "7   ...            235            173            173            173   \n",
       "8   ...            173            107            107            107   \n",
       "9   ...              0              0              0              0   \n",
       "10  ...              0              0              0              0   \n",
       "11  ...              0              0              0              0   \n",
       "13  ...              0              0              0              0   \n",
       "14  ...              0              0              0              0   \n",
       "15  ...              0              0              0              0   \n",
       "16  ...              0              0              0              0   \n",
       "17  ...              0              0              0              0   \n",
       "21  ...              0              0              0              0   \n",
       "24  ...             46             46             46             46   \n",
       "25  ...              0              0              0              0   \n",
       "28  ...            106            106             14              0   \n",
       "29  ...              0              0              0              0   \n",
       "\n",
       "   Destination_16 number_transactions_above_9k_cad  min_day_to_max_day_range  \\\n",
       "0               0                              0.0                       6.0   \n",
       "1            6902                             13.0                       5.0   \n",
       "2               0                              9.0                       1.0   \n",
       "3               0                              0.0                       0.0   \n",
       "5               0                              0.0                       3.0   \n",
       "7             107                              1.0                       4.0   \n",
       "8             107                              0.0                       3.0   \n",
       "9               0                              0.0                       0.0   \n",
       "10              0                              0.0                       0.0   \n",
       "11              0                              1.0                       0.0   \n",
       "13              0                              1.0                       0.0   \n",
       "14              0                              0.0                       0.0   \n",
       "15              0                              0.0                       1.0   \n",
       "16              0                              2.0                       2.0   \n",
       "17              0                              0.0                       7.0   \n",
       "21              0                              0.0                       0.0   \n",
       "24             46                              1.0                       7.0   \n",
       "25              0                              0.0                       1.0   \n",
       "28              0                              1.0                       7.0   \n",
       "29              0                              0.0                       0.0   \n",
       "\n",
       "    avg_transaction_frequency_in_days  transaction_frequency_variance_in_days  \\\n",
       "0                            3.000000                                 1.66884   \n",
       "1                            0.294118                                 1.66884   \n",
       "2                            0.083333                                 1.66884   \n",
       "3                            0.000000                                 1.66884   \n",
       "5                            1.500000                                 1.66884   \n",
       "7                            0.222222                                 1.66884   \n",
       "8                            0.187500                                 1.66884   \n",
       "9                            0.000000                                 1.66884   \n",
       "10                           0.000000                                 1.66884   \n",
       "11                           0.000000                                 1.66884   \n",
       "13                           0.000000                                 1.66884   \n",
       "14                           0.000000                                 1.66884   \n",
       "15                           0.500000                                 1.66884   \n",
       "16                           0.500000                                 1.66884   \n",
       "17                           0.700000                                 1.66884   \n",
       "21                           0.000000                                 1.66884   \n",
       "24                           0.388889                                 1.66884   \n",
       "25                           0.250000                                 1.66884   \n",
       "28                           0.500000                                 1.66884   \n",
       "29                           0.000000                                 1.66884   \n",
       "\n",
       "    Is FanOut  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "5         0.0  \n",
       "7         0.0  \n",
       "8         0.0  \n",
       "9         0.0  \n",
       "10        0.0  \n",
       "11        0.0  \n",
       "13        0.0  \n",
       "14        0.0  \n",
       "15        0.0  \n",
       "16        0.0  \n",
       "17        0.0  \n",
       "21        0.0  \n",
       "24        0.0  \n",
       "25        0.0  \n",
       "28        0.0  \n",
       "29        0.0  \n",
       "\n",
       "[20 rows x 21 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.iloc[:20,::]  # test the first 20 records\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'column_info': {'Destination_1': 'str', 'Destination_2': 'str', 'Destination_3': 'str', 'Destination_4': 'str', 'Destination_5': 'str', 'Destination_6': 'str', 'Destination_7': 'str', 'Destination_8': 'str', 'Destination_9': 'str', 'Destination_10': 'str', 'Destination_11': 'str', 'Destination_12': 'str', 'Destination_13': 'str', 'Destination_14': 'str', 'Destination_15': 'str', 'Destination_16': 'str', 'number_transactions_above_9k_cad': 'float', 'min_day_to_max_day_range': 'float', 'avg_transaction_frequency_in_days': 'float', 'transaction_frequency_variance_in_days': 'float', 'Is FanOut': 'str'}}\n"
     ]
    }
   ],
   "source": [
    "def get_column_info(df):\n",
    "    column_info = {col: str(df[col].dtype).replace('float64', 'float').replace('int64', 'int').replace('object', 'str') for col in df.columns}\n",
    "    return column_info\n",
    "\n",
    "# Get column info\n",
    "column_info = get_column_info(df)\n",
    "\n",
    "# Print in desired format\n",
    "print({\"column_info\": column_info})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Destination_1', 'Destination_2', 'Destination_3', 'Destination_4',\n",
       "       'Destination_5', 'Destination_6', 'Destination_7', 'Destination_8',\n",
       "       'Destination_9', 'Destination_10', 'Destination_11', 'Destination_12',\n",
       "       'Destination_13', 'Destination_14', 'Destination_15', 'Destination_16',\n",
       "       'number_transactions_above_9k_cad', 'min_day_to_max_day_range',\n",
       "       'avg_transaction_frequency_in_days',\n",
       "       'transaction_frequency_variance_in_days', 'Is FanOut'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def generate_json_config(df):\n",
    "    # Get column names\n",
    "    column_names = df.columns.tolist()\n",
    "    \n",
    "    # Identify the target column index (last column)\n",
    "    target_col_idx = [len(column_names) - 1]\n",
    "    \n",
    "    # Initialize lists for numerical and categorical column indices\n",
    "    num_col_idx = []\n",
    "    cat_col_idx = []\n",
    "    \n",
    "    # Initialize dictionary for column info\n",
    "    column_info = {}\n",
    "    \n",
    "    # Determine column types and indices\n",
    "    for idx, col in enumerate(df.columns):\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        # Fill in column_info with appropriate type\n",
    "        if pd.api.types.is_integer_dtype(col_type):\n",
    "            column_info[col] = \"int\"\n",
    "            if idx != target_col_idx[0]:  # Exclude target column from num_col_idx\n",
    "                num_col_idx.append(idx)\n",
    "        elif pd.api.types.is_float_dtype(col_type):\n",
    "            column_info[col] = \"float\"\n",
    "            num_col_idx.append(idx)\n",
    "        elif pd.api.types.is_bool_dtype(col_type):\n",
    "            column_info[col] = \"bool\"\n",
    "            cat_col_idx.append(idx)\n",
    "        else:\n",
    "            column_info[col] = \"str\"\n",
    "            cat_col_idx.append(idx)\n",
    "    \n",
    "    # Calculate train_num and test_num\n",
    "    total_rows = len(df)\n",
    "    train_num = int(total_rows * 0.8)\n",
    "    test_num = total_rows - train_num\n",
    "    \n",
    "    # Construct JSON configuration object\n",
    "    config = {\n",
    "        \"column_names\": column_names,\n",
    "        \"num_col_idx\": num_col_idx,\n",
    "        \"cat_col_idx\": cat_col_idx,\n",
    "        \"target_col_idx\": target_col_idx,\n",
    "        \"file_type\": \"csv\",\n",
    "        \"data_path\": \"/h/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/data/raw_data/IBM_AML_FeatureEngineered_FS1/train.csv\",\n",
    "        \"test_path\":\"/h/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/data/test_data/IBM_AML_FeatureEngineered_FS1/test.csv\",\n",
    "        \"column_info\": column_info,\n",
    "        \"train_num\": train_num,\n",
    "        \"test_num\": test_num\n",
    "    }\n",
    "    \n",
    "    # Print JSON object\n",
    "    print(json.dumps(config, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"column_names\": [\n",
      "        \"Destination_1\",\n",
      "        \"Destination_2\",\n",
      "        \"Destination_3\",\n",
      "        \"Destination_4\",\n",
      "        \"Destination_5\",\n",
      "        \"Destination_6\",\n",
      "        \"Destination_7\",\n",
      "        \"Destination_8\",\n",
      "        \"Destination_9\",\n",
      "        \"Destination_10\",\n",
      "        \"Destination_11\",\n",
      "        \"Destination_12\",\n",
      "        \"Destination_13\",\n",
      "        \"Destination_14\",\n",
      "        \"Destination_15\",\n",
      "        \"Destination_16\",\n",
      "        \"number_transactions_above_9k_cad\",\n",
      "        \"min_day_to_max_day_range\",\n",
      "        \"avg_transaction_frequency_in_days\",\n",
      "        \"transaction_frequency_variance_in_days\",\n",
      "        \"Is FanOut\"\n",
      "    ],\n",
      "    \"num_col_idx\": [\n",
      "        16,\n",
      "        17,\n",
      "        18,\n",
      "        19\n",
      "    ],\n",
      "    \"cat_col_idx\": [\n",
      "        0,\n",
      "        1,\n",
      "        2,\n",
      "        3,\n",
      "        4,\n",
      "        5,\n",
      "        6,\n",
      "        7,\n",
      "        8,\n",
      "        9,\n",
      "        10,\n",
      "        11,\n",
      "        12,\n",
      "        13,\n",
      "        14,\n",
      "        15,\n",
      "        20\n",
      "    ],\n",
      "    \"target_col_idx\": [\n",
      "        20\n",
      "    ],\n",
      "    \"file_type\": \"csv\",\n",
      "    \"data_path\": \"/h/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/data/raw_data/IBM_AML_FeatureEngineered_FS1/train.csv\",\n",
      "    \"test_path\": \"/h/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/data/test_data/IBM_AML_FeatureEngineered_FS1/test.csv\",\n",
      "    \"column_info\": {\n",
      "        \"Destination_1\": \"str\",\n",
      "        \"Destination_2\": \"str\",\n",
      "        \"Destination_3\": \"str\",\n",
      "        \"Destination_4\": \"str\",\n",
      "        \"Destination_5\": \"str\",\n",
      "        \"Destination_6\": \"str\",\n",
      "        \"Destination_7\": \"str\",\n",
      "        \"Destination_8\": \"str\",\n",
      "        \"Destination_9\": \"str\",\n",
      "        \"Destination_10\": \"str\",\n",
      "        \"Destination_11\": \"str\",\n",
      "        \"Destination_12\": \"str\",\n",
      "        \"Destination_13\": \"str\",\n",
      "        \"Destination_14\": \"str\",\n",
      "        \"Destination_15\": \"str\",\n",
      "        \"Destination_16\": \"str\",\n",
      "        \"number_transactions_above_9k_cad\": \"float\",\n",
      "        \"min_day_to_max_day_range\": \"float\",\n",
      "        \"avg_transaction_frequency_in_days\": \"float\",\n",
      "        \"transaction_frequency_variance_in_days\": \"float\",\n",
      "        \"Is FanOut\": \"str\"\n",
      "    },\n",
      "    \"train_num\": 16,\n",
      "    \"test_num\": 4\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "generate_json_config(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Dataset\n",
    "\n",
    "For more explanation of different steps in this section, please refer to TabDDPM's notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/\"\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw_data\")\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed_data\")\n",
    "SYNTH_DATA_DIR = os.path.join(DATA_DIR, \"synthetic_data\")\n",
    "DATA_NAME = \"adult\" #\"IBM_AML_FeatureEngineered_FS1\"\n",
    "\n",
    "MODEL_PATH = \"models/tabsyn\"\n",
    "# process data\n",
    "INFO_DIR = \"data_info\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (20, 21)\n",
      "Train dataset shape: (16, 21)\n",
      "Test dataset shape: (4, 21)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_and_save_dataset(df, train_size=0.8, random_state=42):\n",
    "    # Split the dataset\n",
    "    train_df, test_df = train_test_split(df, train_size=train_size, random_state=random_state)\n",
    "    \n",
    "    # Save to CSV files\n",
    "    train_df.to_csv('data/raw_data/IBM_AML_FeatureEngineered_FS1/train.csv', index=False)\n",
    "    test_df.to_csv('data/raw_data/IBM_AML_FeatureEngineered_FS1/test.csv', index=False)\n",
    "    \n",
    "    # Print the shapes of the resulting datasets\n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "    print(f\"Train dataset shape: {train_df.shape}\")\n",
    "    print(f\"Test dataset shape: {test_df.shape}\")\n",
    "\n",
    "# Assuming you have a DataFrame called 'df'\n",
    "split_and_save_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_URL_DICT_UCI = {\n",
    "    \"adult\": \"https://archive.ics.uci.edu/static/public/2/adult.zip\",\n",
    "    \"default\": \"https://archive.ics.uci.edu/static/public/350/default+of+credit+card+clients.zip\",\n",
    "    \"magic\": \"https://archive.ics.uci.edu/static/public/159/magic+gamma+telescope.zip\",\n",
    "    \"shoppers\": \"https://archive.ics.uci.edu/static/public/468/online+shoppers+purchasing+intention+dataset.zip\",\n",
    "    \"beijing\": \"https://archive.ics.uci.edu/static/public/381/beijing+pm2+5+data.zip\",\n",
    "    \"news\": \"https://archive.ics.uci.edu/static/public/332/online+news+popularity.zip\",\n",
    "}\n",
    "\n",
    "# For shared directory you can change it to \"/projects/diffusion_bootcamp/data/tabular\"\n",
    "DATA_DIR = \"data/\"\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw_data\")\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed_data\")\n",
    "SYNTH_DATA_DIR = os.path.join(DATA_DIR, \"synthetic_data\")\n",
    "DATA_NAME = \"adult\"\n",
    "\n",
    "MODEL_PATH = \"models/tabsyn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = \"IBM_AML_FeatureEngineered_FS1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing dataset IBM_AML_FeatureEngineered_FS1 from UCI.\n",
      "Already downloaded.\n",
      "XXXXXXXXXXXXXXXXXX(16, 21)\n",
      "num Col index:[16, 17, 18, 19], Cat col index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Processing and Saving IBM_AML_FeatureEngineered_FS1 Successfully!\n",
      "Dataset Name: IBM_AML_FeatureEngineered_FS1\n",
      "Total Size: 20\n",
      "Train Size: 16\n",
      "Test Size: 4\n",
      "Number of Numerical Columns: 4\n",
      "Number of Categorical Columns: 17\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "download_from_uci(DATA_NAME, RAW_DATA_DIR, NAME_URL_DICT_UCI)\n",
    "\n",
    "# process data\n",
    "INFO_DIR = \"data_info\"\n",
    "process_data(DATA_NAME, INFO_DIR, DATA_DIR)\n",
    "\n",
    "# review data\n",
    "df = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, DATA_NAME, \"train.csv\"))\n",
    "# visualize_default(df).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat_col_idx': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
      " 'column_info': {'0': {},\n",
      "                 '1': {},\n",
      "                 '10': {},\n",
      "                 '11': {},\n",
      "                 '12': {},\n",
      "                 '13': {},\n",
      "                 '14': {},\n",
      "                 '15': {},\n",
      "                 '16': {},\n",
      "                 '17': {},\n",
      "                 '18': {},\n",
      "                 '19': {},\n",
      "                 '2': {},\n",
      "                 '20': {},\n",
      "                 '3': {},\n",
      "                 '4': {},\n",
      "                 '5': {},\n",
      "                 '6': {},\n",
      "                 '7': {},\n",
      "                 '8': {},\n",
      "                 '9': {},\n",
      "                 'categorizes': [0.0],\n",
      "                 'max': 1.668839537603111,\n",
      "                 'min': 1.668839537603111,\n",
      "                 'type': 'categorical'},\n",
      " 'column_names': ['Destination_1',\n",
      "                  'Destination_2',\n",
      "                  'Destination_3',\n",
      "                  'Destination_4',\n",
      "                  'Destination_5',\n",
      "                  'Destination_6',\n",
      "                  'Destination_7',\n",
      "                  'Destination_8',\n",
      "                  'Destination_9',\n",
      "                  'Destination_10',\n",
      "                  'Destination_11',\n",
      "                  'Destination_12',\n",
      "                  'Destination_13',\n",
      "                  'Destination_14',\n",
      "                  'Destination_15',\n",
      "                  'Destination_16',\n",
      "                  'number_transactions_above_9k_cad',\n",
      "                  'min_day_to_max_day_range',\n",
      "                  'avg_transaction_frequency_in_days',\n",
      "                  'transaction_frequency_variance_in_days',\n",
      "                  'Is FanOut'],\n",
      " 'data_path': '/h/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/data/raw_data/IBM_AML_FeatureEngineered_FS1/train.csv',\n",
      " 'file_type': 'csv',\n",
      " 'header': 'infer',\n",
      " 'idx_mapping': {'0': 4,\n",
      "                 '1': 5,\n",
      "                 '10': 14,\n",
      "                 '11': 15,\n",
      "                 '12': 16,\n",
      "                 '13': 17,\n",
      "                 '14': 18,\n",
      "                 '15': 19,\n",
      "                 '16': 0,\n",
      "                 '17': 1,\n",
      "                 '18': 2,\n",
      "                 '19': 3,\n",
      "                 '2': 6,\n",
      "                 '20': 20,\n",
      "                 '3': 7,\n",
      "                 '4': 8,\n",
      "                 '5': 9,\n",
      "                 '6': 10,\n",
      "                 '7': 11,\n",
      "                 '8': 12,\n",
      "                 '9': 13},\n",
      " 'idx_name_mapping': {'0': 'Destination_1',\n",
      "                      '1': 'Destination_2',\n",
      "                      '10': 'Destination_11',\n",
      "                      '11': 'Destination_12',\n",
      "                      '12': 'Destination_13',\n",
      "                      '13': 'Destination_14',\n",
      "                      '14': 'Destination_15',\n",
      "                      '15': 'Destination_16',\n",
      "                      '16': 'number_transactions_above_9k_cad',\n",
      "                      '17': 'min_day_to_max_day_range',\n",
      "                      '18': 'avg_transaction_frequency_in_days',\n",
      "                      '19': 'transaction_frequency_variance_in_days',\n",
      "                      '2': 'Destination_3',\n",
      "                      '20': 'Is FanOut',\n",
      "                      '3': 'Destination_4',\n",
      "                      '4': 'Destination_5',\n",
      "                      '5': 'Destination_6',\n",
      "                      '6': 'Destination_7',\n",
      "                      '7': 'Destination_8',\n",
      "                      '8': 'Destination_9',\n",
      "                      '9': 'Destination_10'},\n",
      " 'inverse_idx_mapping': {'0': 16,\n",
      "                         '1': 17,\n",
      "                         '10': 6,\n",
      "                         '11': 7,\n",
      "                         '12': 8,\n",
      "                         '13': 9,\n",
      "                         '14': 10,\n",
      "                         '15': 11,\n",
      "                         '16': 12,\n",
      "                         '17': 13,\n",
      "                         '18': 14,\n",
      "                         '19': 15,\n",
      "                         '2': 18,\n",
      "                         '20': 20,\n",
      "                         '3': 19,\n",
      "                         '4': 0,\n",
      "                         '5': 1,\n",
      "                         '6': 2,\n",
      "                         '7': 3,\n",
      "                         '8': 4,\n",
      "                         '9': 5},\n",
      " 'metadata': {'columns': {'0': {'sdtype': 'categorical'},\n",
      "                          '1': {'sdtype': 'categorical'},\n",
      "                          '10': {'sdtype': 'categorical'},\n",
      "                          '11': {'sdtype': 'categorical'},\n",
      "                          '12': {'sdtype': 'categorical'},\n",
      "                          '13': {'sdtype': 'categorical'},\n",
      "                          '14': {'sdtype': 'categorical'},\n",
      "                          '15': {'sdtype': 'categorical'},\n",
      "                          '16': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '17': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '18': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '19': {'computer_representation': 'Float',\n",
      "                                 'sdtype': 'numerical'},\n",
      "                          '2': {'sdtype': 'categorical'},\n",
      "                          '20': {'sdtype': 'categorical'},\n",
      "                          '3': {'sdtype': 'categorical'},\n",
      "                          '4': {'sdtype': 'categorical'},\n",
      "                          '5': {'sdtype': 'categorical'},\n",
      "                          '6': {'sdtype': 'categorical'},\n",
      "                          '7': {'sdtype': 'categorical'},\n",
      "                          '8': {'sdtype': 'categorical'},\n",
      "                          '9': {'sdtype': 'categorical'}}},\n",
      " 'name': 'IBM_AML_FeatureEngineered_FS1',\n",
      " 'num_col_idx': [16, 17, 18, 19],\n",
      " 'target_col_idx': [20],\n",
      " 'task_type': 'binclass',\n",
      " 'test_num': 4,\n",
      " 'test_path': '/h/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/data/test_data/IBM_AML_FeatureEngineered_FS1/test.csv',\n",
      " 'train_num': 16}\n"
     ]
    }
   ],
   "source": [
    "# review json file and its contents\n",
    "with open(f\"{PROCESSED_DATA_DIR}/{DATA_NAME}/info.json\", \"r\") as file:\n",
    "    data_info = json.load(file)\n",
    "pprint(data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabSyn Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will describe the design of TabSyn as well as its main hyperparameters loaded through config, which affect the modelâ€™s effectiveness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TabSyn** consists of two parts:\n",
    "1. A *variational auto-encoder (VAE)* which learns a joint representation space for the given tabular data.\n",
    "2. A *Diffusion model* which learns the distribution of data in the joint representation space.\n",
    "\n",
    "The figure below shows a diagram of the TabSyn model.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn.jpg\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VAE**\n",
    "\n",
    "The left-side of the figure shows the VAE which operates in the original data space. The VAE itself consists of two parts: an encoder and a decoder. It also contains the corresponding tokenizer and detokenizer.\n",
    "Each row of the input tabular data ($\\pmb{x}$) is tokenized, then embedded by a transformer. Another transformer decodes the embeddings and a detokenizer reconstructs the table ($\\pmb{\\tilde{x}}$). The VAE is trained by minimizing the reconstruction loss between $\\pmb{x}$ and $\\pmb{\\tilde{x}}$.\n",
    "\n",
    "After the VAE is fully trained, the whole data ($\\pmb{x}$) is tokenized and embedded. The embedding of each row is flattened to form a 1-dimensional vector $\\pmb{z}$.\n",
    "These 1-dimensional embeddings for all rows are stored on disk, and will later be used to train the diffusion model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diffusion**\n",
    "\n",
    "The right-side of the figure shows the diffusion model which operates in the latent representation space; in other words, it only *sees* the embeddings obtained by the VAE, not the original tabular data.\n",
    "The diffusion model can be similarly divided into two parts: a forward process, and a reverse process.\n",
    "\n",
    "The forward process receives the embedded data points. A single data point is denoted by $\\pmb{z_0}$ in the figure. Gaussian noise is incrementally added to the embeddings in numerous incremental steps during the forward process. The number of the steps is denoted by $T$ in the figure. $T$ should be high enough that the distribution of embeddings at step $t=T$ is essentially a standard Gaussian distribution; in other words, the signal-to-noise ratio is practically zero.\n",
    "\n",
    "The reverse process, on the other hand, learns to *predict* an earlier-step embedding (e.g. $\\pmb{z_{t-\\Delta t}}$) from a later-step embedding (e.g. $\\pmb{z_t}$) via a neural network.\n",
    "\n",
    "After the diffusion model is fully trained, the reverse process can estimate the data distribution at step $t=0$ if it receives a standard Gaussian distribution at step $t=T$. New data points can be synthesized by sampling from this estimated distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will load the configuration file that contains the hyperparameters for the TabSyn model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_params': {'lambd': 0.7, 'max_beta': 0.01, 'min_beta': 1e-05},\n",
      " 'model_params': {'d_token': 4, 'factor': 32, 'n_head': 1, 'num_layers': 2},\n",
      " 'task_type': 'binclass',\n",
      " 'train': {'diffusion': {'batch_size': 4096,\n",
      "                         'num_dataset_workers': 4,\n",
      "                         'num_epochs': 9},\n",
      "           'optim': {'diffusion': {'factor': 0.9,\n",
      "                                   'lr': 0.001,\n",
      "                                   'patience': 20,\n",
      "                                   'weight_decay': 0},\n",
      "                     'vae': {'factor': 0.95,\n",
      "                             'lr': 0.001,\n",
      "                             'patience': 10,\n",
      "                             'weight_decay': 0}},\n",
      "           'vae': {'batch_size': 4096,\n",
      "                   'num_dataset_workers': 4,\n",
      "                   'num_epochs': 10}},\n",
      " 'transforms': {'cat_encoding': None,\n",
      "                'cat_min_frequency': None,\n",
      "                'cat_nan_policy': None,\n",
      "                'normalization': 'quantile',\n",
      "                'num_nan_policy': 'mean',\n",
      "                'y_policy': 'default'}}\n"
     ]
    }
   ],
   "source": [
    "config_path = os.path.join(\"src/baselines/tabsyn/configs\", f\"{DATA_NAME}.toml\")\n",
    "raw_config = src.load_config(config_path)\n",
    "\n",
    "pprint(raw_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file is a TOML file that contains the following hyperparameters:\n",
    "\n",
    "1. **model_params:** specifies the structure of the transformers (both encoder and decoder) in the VAE model, including number of transformer layers, number of self-attnetion heads and token dimension.\n",
    "\n",
    "2. **transforms:** specifies the transformations and preprocessing of the data before tokenization, such as cleaning, normalization, and encoding.\n",
    "    - For preprocessing numerical features, we use the gaussian quantile transformation and replace the NaN values with mean of each row.\n",
    "    - For categorical features, we use the one-hot encoding method. NaN values are left unchanged, but we have the option to replace them. We have the option to drop the values that appear with less than a given minimum frequency under each column. Furthermore, we have the option to add an extra encoding step for categorical features during tokenization.\n",
    "\n",
    "3. **train.vae:** specifies training parameters of the VAE, including batch size, number of epochs, and number of dataset workers.\n",
    "\n",
    "4. **train.diffusion:** specifies the same training parameters as above for the diffusion model.\n",
    "\n",
    "5. **train.optim.vae:** specifies the parameters of the *Adam* optimizer and the `ReduceLROnPlateau` learning rate scheduler used to train the VAE. Optimizer parameters include initial learning rate and weight decay. LR scheduler parameters includer `factor` and `patience`.\n",
    "\n",
    "6. **train.optim.diffusion:** specifies the same parameters as above for the diffusion model.\n",
    "\n",
    "7. **loss_params:** specifies parameters of the loss function used to train the VAE including `max_beta`, `min_beta` and `lambd`.\n",
    "\n",
    "$\\beta$ is the coefficient of the KL divergence term in the VAE loss formula,\n",
    "\n",
    "$\\mathcal{L}_{vae} = \\mathcal{L}_{mse} + \\mathcal{L}_{ce} + \\beta \\mathcal{L}_{kl}$\n",
    ".\n",
    "\n",
    "Parameters `max_beta` and `min_beta` determine the range of $\\beta$. $\\beta$ is first set to `max_beta`. If the loss stops decreasing for a certain number of epochs (e.g. $10$ epochs), then at the end of each epoch after that (e.g. epoch $11$, $12$, etc.) $\\beta$ is decreased by a factor of `lambd`,\n",
    "$\\beta_{new} = \\lambda \\beta_{curr}$,\n",
    "until it reaches `beta_min`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we pre-process the data and make a dataset object.\n",
    "\n",
    "First, we determine transformations needed for the dataset, such as normalization and cleaning, in `transforms`. Next, using `preprocess` function we load the data from disk in arrays that contain both training and test data (`X_num` and `X_cat`), as well as the number of categories for each categorical feature (`categories`) and the number of numerical features (`d_numerical`).\n",
    "\n",
    "We then separate the train and test data in different arrays and convert them to Pytorch tensors.\n",
    "We create a dataset object (`TabularDataset`) with the train data. `TabularDataset` is a simple module which returns the tokens of a single row at a time. Each row constiutes a single data sample in TabSyn. Afterwards, we create a Dataloader for the train data using the `batch_size` and `num_workers` specified in config.\n",
    "\n",
    "In contrast, we keep the test data as tensors (`X_test_num` and `X_test_cat`). If a GPU is available, we move these tensors to GPU so that they can be accessed by the model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaNs in numerical features, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/ws_aabboud/diffusion_model_bootcamp/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# preprocess data\n",
    "X_num, X_cat, categories, d_numerical = preprocess(os.path.join(PROCESSED_DATA_DIR, DATA_NAME),\n",
    "                                                   transforms = raw_config[\"transforms\"],\n",
    "                                                   task_type = raw_config[\"task_type\"])\n",
    "\n",
    "# separate train and test data\n",
    "X_train_num, X_test_num = X_num\n",
    "X_train_cat, X_test_cat = X_cat\n",
    "\n",
    "# convert to float tensor\n",
    "X_train_num, X_test_num = torch.tensor(X_train_num).float(), torch.tensor(X_test_num).float()\n",
    "X_train_cat, X_test_cat =  torch.tensor(X_train_cat), torch.tensor(X_test_cat)\n",
    "\n",
    "# create dataset module\n",
    "train_data = TabularDataset(X_train_num.float(), X_train_cat)\n",
    "\n",
    "# move test data to gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_test_num = X_test_num.float().to(device)\n",
    "X_test_cat = X_test_cat.to(device)\n",
    "\n",
    "# create train dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size = raw_config[\"train\"][\"vae\"][\"batch_size\"],\n",
    "    shuffle = True,\n",
    "    num_workers = raw_config[\"train\"][\"vae\"][\"num_dataset_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the model using the `TabSyn` class. `TabSyn` class takes the following arguments:\n",
    "\n",
    "1. `train_loader`: dataloader for train data.\n",
    "2. `X_test_num`: numerical features of the test data.\n",
    "3. `X_test_cat`: categorical features of the train data.\n",
    "4. `num_numerical_features`: number of numerical features in the dataset.\n",
    "5. `num_classes`: number of classes (i.e. categories) of each categorical feature in the dataset.\n",
    "6. `device`: the device on which the model and data exist, either \"cpu\" or \"cuda\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabsyn = TabSyn(train_loader,\n",
    "                X_test_num, X_test_cat,\n",
    "                num_numerical_features = d_numerical,\n",
    "                num_classes = categories,\n",
    "                device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TabSyn` class has the tools to instantiate VAE and diffusion models, train both, and sample from the trained diffusion model.\n",
    "We will demonstrate how to use these tools in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE and the diffusion model are trained independently. The following subsections explain each training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### A. Train VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to instantiate the VAE using the `instantiate_vae` method. This method takes the VAE model hyperparameters, optimizer and lr scheduler parameters from config, and instantiates them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully instantiated VAE model.\n"
     ]
    }
   ],
   "source": [
    "# instantiate VAE model for training\n",
    "tabsyn.instantiate_vae(**raw_config[\"model_params\"], optim_params = raw_config[\"train\"][\"optim\"][\"vae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have instantiated the VAE, we can train it using the `train_vae` function.\n",
    "This function receives the loss hyperparameters and number of epochs from the config.\n",
    "Moreover, it recieves `save_path` which is the directory where trained model checkpoints will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                                                                                                                                                                           | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasLtMatmul( ltHandle, computeDesc.descriptor(), &alpha_val, mat1_ptr, Adesc.descriptor(), mat2_ptr, Bdesc.descriptor(), &beta_val, result_ptr, Cdesc.descriptor(), result_ptr, Cdesc.descriptor(), &heuristicResult.algo, workspace.data_ptr(), workspaceSize, at::cuda::getCurrentCUDAStream())`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/vae\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtabsyn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_vae\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mraw_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss_params\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mraw_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvae\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATA_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvae\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/src/baselines/tabsyn/pipeline.py:110\u001b[0m, in \u001b[0;36mTabSyn.train_vae\u001b[0;34m(self, max_beta, min_beta, lambd, num_epochs, save_path)\u001b[0m\n\u001b[1;32m    107\u001b[0m batch_num \u001b[38;5;241m=\u001b[39m batch_num\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    108\u001b[0m batch_cat \u001b[38;5;241m=\u001b[39m batch_cat\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 110\u001b[0m Recon_X_num, Recon_X_cat, mu_z, std_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_cat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m loss_mse, loss_ce, loss_kld, train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(batch_num, batch_cat, Recon_X_num, Recon_X_cat, mu_z, std_z)\n\u001b[1;32m    114\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_mse \u001b[38;5;241m+\u001b[39m loss_ce \u001b[38;5;241m+\u001b[39m beta \u001b[38;5;241m*\u001b[39m loss_kld\n",
      "File \u001b[0;32m/fs01/home/ws_aabboud/diffusion_model_bootcamp/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fs01/home/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/src/baselines/tabsyn/model/vae.py:364\u001b[0m, in \u001b[0;36mModel_VAE.forward\u001b[0;34m(self, x_num, x_cat)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_num, x_cat):\n\u001b[0;32m--> 364\u001b[0m     h, mu_z, std_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_cat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;66;03m# recon_x_num, recon_x_cat = self.Reconstructor(h[:, 1:])\u001b[39;00m\n\u001b[1;32m    367\u001b[0m     recon_x_num, recon_x_cat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReconstructor(h)\n",
      "File \u001b[0;32m/fs01/home/ws_aabboud/diffusion_model_bootcamp/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fs01/home/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/src/baselines/tabsyn/model/vae.py:295\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x_num, x_cat)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_num, x_cat):\n\u001b[1;32m    293\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTokenizer(x_num, x_cat)\n\u001b[0;32m--> 295\u001b[0m     mu_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_mu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     std_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_logvar(x)\n\u001b[1;32m    298\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu_z, std_z)\n",
      "File \u001b[0;32m/fs01/home/ws_aabboud/diffusion_model_bootcamp/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fs01/home/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/src/baselines/tabsyn/model/vae.py:220\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m    219\u001b[0m     x_residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_residual(x, layer, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 220\u001b[0m     x_residual \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# for the last attention, it is enough to process only [CLS]\u001b[39;49;00m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_residual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_residual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_end_residual(x, x_residual, layer, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    228\u001b[0m     x_residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_residual(x, layer, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/fs01/home/ws_aabboud/diffusion_model_bootcamp/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fs01/home/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/src/baselines/tabsyn/model/vae.py:117\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, x_q, x_kv, key_compression, value_compression)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_q, x_kv, key_compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, value_compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 117\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_q\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_k(x_kv), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_v(x_kv)\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m [q, k, v]:\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/fs01/home/ws_aabboud/diffusion_model_bootcamp/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fs01/home/ws_aabboud/diffusion_model_bootcamp/.venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasLtMatmul( ltHandle, computeDesc.descriptor(), &alpha_val, mat1_ptr, Adesc.descriptor(), mat2_ptr, Bdesc.descriptor(), &beta_val, result_ptr, Cdesc.descriptor(), result_ptr, Cdesc.descriptor(), &heuristicResult.algo, workspace.data_ptr(), workspaceSize, at::cuda::getCurrentCUDAStream())`"
     ]
    }
   ],
   "source": [
    "os.makedirs(f\"{MODEL_PATH}/{DATA_NAME}/vae\")\n",
    "tabsyn.train_vae(**raw_config[\"loss_params\"],\n",
    "                 num_epochs = raw_config[\"train\"][\"vae\"][\"num_epochs\"],\n",
    "                 save_path = os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the VAE, we embed the training data with the trained encoder and store the embeddings in a direcotry specified by `vae_ckpt_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed all inputs in the latent space\n",
    "tabsyn.save_vae_embeddings(X_train_num, X_train_cat,\n",
    "                           vae_ckpt_dir = os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Train Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have stored the training data embeddings, we need to load and prepare them for the diffusion model.\n",
    "We load the embeddings using `load_vae_embeddings`. We normalize the embeddings by subtracting the mean and dividing by the standard deviation. Then, we create a Dataloader with the specified `batch_size` and `num_workers` from the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load latent space embeddings\n",
    "train_z, _ = tabsyn.load_latent_embeddings(os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))  # train_z dim: B x in_dim\n",
    "\n",
    "# normalize embeddings\n",
    "mean, std = train_z.mean(0), train_z.std(0)\n",
    "train_z = (train_z - mean) / std\n",
    "latent_train_data = train_z\n",
    "\n",
    "# create data loader\n",
    "latent_train_loader = DataLoader(\n",
    "    latent_train_data,\n",
    "    batch_size = raw_config[\"train\"][\"diffusion\"][\"batch_size\"],\n",
    "    shuffle = True,\n",
    "    num_workers = raw_config[\"train\"][\"diffusion\"][\"num_dataset_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is ready, we instantiate the diffusion model with `instantiate_diffusion`. The input dimension and hidden dimention of the diffusion model is determined by the dimension of the embeddings. \n",
    "Moreover, we instantiate the optimizer and lr scheduler using hyperparameters from config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate diffusion model for training\n",
    "tabsyn.instantiate_diffusion(in_dim = train_z.shape[1], hid_dim = train_z.shape[1], optim_params = raw_config[\"train\"][\"optim\"][\"diffusion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the diffusion model with `train_diffusion` function.\n",
    "This function takes the following arguements:\n",
    "1. `latent_train_loader`: dataloader for the latent representations which are used to train the diffusion model.\n",
    "2. `num_epochs`: number of training epochs.\n",
    "3. `ckpt_path`: directory where the model checkpoints will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"{MODEL_PATH}/{DATA_NAME}\")\n",
    "# train diffusion model\n",
    "tabsyn.train_diffusion(latent_train_loader,\n",
    "                       num_epochs = raw_config[\"train\"][\"diffusion\"][\"num_epochs\"],\n",
    "                       ckpt_path = os.path.join(MODEL_PATH, DATA_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training model from scratch, we can also load weights of a pre-trained model from a given checkpoint with `load_model_state` function.\n",
    "If we haven't instantiated the VAE and diffusion model beforehand, we need to instantiate them first using `instantiate_vae` and `instantiate_diffusion` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate VAE model\n",
    "tabsyn.instantiate_vae(**raw_config[\"model_params\"], optim_params = None)\n",
    "\n",
    "latent_embeddings_path = \"/projects/diffusion_bootcamp/models/tabular/tabsyn/default/vae\"\n",
    "# load latent embeddings of input data\n",
    "train_z, token_dim = tabsyn.load_latent_embeddings(latent_embeddings_path)\n",
    "\n",
    "# instantiate diffusion model\n",
    "tabsyn.instantiate_diffusion(in_dim = train_z.shape[1], hid_dim = train_z.shape[1], optim_params = None)\n",
    "\n",
    "pretrained_model_path = \"/projects/diffusion_bootcamp/models/tabular/tabsyn/default\"\n",
    "# load state from checkpoint\n",
    "tabsyn.load_model_state(ckpt_dir = pretrained_model_path,\n",
    "                        dif_ckpt_name = \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained the model effectively, using `sample` function we can generate synthetic data starting from compelete noise. The input of this function is as follows:\n",
    "\n",
    "1. `train_z`: latent embeddings of the training data.\n",
    "2. `info`: info about the data from the json file we reviewed at the beginning of this notebook.\n",
    "3. `num_inverse`: detokenizer for numerical features.\n",
    "4. `cat_inverse`: detokenizer for categorical features.\n",
    "5. `save_path`: file-path where the synthetic table will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data info file\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, DATA_NAME, \"info.json\"), \"r\") as file:\n",
    "    data_info = json.load(file)\n",
    "data_info[\"token_dim\"] = token_dim\n",
    "\n",
    "# get inverse tokenizers\n",
    "_, _, categories, d_numerical, num_inverse, cat_inverse = preprocess(os.path.join(PROCESSED_DATA_DIR, DATA_NAME),\n",
    "                                                                     transforms = raw_config[\"transforms\"],\n",
    "                                                                     task_type = raw_config[\"task_type\"],\n",
    "                                                                     inverse = True)\n",
    "\n",
    "# sample data\n",
    "num_samples = train_z.shape[0]\n",
    "in_dim = train_z.shape[1] \n",
    "mean_input_emb = train_z.mean(0)\n",
    "tabsyn.sample(num_samples,\n",
    "              in_dim,\n",
    "              mean_input_emb,\n",
    "              info = data_info,\n",
    "              num_inverse = num_inverse,\n",
    "              cat_inverse = cat_inverse,\n",
    "              save_path = os.path.join(SYNTH_DATA_DIR, DATA_NAME, \"tabsyn.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally here, we review the synthesized data. In the following `evaluate_synthetic_data.ipynb` notebook, we will evaluate this synthesized data with respect to various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(SYNTH_DATA_DIR, DATA_NAME, \"tabsyn.csv\"))\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "visualize_default(df).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zhang, Hengrui, et al.** \"Mixed-type tabular data synthesis with score-based diffusion in latent space.\" *International Conference on Learning Representations (ICLR)* (2023).\n",
    "\n",
    "**GitHub Repository:** [Amazon Science - Tabsyn](https://github.com/amazon-science/tabsyn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_models",
   "language": "python",
   "name": "diffusion_models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
