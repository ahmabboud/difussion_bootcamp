{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "royal-phase",
   "metadata": {},
   "source": [
    "<center>\n",
    "  \n",
    "# Synthetic Data Evaluation\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-annotation",
   "metadata": {},
   "source": [
    "In this notebook, we will evaluate the generated synthetic data using both the TabDDPM and TabSyn algorithms based on various metrics. The notebook is organized as follows:\n",
    "\n",
    "1. [Imports and Setup]()\n",
    "\n",
    "\n",
    "2. [Density Estimation of Single Column and Pair-wise Correlation]()\n",
    "    \n",
    "    \n",
    "3. [$\\alpha$-Precision and $\\beta$-Recall ]()\n",
    "\n",
    "    \n",
    "4. [Machine Learning Efficiency]()\n",
    "\n",
    "\n",
    "5. [Privacy Protection: Distance to Closest Record (DCR)]()\n",
    "\n",
    "\n",
    "6. [Detection: Classifier Two Sample Tests (C2ST)]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-cornwall",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-speaker",
   "metadata": {},
   "source": [
    "First, we will import functions to evaluate the data using various metrics. Then, we will define the paths to the real train and test data, as well as the TabDDPM and TabSyn generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff6cc966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "stretch-terminal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pprint import pprint\n",
    "\n",
    "from scripts.impute import impute\n",
    "from scripts.eval.eval_impute import eval_impute\n",
    "from scripts.eval.eval_density import eval_density\n",
    "from scripts.eval.eval_quality import eval_quality\n",
    "from scripts.eval.eval_mle import eval_mle\n",
    "from scripts.eval.eval_dcr import eval_dcr\n",
    "from scripts.eval.eval_detection import eval_detection\n",
    "\n",
    "dataname = \"IBM_AML_FeatureEngineered_FS1\"\n",
    "\n",
    "# For shared directory you can change it to \"/projects/diffusion_bootcamp/data/tabular\"\n",
    "DATA_DIR = \"data\"\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed_data\")\n",
    "\n",
    "TRAIN_DATA_PATH = f\"{DATA_DIR}/processed_data/{dataname}/train.csv\"\n",
    "TEST_DATA_PATH = f\"{DATA_DIR}/test_data/{dataname}/test.csv\"\n",
    "# TABDDPM_DATA_PATH = f\"{DATA_DIR}/synthetic_data/{dataname}/tabddpm.csv\"\n",
    "TABSYN_DATA_PATH = f\"{DATA_DIR}/synthetic_data/{dataname}/tabsyn.csv\"\n",
    "INFO_PATH = f\"{DATA_DIR}/processed_data/{dataname}/info.json\"\n",
    "\n",
    "\n",
    "SYNTH_DATA_DIR = os.path.join(DATA_DIR, \"synthetic_data\")\n",
    "DATA_NAME = \"IBM_AML_FeatureEngineered_FS1\"\n",
    "\n",
    "MODEL_PATH = \"models/tabsyn\"\n",
    "# process data\n",
    "INFO_DIR = \"data_info\"\n",
    "IMPUTE_PATH = f\"{DATA_DIR}/test_data/{dataname}/test.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-virus",
   "metadata": {},
   "source": [
    "# Density Estimation of Single Column and Pair-wise Correlation\n",
    "\n",
    "Two metrics are computed in this section: column shapes and column pair trends.\n",
    "We explain each in the following subsections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a383a5c",
   "metadata": {},
   "source": [
    "## Single Column Similarity Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc81863e",
   "metadata": {},
   "source": [
    "Essentially, each column in a table represents a single feature which accepts various values from a certain type; e.g. numerical, categorical, datetime or boolean. This feature can be described as a random variable where the values listed under the column are its samples. As a result, the density distribution of each column can be computed and compared between the real data and the synthetic data.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn_column_shapes_2.png\" width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "The better the distributions match each other, the better the quality of synthetic data.\n",
    "\n",
    "The similarity of the two distriutions can be measured via different metrics for different data types; for example, **KSComplement (Kolmogorov-Smirnov Complement)** is used for numerical features and **TVComplement (Total Variation Distance Complement)** for categorical features. The overall Column Shape Score for the whole table is the average of column shape similarity scores of all columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d521918f",
   "metadata": {},
   "source": [
    "**KST (Kolmogorov-Smirnov Test)**\n",
    "computes the cumulative distribution function (CDF) of a numerical random variable for real and synthetic data. Then, finds the maximum different between the two CDFs $M$. Finally, the *KSComplement* score is defined as $1 - M$ so that the higher the score, the more similar the distributions.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn_kst.png\" width=\"1000\"/>\n",
    "</p>\n",
    "\n",
    "**TVD (Total Variation Distance)**\n",
    "computes the frequency of each category's appearance under a certain column and defines it as the said categorie's probability. Then, it computes the sum of differences of probabilities between real and synthetic data as \n",
    "$\\delta(R, S) = \\frac{1}{2} \\sum_{\\omega \\in \\Omega}  | R_\\omega - S_\\omega |$\n",
    ".\n",
    "\n",
    "Here, $\\omega$ describes all the possible categories in a column, $\\Omega$. Meanwhile, $R$ and $S$ refer to the real and synthetic frequencies for those categories. The *TVComplement* returns $1-TVD$ so that a higher score means higher quality.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn_tvd.png\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb4e44d",
   "metadata": {},
   "source": [
    "## Pair-wise Correlation Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3abc8c0",
   "metadata": {},
   "source": [
    "The correlation between two random variables describes how they vary in relation to each other. The higher the score, the more the trends are alike.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn_column_pair_trends.png\" width=\"900\"/>\n",
    "</p>\n",
    "\n",
    "Here, we use different metrics to compute the correlation between different pairs of data types:\n",
    "\n",
    "| Column Type | Metric |\n",
    "| ----------- | ------ |\n",
    "| numerical & numerical | [correlation similarity](https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/correlationsimilarity) |\n",
    "| categorical & categorical | [contingency similarity](https://docs.sdv.dev/sdmetrics/metrics/metrics-glossary/contingencysimilarity) |\n",
    "| numerical & categorical | discretize the numerical columns into bins, then apply contingency similarity. |\n",
    "\n",
    "This yields a score between every pair of columns. The **Column Pair Trends** score is the average of all the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "useful-mention",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "(1/2) Evaluating Column Shapes: : 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 37/37 [00:00<00:00, 1087.13it/s]\n",
      "(2/2) Evaluating Column Pair Trends: : 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 666/666 [00:10<00:00, 65.05it/s]\n",
      "\n",
      "Overall Score: 57.33%\n",
      "\n",
      "Properties:\n",
      "- Column Shapes: 65.77%\n",
      "- Column Pair Trends: 48.89%\n",
      "Shape: 0.6576576576576577\n",
      "Trend: 0.48893530312365585\n"
     ]
    }
   ],
   "source": [
    "shape, trend = eval_density(TABSYN_DATA_PATH, TRAIN_DATA_PATH, INFO_PATH)\n",
    "print(\"Shape:\", shape)\n",
    "print(\"Trend:\", trend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-genome",
   "metadata": {},
   "source": [
    "# $\\alpha$-Precision and $\\beta$-Recall "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c9f731",
   "metadata": {},
   "source": [
    "$\\alpha$-Preicison and $\\beta$-Recall are generalizations of Precision and Recall metrics proposed by [Sajjadi et al.](https://proceedings.neurips.cc/paper/2018/hash/f7696a9b362ac5a51c3dc8f098b73923-Abstract.html) in 2018. These metrics can range between $[0, 1]$ and the closest they are to $1$ the better.\n",
    "\n",
    "\n",
    "Preicison measures the fidelity or quality of sythetic data. In more clear terms, it computes the proporation of synthetic datapoints that are *close* to real datapoints. \n",
    "Recall, on the other hand, measures the diversity of synthetic data; i.e. the extent to which these samples cover the full variability of real samples. More clearly, recall computes the proportion of real datapoints that are *close* to synthetic datapoints.\n",
    "\n",
    "\n",
    "If we denote the distibution of real datapoints by $P(X)$ and the distribution of sythetic datapoints by $Q(Y)$, precision is the portion of $Q(Y)$ that can be generated by $P(X)$, while recall is the portion of $P(X)$ that can be generated by $Q(Y)$.\n",
    "\n",
    "To better understand these concepts, let's assume that the real/generated dataponits are samples from an underlying real/generated manifold.\n",
    "Precision measures the proportation of generated datapoints that fall on the real manifold, while recall measures the proportion of real datapoints that fall on the generated manifold.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn_precision.png\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn_recall.png\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "The Precision-Recall metrics are very sensitive to outliers since even a few outliers can greatly change the shape of the underlying manifold.\n",
    "To address this limitation, $\\pmb{\\alpha}$**-Precision** and $\\pmb{\\beta}$**-Recall** are defined by assuming that a fraction $1−\\alpha$ (or $1−\\beta$) of the real (and synthetic) data are “outliers”, and $\\alpha$ (or $\\beta$) are “typical”. \n",
    "$\\alpha$-Precision is the fraction of synthetic samples that resemble the “most typical” fraction $\\alpha$ of real samples, whereas $\\beta$-Recall is the fraction of real samples covered by the most typical fraction $\\beta$ of synthetic samples.\n",
    "The two metrics are evaluated for all $\\alpha, \\beta \\in [0, 1]$, providing entire precision and recall curves instead of single numbers.\n",
    "\n",
    "\n",
    "To illustrate, consider the below image. Blue and red points are real and generated datapoints, respectively. The large blue and red spheres show the underlying manifold that was estimated from real and generated datapoints.\n",
    "Good quality generated datapoints should fall within the blue sphere like image *(c)*. They should not lie far from the blue sphere like *(a)*. Moreover, they should not be placed too close (or *copied*) to a real datapoint like *(b)*.\n",
    "Image *(d)* shows an outlier in the real datapoints which is cut outside of the manifold due to the application on $\\alpha$ and $\\beta$.\n",
    "If we used vanilla Precision and Recall, the blue sphere's radius should have increased to include the outlier which would also lead it to include noisy synthetic datapoints like *(a)*.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn_alpha_precision_beta_recall.png\" width=\"500\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "faced-accuracy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/synthetic_data/IBM_AML_FeatureEngineered_FS1/tabsyn.csv\n",
      "=========== All Features ===========\n",
      "Data shape:  (81, 50)\n",
      "Alpha precision: 0.07242798353909452\n",
      "Beta recall: 0.04115226337448563\n"
     ]
    }
   ],
   "source": [
    "alpha_precision_all, beta_recall_all = eval_quality(\n",
    "    TABSYN_DATA_PATH, TRAIN_DATA_PATH, INFO_PATH\n",
    ")\n",
    "print(\"Alpha precision:\", alpha_precision_all)\n",
    "print(\"Beta recall:\", beta_recall_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-duncan",
   "metadata": {},
   "source": [
    "# Machine Learning Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92c0e04",
   "metadata": {},
   "source": [
    "This method trains a machine learning model (in our case, an XGBoost model) on the synthetic data and evaluates it on the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "classified-editing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                       | 0/36 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## does a grid search over given params and reports all scores for each best of them\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# tabular dataload and tabular transformer look extra\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m overall_score \u001b[38;5;241m=\u001b[39m \u001b[43meval_mle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTABSYN_DATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTEST_DATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mINFO_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTABSYN - Overall score:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m pprint(overall_score)\n",
      "File \u001b[0;32m/fs01/home/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/scripts/eval/eval_mle.py:47\u001b[0m, in \u001b[0;36meval_mle\u001b[0;34m(train_path, test_path, info_path)\u001b[0m\n\u001b[1;32m     38\u001b[0m             overall_scores[score_name][name] \u001b[38;5;241m=\u001b[39m method\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     (\n\u001b[1;32m     42\u001b[0m         best_f1_scores,\n\u001b[1;32m     43\u001b[0m         best_weighted_scores,\n\u001b[1;32m     44\u001b[0m         best_auroc_scores,\n\u001b[1;32m     45\u001b[0m         best_acc_scores,\n\u001b[1;32m     46\u001b[0m         best_avg_scores,\n\u001b[0;32m---> 47\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     overall_scores \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m score_name \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_f1_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_weighted_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_avg_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     56\u001b[0m     ]:\n",
      "File \u001b[0;32m/fs01/home/ws_aabboud/diffusion_model_bootcamp/.venv/lib/python3.9/site-packages/sklearn/utils/_testing.py:160\u001b[0m, in \u001b[0;36m_IgnoreWarnings.__call__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m    159\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategory)\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/scripts/eval/mle/mle.py:559\u001b[0m, in \u001b[0;36m_evaluate_binary_classification\u001b[0;34m(train, test, info)\u001b[0m\n\u001b[1;32m    556\u001b[0m     pred_prob \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(x_valid)\n\u001b[1;32m    558\u001b[0m binary_f1 \u001b[38;5;241m=\u001b[39m f1_score(y_valid, pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 559\u001b[0m weighted_f1 \u001b[38;5;241m=\u001b[39m \u001b[43m_weighted_f1\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_valid, pred)\n\u001b[1;32m    561\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(y_valid, pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/fs01/home/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/scripts/eval/mle/mle.py:331\u001b[0m, in \u001b[0;36m_weighted_f1\u001b[0;34m(y_test, pred)\u001b[0m\n\u001b[1;32m    328\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(report\u001b[38;5;241m.\u001b[39mkeys())[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m    329\u001b[0m proportion \u001b[38;5;241m=\u001b[39m [report[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_test) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m classes]\n\u001b[1;32m    330\u001b[0m weighted_f1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\n\u001b[0;32m--> 331\u001b[0m     \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprop\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf1-score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproportion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m )\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weighted_f1\n",
      "File \u001b[0;32m/fs01/home/ws_aabboud/diffusion_model_bootcamp/deloitte_team/single_table_synthesis/scripts/eval/mle/mle.py:333\u001b[0m, in \u001b[0;36m_weighted_f1.<locals>.<lambda>\u001b[0;34m(i, prop)\u001b[0m\n\u001b[1;32m    328\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(report\u001b[38;5;241m.\u001b[39mkeys())[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m    329\u001b[0m proportion \u001b[38;5;241m=\u001b[39m [report[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_test) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m classes]\n\u001b[1;32m    330\u001b[0m weighted_f1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mmap\u001b[39m(\n\u001b[0;32m--> 333\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m i, prop: \u001b[43mreport\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf1-score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    334\u001b[0m             classes,\n\u001b[1;32m    335\u001b[0m             proportion,\n\u001b[1;32m    336\u001b[0m         )\n\u001b[1;32m    337\u001b[0m     )\n\u001b[1;32m    338\u001b[0m )\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weighted_f1\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "## does a grid search over given params and reports all scores for each best of them\n",
    "# tabular dataload and tabular transformer look extra\n",
    "\n",
    "# overall_score = eval_mle(TABSYN_DATA_PATH, TEST_DATA_PATH, INFO_PATH)\n",
    "# print(\"TABSYN - Overall score:\")\n",
    "# pprint(overall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803cae50",
   "metadata": {},
   "source": [
    "As baseline, we also evaluate a similar ML model (i.e. XGBoost) on the real training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5cfce5cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 36/36 [00:02<00:00, 14.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE - Overall score:\n",
      "{'best_acc_scores': {'XGBClassifier': {'accuracy': 0.6666666666666666,\n",
      "                                       'binary_f1': 0.5714285714285714,\n",
      "                                       'roc_auc': 0.7777777777777778,\n",
      "                                       'weighted_f1': 0.6233766233766235}},\n",
      " 'best_auroc_scores': {'XGBClassifier': {'accuracy': 0.6666666666666666,\n",
      "                                         'binary_f1': 0.5714285714285714,\n",
      "                                         'roc_auc': 0.6111111111111112,\n",
      "                                         'weighted_f1': 0.6233766233766235}},\n",
      " 'best_avg_scores': {'XGBClassifier': {'accuracy': 0.6666666666666666,\n",
      "                                       'binary_f1': 0.5714285714285714,\n",
      "                                       'roc_auc': 0.7777777777777778,\n",
      "                                       'weighted_f1': 0.6233766233766235}},\n",
      " 'best_f1_scores': {'XGBClassifier': {'accuracy': 0.6666666666666666,\n",
      "                                      'binary_f1': 0.5714285714285714,\n",
      "                                      'roc_auc': 0.7777777777777778,\n",
      "                                      'weighted_f1': 0.6233766233766235}},\n",
      " 'best_weighted_scores': {'XGBClassifier': {'accuracy': 0.6666666666666666,\n",
      "                                            'binary_f1': 0.5714285714285714,\n",
      "                                            'roc_auc': 0.7777777777777778,\n",
      "                                            'weighted_f1': 0.6233766233766235}}}\n"
     ]
    }
   ],
   "source": [
    "overall_score = eval_mle(TRAIN_DATA_PATH, TEST_DATA_PATH, INFO_PATH)\n",
    "print(\"BASELINE - Overall score:\")\n",
    "pprint(overall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd38e9",
   "metadata": {},
   "source": [
    "# Privacy Protection: Distance to Closest Record (DCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476c5c1",
   "metadata": {},
   "source": [
    "One of the applications of synthetically generated data is protecting sensitive information while creating similar substitute data that could be used to train machine learning models or published on public platforms.\n",
    "For this purpose, we must ensure that the synthetic datapoints are far enough from any real datapoints to prevent leaking of real sensitive information.\n",
    "\n",
    "One metric that is used for this purpose is **Distance to Closest Record (DCR)**.\n",
    "DCR is the Euclidean distance between a synthetic datapoint and its nearest real datapoint.\n",
    "DCR equal to zero means that the synthetic datapoint will leak the real information, while higher DCR values mean less risk of privacy leakage.\n",
    "\n",
    "`eval_dcr` computes the DCR of each synthetic datapoint to real datapoints in two different sets: training and test. Then, it returns the proportion of synthetic datapoints that are closer to the training dataset than the test dataset.\n",
    "If the size of the training and test datasets are equal, this score should ideally be $0.5$ indicating that the model has not overfit to training data and the synthetic datapoints are not memorized copies of training data.\n",
    "If the size of the training and test datasets are different, the ideal value for this score is #Train / (#Train + #Test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3b0b92f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# review json file and its contents\n",
    "with open(INFO_PATH, \"r\") as file:\n",
    "    data_info = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-roller",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCR Score, a value closer to 0.9 is better\n",
      "Distance to Closest Record: 0.9014444444444445\n"
     ]
    }
   ],
   "source": [
    "# ideal_dcr = data_info[\"train_num\"] / (data_info[\"train_num\"] + data_info[\"test_num\"])\n",
    "\n",
    "# dcr_score = eval_dcr(TABDDPM_DATA_PATH, TRAIN_DATA_PATH, TEST_DATA_PATH, INFO_PATH)\n",
    "# print(f\"DCR Score, a value closer to {ideal_dcr} is better\")\n",
    "# print(\"Distance to Closest Record:\", dcr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "990d31ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCR Score, a value closer to 0.9 is better\n",
      "Distance to Closest Record: 0.0\n"
     ]
    }
   ],
   "source": [
    "ideal_dcr = data_info[\"train_num\"] / (data_info[\"train_num\"] + data_info[\"test_num\"])\n",
    "\n",
    "dcr_score = eval_dcr(TABSYN_DATA_PATH, TRAIN_DATA_PATH, TEST_DATA_PATH, INFO_PATH)\n",
    "print(f\"DCR Score, a value closer to {ideal_dcr} is better\")\n",
    "print(\"Distance to Closest Record:\", dcr_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf7d99",
   "metadata": {},
   "source": [
    "# Detection: Classifier Two Sample Tests (C2ST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff5955",
   "metadata": {},
   "source": [
    "This metric evaluates if the synthetic data can be detected from the real data via a machine learning model, hence measuring how difficult it is to distinguish synthetic from real data. A logistic regression model is used in `eval_detection`.\n",
    "\n",
    "This score is measured through below steps:\n",
    "1. Create a single, augmented table that has all the rows of real data and all the rows of synthetic data. Add an extra column to keep track of whether each original row is real or synthetic.\n",
    "2. Split the augmented data to create a training and validation sets.\n",
    "3. Choose a machine learning model. Train the model on the training split. The model will predict whether each row is real or synthetic (i.e. predict the extra column we created in step #1).\n",
    "4. Validate the model on the validation set.\n",
    "5. Repeat steps #2-4 multiple times.\n",
    "The final score is based on the average ROC-AUC score across all the cross validation splits,\n",
    "\n",
    "$score = 1 - (max($ <span style=\"text-decoration:overline\">ROC-AUC</span> $, 0.5) \\times 2 - 1)$\n",
    ".\n",
    "\n",
    "This score can range between $[0, 1]$ with $0$ being the lowest (meaning that the machine learning model can perfectly identify synthetic data apart from the real data), and $1$ being the highest (meaning that the machine learning model cannot identify the synthetic data apart from the real data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "compressed-executive",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABSYN - Detection score: 0.005486968449931573\n"
     ]
    }
   ],
   "source": [
    "detection_score = eval_detection(TABSYN_DATA_PATH, TRAIN_DATA_PATH, INFO_PATH, dataname, model=\"tabsyn\")\n",
    "print(\"TABSYN - Detection score:\", detection_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d7e5725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# detection_score = eval_detection(TABDDPM_DATA_PATH, TRAIN_DATA_PATH, INFO_PATH, dataname, model=\"tabsyn\")\n",
    "# print(\"TABDDPM - Detection score:\", detection_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b09421b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE - Detection score: 1.0\n"
     ]
    }
   ],
   "source": [
    "detection_score = eval_detection(TRAIN_DATA_PATH, TRAIN_DATA_PATH, INFO_PATH, dataname, model=\"tabsyn\")\n",
    "print(\"BASELINE - Detection score:\", detection_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92710a70",
   "metadata": {},
   "source": [
    "# Missing Value Imputation for the Target Column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067dca5",
   "metadata": {},
   "source": [
    "We can use a trained TabSyn model for imputation with any missing columns during inference. An interesting application of this is treating classification/regression as missing value imputation tasks. Here's how we approach this:\n",
    "\n",
    "1. Masking Target Columns:\n",
    "    - For numerical columns, missing values are replaced with the average values from the training set.\n",
    "    - For categorical columns, in each imputation trial, we randomly select from all possible categories with uniform probabilities.\n",
    "2. Feeding Data into the Model:\n",
    "    - The masked data is fed into the VAE model to obtain embeddings.\n",
    "3. Applying Diffusion Model Inpainting:\n",
    "    - We apply diffusion model inpainting to these embeddings to get the imputed embeddings.\n",
    "    - In diffusion inpainting, the reverse step combines the forward process of known (observed) parts with the backward process of unknown (imputed) parts.\n",
    "    - For more details on this process, refer to the [RePaint paper](https://arxiv.org/pdf/2201.09865).\n",
    "\n",
    "Since diffusion inpainting is stochastic, and for categorical columns we randomly sample the category, we need to repeat the imputation algorithm several times (e.g., 50) and take the averaged imputation result as the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "numeric-crystal",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 1 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 2 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 3 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 4 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 5 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 6 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 7 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 8 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 9 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 10 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 11 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 12 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 13 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 14 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 15 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 16 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 17 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 18 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 19 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 20 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 21 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 22 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 23 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 24 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 25 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 26 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 27 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 28 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n",
      "Trial 29 started!\n",
      "No NaNs in numerical features, skipping\n",
      "No NaNs in numerical features, skipping\n",
      "(9, 14)\n"
     ]
    }
   ],
   "source": [
    "# Change that path to your local modal path to impute\n",
    "MODEL_PATH = os.path.join('models','tabsyn')\n",
    "IMPUTE_PATH = \"impute/tabsyn\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "impute(dataname, PROCESSED_DATA_DIR, INFO_PATH, MODEL_PATH, IMPUTE_PATH, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c56ce88",
   "metadata": {},
   "source": [
    "Finally, to evaluate the imputation quality, we compute the ROC-AUC and Micro-F1 score of the imputed target column against the real target column and reported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c6514ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-F1: 0.2222222222222222\n",
      "AUC: 0.25925925925925924\n",
      "\n",
      "Confusion Matrix:\n",
      "          Predicted N  Predicted Y\n",
      "Actual N            2            0\n",
      "Actual Y            7            0\n"
     ]
    }
   ],
   "source": [
    "# Uncomment below line to evaluate pre-imputed data\n",
    "# IMPUTE_PATH = \"/projects/diffusion_bootcamp/data/tabular/impute_data/tabsyn\"\n",
    "\n",
    "eval_impute(dataname, PROCESSED_DATA_DIR, IMPUTE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_models",
   "language": "python",
   "name": "diffusion_models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
