{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "  \n",
    "# TABSYN: Tabular Data Synthesis with Diffusion Models\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two challenges regarding the extention of diffusion models to tabular data are:\n",
    "1. **Diverse data types:** a single table can have different columns each containing data of different types, including numerical, categorical, text, etc.\n",
    "2. **Varied distributions:** the distribution of data under different columns in a single table varry widely from column to column.\n",
    "\n",
    "**TabSyn** addresses these challenges by introducing a latent space where tabular data of all columns are jointly represented. It then proceedes to train a diffusion model on the latent representations.\n",
    "This tactic allows TabSyn to:\n",
    "1. Train a single diffusion model for all data types in the dataset (i.e. Generality).\n",
    "2. Optimize the distribution of latent embeddings to facilitate training of the subsequent diffusion model, thus generating higher quality synthetic data (i.e. Quality).\n",
    "3. Require much fewer reverse steps during training of the diffusion model, and synthesize data faster (i.e. Speed).\n",
    "\n",
    "In this notebook, we review and implement the TabSyn model. The notebook is organized as follows:\n",
    "\n",
    "1. [Imports and Setup]()\n",
    "\n",
    "\n",
    "2. [Default Dataset]()\n",
    "    \n",
    "    \n",
    "3. [TabSyn Algorithm]()\n",
    "    \n",
    "    3.1. [Load Config]()\n",
    "    \n",
    "    3.2. [Make Dataset]()\n",
    "    \n",
    "    3.3. [Instantiate Model]()\n",
    "    \n",
    "    3.4. [Train Model]()\n",
    "        \n",
    "    3.5. [Load Pretrained Model]()\n",
    "    \n",
    "    3.6. [Sample Data]()\n",
    "    \n",
    "    3.7. [Review Synthetic Data]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we import all necessary libraries and modules required for setting up the environment.\n",
    "Most of the libraries we need to implement TabSyn are the same as TabDDPM.\n",
    "We also specify `NAME_URL_DICT_UCI`, `DATA_NAME`, `DATA_DIR` and other paths as in TabDDPM's implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/projects/diffusion_bootcamp/datasets/ibmaml/HI-Small_Trans.csv')\n",
    "\n",
    "# Feature Engineering 1: identical account activity\n",
    "df[\"Identitcal Account Activity\"] = ((df[\"Account\"] == df[\"Account.1\"]) & (df[\"From Bank\"] == df[\"To Bank\"])).astype(int)\n",
    "\n",
    "# Feature Engineering 2: Timestamp normalization\n",
    "df['Datetime'] = pd.to_datetime(df['Timestamp'], format='%Y/%m/%d %H:%M')\n",
    "df[\"SecondsSinceEpoch\"] = (df['Datetime'].astype(int) // 10**9)\n",
    "\n",
    "# Final DF\n",
    "# removed \"From Bank\", \"Account\",\"To Bank\",\"Account.1\",  from the en\n",
    "feature_engineered_df = df[[\"SecondsSinceEpoch\", \"Identitcal Account Activity\", \"Amount Received\", \"Receiving Currency\", \"Amount Paid\", \"Payment Currency\", \"Payment Format\", \"Is Laundering\"]]\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_column_info(df):\n",
    "    column_info = {col: str(df[col].dtype).replace('float64', 'float').replace('int64', 'int').replace('object', 'str') for col in df.columns}\n",
    "    return column_info\n",
    "\n",
    "# Get column info\n",
    "column_info = get_column_info(feature_engineered_df)\n",
    "\n",
    "# Print in desired format\n",
    "print({\"column_info\": column_info})"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "feature_engineered_df"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Dataset\n",
    "\n",
    "For more explanation of different steps in this section, please refer to TabDDPM's notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing dataset default from UCI.\n",
      "Already downloaded.\n",
      "Processing and Saving default Successfully!\n",
      "Dataset Name: default\n",
      "Total Size: 30000\n",
      "Train Size: 27000\n",
      "Test Size: 3000\n",
      "Number of Numerical Columns: 14\n",
      "Number of Categorical Columns: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>university</td>\n",
       "      <td>married</td>\n",
       "      <td>24.0</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>18457.0</td>\n",
       "      <td>21381.0</td>\n",
       "      <td>18914.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>678.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>university</td>\n",
       "      <td>married</td>\n",
       "      <td>39.0</td>\n",
       "      <td>payment delay for four months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>...</td>\n",
       "      <td>125357.0</td>\n",
       "      <td>121853.0</td>\n",
       "      <td>124731.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6216.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>4552.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>230000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>university</td>\n",
       "      <td>single</td>\n",
       "      <td>23.0</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>...</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>12525.0</td>\n",
       "      <td>12219.0</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>14019.0</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>12525.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>725.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>graduate school</td>\n",
       "      <td>married</td>\n",
       "      <td>35.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160000.0</td>\n",
       "      <td>male</td>\n",
       "      <td>graduate school</td>\n",
       "      <td>married</td>\n",
       "      <td>39.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2920.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2920.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12140.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>male</td>\n",
       "      <td>high school</td>\n",
       "      <td>others</td>\n",
       "      <td>59.0</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>18055.0</td>\n",
       "      <td>18755.0</td>\n",
       "      <td>20299.0</td>\n",
       "      <td>1596.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>male</td>\n",
       "      <td>university</td>\n",
       "      <td>single</td>\n",
       "      <td>42.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>17029.0</td>\n",
       "      <td>10575.0</td>\n",
       "      <td>9478.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>130000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>graduate school</td>\n",
       "      <td>single</td>\n",
       "      <td>26.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>pay duly</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>-884.0</td>\n",
       "      <td>-6332.0</td>\n",
       "      <td>-9333.0</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>6730.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>5448.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>300000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>university</td>\n",
       "      <td>married</td>\n",
       "      <td>40.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>196467.0</td>\n",
       "      <td>201425.0</td>\n",
       "      <td>-8276.0</td>\n",
       "      <td>7028.0</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>10013.0</td>\n",
       "      <td>9505.0</td>\n",
       "      <td>175074.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>male</td>\n",
       "      <td>university</td>\n",
       "      <td>married</td>\n",
       "      <td>27.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for four months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>...</td>\n",
       "      <td>18472.0</td>\n",
       "      <td>18037.0</td>\n",
       "      <td>18487.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL     SEX        EDUCATION MARRIAGE   AGE  \\\n",
       "0    20000.0  female       university  married  24.0   \n",
       "1   200000.0  female       university  married  39.0   \n",
       "2   230000.0  female       university   single  23.0   \n",
       "3    50000.0  female  graduate school  married  35.0   \n",
       "4   160000.0    male  graduate school  married  39.0   \n",
       "5    20000.0    male      high school   others  59.0   \n",
       "6    50000.0    male       university   single  42.0   \n",
       "7   130000.0  female  graduate school   single  26.0   \n",
       "8   300000.0  female       university  married  40.0   \n",
       "9    20000.0    male       university  married  27.0   \n",
       "\n",
       "                            PAY_0                           PAY_2  \\\n",
       "0  payment delay for three months  payment delay for three months   \n",
       "1   payment delay for four months  payment delay for three months   \n",
       "2                        pay duly                        pay duly   \n",
       "3     payment delay for one month     payment delay for one month   \n",
       "4                         unknown                         unknown   \n",
       "5  payment delay for three months     payment delay for one month   \n",
       "6     payment delay for one month     payment delay for one month   \n",
       "7     payment delay for one month                        pay duly   \n",
       "8     payment delay for one month     payment delay for one month   \n",
       "9     payment delay for one month     payment delay for one month   \n",
       "\n",
       "                            PAY_3                           PAY_4  \\\n",
       "0     payment delay for one month     payment delay for one month   \n",
       "1  payment delay for three months  payment delay for three months   \n",
       "2                        pay duly                        pay duly   \n",
       "3     payment delay for one month                         unknown   \n",
       "4                         unknown                         unknown   \n",
       "5     payment delay for one month     payment delay for one month   \n",
       "6     payment delay for one month     payment delay for one month   \n",
       "7                        pay duly                        pay duly   \n",
       "8     payment delay for one month     payment delay for one month   \n",
       "9     payment delay for one month   payment delay for four months   \n",
       "\n",
       "                            PAY_5  ... BILL_AMT4  BILL_AMT5  BILL_AMT6  \\\n",
       "0     payment delay for one month  ...   18457.0    21381.0    18914.0   \n",
       "1  payment delay for three months  ...  125357.0   121853.0   124731.0   \n",
       "2                        pay duly  ...    1045.0    12525.0    12219.0   \n",
       "3                         unknown  ...       0.0        0.0        0.0   \n",
       "4                         unknown  ...       0.0     2920.0        0.0   \n",
       "5     payment delay for one month  ...   18055.0    18755.0    20299.0   \n",
       "6     payment delay for one month  ...   17029.0    10575.0     9478.0   \n",
       "7                         unknown  ...    -884.0    -6332.0    -9333.0   \n",
       "8     payment delay for one month  ...  196467.0   201425.0    -8276.0   \n",
       "9  payment delay for three months  ...   18472.0    18037.0    18487.0   \n",
       "\n",
       "   PAY_AMT1  PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \\\n",
       "0       0.0    1500.0    1600.0    1646.0     678.0    1000.0   \n",
       "1       0.0    6216.0   10000.0       0.0    5000.0    4552.0   \n",
       "2    1444.0   14019.0    1045.0   12525.0     244.0     725.0   \n",
       "3    2400.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4      35.0       0.0       0.0    2920.0       0.0   12140.0   \n",
       "5    1596.0    1600.0    1300.0    1000.0    2000.0       0.0   \n",
       "6    2500.0    2000.0    2500.0     500.0     500.0     500.0   \n",
       "7    1298.0    6730.0     900.0    5448.0       0.0   25000.0   \n",
       "8    7028.0    6000.0   50000.0   10013.0    9505.0  175074.0   \n",
       "9    3000.0   10000.0       0.0       0.0     900.0    2000.0   \n",
       "\n",
       "   default payment next month  \n",
       "0                           1  \n",
       "1                           1  \n",
       "2                           0  \n",
       "3                           0  \n",
       "4                           0  \n",
       "5                           1  \n",
       "6                           0  \n",
       "7                           0  \n",
       "8                           0  \n",
       "9                           0  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = \"data/\"\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, \"raw_data\")\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, \"processed_data\")\n",
    "SYNTH_DATA_DIR = os.path.join(DATA_DIR, \"synthetic_data\")\n",
    "DATA_NAME = \"IBM_AML_FeatureEngineered.json\"\n",
    "\n",
    "MODEL_PATH = \"models/tabsyn\"\n",
    "\n",
    "INFO_DIR = \"data_info\"\n",
    "process_data(DATA_NAME, INFO_DIR, DATA_DIR)\n",
    "\n",
    "# review data\n",
    "df = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, DATA_NAME, \"train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review json file and its contents\n",
    "with open(f\"{PROCESSED_DATA_DIR}/{DATA_NAME}/info.json\", \"r\") as file:\n",
    "    data_info = json.load(file)\n",
    "# pprint(data_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabSyn Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will describe the design of TabSyn as well as its main hyperparameters loaded through config, which affect the model’s effectiveness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TabSyn** consists of two parts:\n",
    "1. A *variational auto-encoder (VAE)* which learns a joint representation space for the given tabular data.\n",
    "2. A *Diffusion model* which learns the distribution of data in the joint representation space.\n",
    "\n",
    "The figure below shows a diagram of the TabSyn model.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figures/tabsyn.jpg\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VAE**\n",
    "\n",
    "The left-side of the figure shows the VAE which operates in the original data space. The VAE itself consists of two parts: an encoder and a decoder. It also contains the corresponding tokenizer and detokenizer.\n",
    "Each row of the input tabular data ($\\pmb{x}$) is tokenized, then embedded by a transformer. Another transformer decodes the embeddings and a detokenizer reconstructs the table ($\\pmb{\\tilde{x}}$). The VAE is trained by minimizing the reconstruction loss between $\\pmb{x}$ and $\\pmb{\\tilde{x}}$.\n",
    "\n",
    "After the VAE is fully trained, the whole data ($\\pmb{x}$) is tokenized and embedded. The embedding of each row is flattened to form a 1-dimensional vector $\\pmb{z}$.\n",
    "These 1-dimensional embeddings for all rows are stored on disk, and will later be used to train the diffusion model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diffusion**\n",
    "\n",
    "The right-side of the figure shows the diffusion model which operates in the latent representation space; in other words, it only *sees* the embeddings obtained by the VAE, not the original tabular data.\n",
    "The diffusion model can be similarly divided into two parts: a forward process, and a reverse process.\n",
    "\n",
    "The forward process receives the embedded data points. A single data point is denoted by $\\pmb{z_0}$ in the figure. Gaussian noise is incrementally added to the embeddings in numerous incremental steps during the forward process. The number of the steps is denoted by $T$ in the figure. $T$ should be high enough that the distribution of embeddings at step $t=T$ is essentially a standard Gaussian distribution; in other words, the signal-to-noise ratio is practically zero.\n",
    "\n",
    "The reverse process, on the other hand, learns to *predict* an earlier-step embedding (e.g. $\\pmb{z_{t-\\Delta t}}$) from a later-step embedding (e.g. $\\pmb{z_t}$) via a neural network.\n",
    "\n",
    "After the diffusion model is fully trained, the reverse process can estimate the data distribution at step $t=0$ if it receives a standard Gaussian distribution at step $t=T$. New data points can be synthesized by sampling from this estimated distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will load the configuration file that contains the hyperparameters for the TabSyn model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'impute': {'N': 20,\n",
      "            'SIGMA_MAX': 80,\n",
      "            'SIGMA_MIN': 0.002,\n",
      "            'S_churn': 1,\n",
      "            'S_max': inf,\n",
      "            'S_min': 0,\n",
      "            'S_noise': 1,\n",
      "            'num_steps': 50,\n",
      "            'num_trials': 50,\n",
      "            'rho': 7},\n",
      " 'loss_params': {'lambd': 0.7, 'max_beta': 0.01, 'min_beta': 1e-05},\n",
      " 'model_params': {'d_token': 4, 'factor': 32, 'n_head': 1, 'num_layers': 2},\n",
      " 'task_type': 'binclass',\n",
      " 'train': {'diffusion': {'batch_size': 4096,\n",
      "                         'num_dataset_workers': 4,\n",
      "                         'num_epochs': 9},\n",
      "           'optim': {'diffusion': {'factor': 0.9,\n",
      "                                   'lr': 0.001,\n",
      "                                   'patience': 20,\n",
      "                                   'weight_decay': 0},\n",
      "                     'vae': {'factor': 0.95,\n",
      "                             'lr': 0.001,\n",
      "                             'patience': 10,\n",
      "                             'weight_decay': 0}},\n",
      "           'vae': {'batch_size': 4096,\n",
      "                   'num_dataset_workers': 4,\n",
      "                   'num_epochs': 10}},\n",
      " 'transforms': {'cat_encoding': None,\n",
      "                'cat_min_frequency': None,\n",
      "                'cat_nan_policy': None,\n",
      "                'normalization': 'quantile',\n",
      "                'num_nan_policy': 'mean',\n",
      "                'y_policy': 'default'}}\n"
     ]
    }
   ],
   "source": [
    "config_path = os.path.join(\"src/baselines/tabsyn/configs\", f\"{DATA_NAME}.toml\")\n",
    "raw_config = src.load_config(config_path)\n",
    "\n",
    "pprint(raw_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file is a TOML file that contains the following hyperparameters:\n",
    "\n",
    "1. **model_params:** specifies the structure of the transformers (both encoder and decoder) in the VAE model, including number of transformer layers, number of self-attnetion heads and token dimension.\n",
    "\n",
    "2. **transforms:** specifies the transformations and preprocessing of the data before tokenization, such as cleaning, normalization, and encoding.\n",
    "    - For preprocessing numerical features, we use the gaussian quantile transformation and replace the NaN values with mean of each row.\n",
    "    - For categorical features, we use the one-hot encoding method. NaN values are left unchanged, but we have the option to replace them. We have the option to drop the values that appear with less than a given minimum frequency under each column. Furthermore, we have the option to add an extra encoding step for categorical features during tokenization.\n",
    "\n",
    "3. **train.vae:** specifies training parameters of the VAE, including batch size, number of epochs, and number of dataset workers.\n",
    "\n",
    "4. **train.diffusion:** specifies the same training parameters as above for the diffusion model.\n",
    "\n",
    "5. **train.optim.vae:** specifies the parameters of the *Adam* optimizer and the `ReduceLROnPlateau` learning rate scheduler used to train the VAE. Optimizer parameters include initial learning rate and weight decay. LR scheduler parameters includer `factor` and `patience`.\n",
    "\n",
    "6. **train.optim.diffusion:** specifies the same parameters as above for the diffusion model.\n",
    "\n",
    "7. **loss_params:** specifies parameters of the loss function used to train the VAE including `max_beta`, `min_beta` and `lambd`.\n",
    "\n",
    "$\\beta$ is the coefficient of the KL divergence term in the VAE loss formula,\n",
    "\n",
    "$\\mathcal{L}_{vae} = \\mathcal{L}_{mse} + \\mathcal{L}_{ce} + \\beta \\mathcal{L}_{kl}$\n",
    ".\n",
    "\n",
    "Parameters `max_beta` and `min_beta` determine the range of $\\beta$. $\\beta$ is first set to `max_beta`. If the loss stops decreasing for a certain number of epochs (e.g. $10$ epochs), then at the end of each epoch after that (e.g. epoch $11$, $12$, etc.) $\\beta$ is decreased by a factor of `lambd`,\n",
    "$\\beta_{new} = \\lambda \\beta_{curr}$,\n",
    "until it reaches `beta_min`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we pre-process the data and make a dataset object.\n",
    "\n",
    "First, we determine transformations needed for the dataset, such as normalization and cleaning, in `transforms`. Next, using `preprocess` function we load the data from disk in arrays that contain both training and test data (`X_num` and `X_cat`), as well as the number of categories for each categorical feature (`categories`) and the number of numerical features (`d_numerical`).\n",
    "\n",
    "We then separate the train and test data in different arrays and convert them to Pytorch tensors.\n",
    "We create a dataset object (`TabularDataset`) with the train data. `TabularDataset` is a simple module which returns the tokens of a single row at a time. Each row constiutes a single data sample in TabSyn. Afterwards, we create a Dataloader for the train data using the `batch_size` and `num_workers` specified in config.\n",
    "\n",
    "In contrast, we keep the test data as tensors (`X_test_num` and `X_test_cat`). If a GPU is available, we move these tensors to GPU so that they can be accessed by the model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaNs in numerical features, skipping\n"
     ]
    }
   ],
   "source": [
    "# preprocess data\n",
    "X_num, X_cat, categories, d_numerical = preprocess(os.path.join(PROCESSED_DATA_DIR, DATA_NAME),\n",
    "                                                   transforms = raw_config[\"transforms\"],\n",
    "                                                   task_type = raw_config[\"task_type\"])\n",
    "\n",
    "# separate train and test data\n",
    "X_train_num, X_test_num = X_num\n",
    "X_train_cat, X_test_cat = X_cat\n",
    "\n",
    "# convert to float tensor\n",
    "X_train_num, X_test_num = torch.tensor(X_train_num).float(), torch.tensor(X_test_num).float()\n",
    "X_train_cat, X_test_cat =  torch.tensor(X_train_cat), torch.tensor(X_test_cat)\n",
    "\n",
    "# create dataset module\n",
    "train_data = TabularDataset(X_train_num.float(), X_train_cat)\n",
    "\n",
    "# move test data to gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_test_num = X_test_num.float().to(device)\n",
    "X_test_cat = X_test_cat.to(device)\n",
    "\n",
    "# create train dataloader\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size = raw_config[\"train\"][\"vae\"][\"batch_size\"],\n",
    "    shuffle = True,\n",
    "    num_workers = raw_config[\"train\"][\"vae\"][\"num_dataset_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the model using the `TabSyn` class. `TabSyn` class takes the following arguments:\n",
    "\n",
    "1. `train_loader`: dataloader for train data.\n",
    "2. `X_test_num`: numerical features of the test data.\n",
    "3. `X_test_cat`: categorical features of the train data.\n",
    "4. `num_numerical_features`: number of numerical features in the dataset.\n",
    "5. `num_classes`: number of classes (i.e. categories) of each categorical feature in the dataset.\n",
    "6. `device`: the device on which the model and data exist, either \"cpu\" or \"cuda\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabsyn = TabSyn(train_loader,\n",
    "                X_test_num, X_test_cat,\n",
    "                num_numerical_features = d_numerical,\n",
    "                num_classes = categories,\n",
    "                device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TabSyn` class has the tools to instantiate VAE and diffusion models, train both, and sample from the trained diffusion model.\n",
    "We will demonstrate how to use these tools in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE and the diffusion model are trained independently. The following subsections explain each training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### A. Train VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to instantiate the VAE using the `instantiate_vae` method. This method takes the VAE model hyperparameters, optimizer and lr scheduler parameters from config, and instantiates them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "self.category_embeddings.weight.shape=torch.Size([79, 4])\n",
      "Successfully instantiated VAE model.\n"
     ]
    }
   ],
   "source": [
    "# instantiate VAE model for training\n",
    "tabsyn.instantiate_vae(**raw_config[\"model_params\"], optim_params = raw_config[\"train\"][\"optim\"][\"vae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have instantiated the VAE, we can train it using the `train_vae` function.\n",
    "This function receives the loss hyperparameters and number of epochs from the config.\n",
    "Moreover, it recieves `save_path` which is the directory where trained model checkpoints will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 3/3 [00:00<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, beta = 0.010000, Train MSE: 13.663756, Train CE:2.154334, Train KL:0.945002, Val MSE:12.495003, Val CE:2.104598, Train ACC:0.280852, Val ACC:0.286902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 3/3 [00:00<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, beta = 0.010000, Train MSE: 12.019743, Train CE:2.083766, Train KL:1.020754, Val MSE:11.045834, Val CE:2.061779, Train ACC:0.296256, Val ACC:0.301399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 3/3 [00:00<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, beta = 0.010000, Train MSE: 10.634787, Train CE:2.044676, Train KL:1.146477, Val MSE:9.823824, Val CE:2.036949, Train ACC:0.304690, Val ACC:0.300689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 3/3 [00:00<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, beta = 0.010000, Train MSE: 9.484139, Train CE:2.034360, Train KL:1.300207, Val MSE:8.833112, Val CE:2.014092, Train ACC:0.311833, Val ACC:0.318329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 3/3 [00:00<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, beta = 0.010000, Train MSE: 8.459760, Train CE:2.017638, Train KL:1.464170, Val MSE:7.862492, Val CE:1.994941, Train ACC:0.337866, Val ACC:0.348642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 3/3 [00:00<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, beta = 0.010000, Train MSE: 7.611962, Train CE:1.999599, Train KL:1.626225, Val MSE:7.053391, Val CE:1.986582, Train ACC:0.383262, Val ACC:0.402778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 3/3 [00:00<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, beta = 0.010000, Train MSE: 6.869578, Train CE:1.979303, Train KL:1.785650, Val MSE:6.403675, Val CE:1.964470, Train ACC:0.437565, Val ACC:0.456103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 3/3 [00:00<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, beta = 0.010000, Train MSE: 6.204691, Train CE:1.968276, Train KL:1.947868, Val MSE:5.838765, Val CE:1.956694, Train ACC:0.476162, Val ACC:0.486922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 3/3 [00:00<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, beta = 0.010000, Train MSE: 5.646989, Train CE:1.966729, Train KL:2.115103, Val MSE:5.310214, Val CE:1.970527, Train ACC:0.499656, Val ACC:0.497972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 3/3 [00:00<00:00,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, beta = 0.010000, Train MSE: 5.151427, Train CE:1.979931, Train KL:2.282740, Val MSE:4.881950, Val CE:1.979485, Train ACC:0.497676, Val ACC:0.498581\n",
      "Training time: 0.1025 mins\n",
      "Successfully trained and saved the VAE model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(f\"{MODEL_PATH}/{DATA_NAME}/vae\")\n",
    "tabsyn.train_vae(**raw_config[\"loss_params\"],\n",
    "                 num_epochs = raw_config[\"train\"][\"vae\"][\"num_epochs\"],\n",
    "                 save_path = os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the VAE, we embed the training data with the trained encoder and store the embeddings in a direcotry specified by `vae_ckpt_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved pretrained embeddings on disk!\n"
     ]
    }
   ],
   "source": [
    "# embed all inputs in the latent space\n",
    "tabsyn.save_vae_embeddings(X_train_num, X_train_cat,\n",
    "                           vae_ckpt_dir = os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Train Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have stored the training data embeddings, we need to load and prepare them for the diffusion model.\n",
    "We load the embeddings using `load_vae_embeddings`. We normalize the embeddings by subtracting the mean and dividing by the standard deviation. Then, we create a Dataloader with the specified `batch_size` and `num_workers` from the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load latent space embeddings\n",
    "train_z, _ = tabsyn.load_latent_embeddings(os.path.join(MODEL_PATH, DATA_NAME, \"vae\"))  # train_z dim: B x in_dim\n",
    "\n",
    "# normalize embeddings\n",
    "mean, std = train_z.mean(0), train_z.std(0)\n",
    "train_z = (train_z - mean) / std\n",
    "latent_train_data = train_z\n",
    "\n",
    "# create data loader\n",
    "latent_train_loader = DataLoader(\n",
    "    latent_train_data,\n",
    "    batch_size = raw_config[\"train\"][\"diffusion\"][\"batch_size\"],\n",
    "    shuffle = True,\n",
    "    num_workers = raw_config[\"train\"][\"diffusion\"][\"num_dataset_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is ready, we instantiate the diffusion model with `instantiate_diffusion`. The input dimension and hidden dimention of the diffusion model is determined by the dimension of the embeddings. \n",
    "Moreover, we instantiate the optimizer and lr scheduler using hyperparameters from config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPDiffusion(\n",
      "  (proj): Linear(in_features=96, out_features=1024, bias=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=1024, out_features=96, bias=True)\n",
      "  )\n",
      "  (map_noise): PositionalEmbedding()\n",
      "  (time_embed): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "The number of parameters: 10690656\n",
      "Successfully instantiated diffusion model.\n"
     ]
    }
   ],
   "source": [
    "# instantiate diffusion model for training\n",
    "tabsyn.instantiate_diffusion(in_dim = train_z.shape[1], hid_dim = train_z.shape[1], optim_params = raw_config[\"train\"][\"optim\"][\"diffusion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the diffusion model with `train_diffusion` function.\n",
    "This function takes the following arguements:\n",
    "1. `latent_train_loader`: dataloader for the latent representations which are used to train the diffusion model.\n",
    "2. `num_epochs`: number of training epochs.\n",
    "3. `ckpt_path`: directory where the model checkpoints will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/9: 100%|██████████| 3/3 [00:00<00:00,  4.81it/s, Loss=2.03]\n",
      "Epoch 2/9: 100%|██████████| 3/3 [00:00<00:00,  5.66it/s, Loss=1.73]\n",
      "Epoch 3/9: 100%|██████████| 3/3 [00:00<00:00,  5.37it/s, Loss=1.57]\n",
      "Epoch 4/9: 100%|██████████| 3/3 [00:00<00:00,  5.68it/s, Loss=1.46]\n",
      "Epoch 5/9: 100%|██████████| 3/3 [00:00<00:00,  5.47it/s, Loss=1.36]\n",
      "Epoch 6/9: 100%|██████████| 3/3 [00:00<00:00,  5.58it/s, Loss=1.26]\n",
      "Epoch 7/9: 100%|██████████| 3/3 [00:00<00:00,  5.65it/s, Loss=1.2] \n",
      "Epoch 8/9: 100%|██████████| 3/3 [00:00<00:00,  5.62it/s, Loss=1.19]\n",
      "Epoch 9/9: 100%|██████████| 3/3 [00:00<00:00,  5.53it/s, Loss=1.14]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  5.782466650009155\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(f\"{MODEL_PATH}/{DATA_NAME}\")\n",
    "# train diffusion model\n",
    "tabsyn.train_diffusion(latent_train_loader,\n",
    "                       num_epochs = raw_config[\"train\"][\"diffusion\"][\"num_epochs\"],\n",
    "                       ckpt_path = os.path.join(MODEL_PATH, DATA_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of training model from scratch, we can also load weights of a pre-trained model from a given checkpoint with `load_model_state` function.\n",
    "If we haven't instantiated the VAE and diffusion model beforehand, we need to instantiate them first using `instantiate_vae` and `instantiate_diffusion` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully instantiated VAE model.\n",
      "MLPDiffusion(\n",
      "  (proj): Linear(in_features=96, out_features=1024, bias=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "    (3): SiLU()\n",
      "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=1024, out_features=96, bias=True)\n",
      "  )\n",
      "  (map_noise): PositionalEmbedding()\n",
      "  (time_embed): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "The number of parameters: 10690656\n",
      "Successfully instantiated diffusion model.\n",
      "Loaded model state from /projects/diffusion_bootcamp/models/tabular/tabsyn/default_2\n"
     ]
    }
   ],
   "source": [
    "# instantiate VAE model\n",
    "tabsyn.instantiate_vae(**raw_config[\"model_params\"], optim_params = None)\n",
    "\n",
    "latent_embeddings_path = \"/projects/diffusion_bootcamp/models/tabular/tabsyn/default/vae\"\n",
    "# load latent embeddings of input data\n",
    "train_z, token_dim = tabsyn.load_latent_embeddings(latent_embeddings_path)\n",
    "\n",
    "# instantiate diffusion model\n",
    "tabsyn.instantiate_diffusion(in_dim = train_z.shape[1], hid_dim = train_z.shape[1], optim_params = None)\n",
    "\n",
    "pretrained_model_path = \"/projects/diffusion_bootcamp/models/tabular/tabsyn/default\"\n",
    "# load state from checkpoint\n",
    "tabsyn.load_model_state(ckpt_dir = pretrained_model_path,\n",
    "                        dif_ckpt_name = \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained the model effectively, using `sample` function we can generate synthetic data starting from compelete noise. The input of this function is as follows:\n",
    "\n",
    "1. `train_z`: latent embeddings of the training data.\n",
    "2. `info`: info about the data from the json file we reviewed at the beginning of this notebook.\n",
    "3. `num_inverse`: detokenizer for numerical features.\n",
    "4. `cat_inverse`: detokenizer for categorical features.\n",
    "5. `save_path`: file-path where the synthetic table will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaNs in numerical features, skipping\n",
      "(27000, 10)\n",
      "Time: 13.619803667068481\n",
      "Saving sampled data to data/synthetic_data/default/tabsyn.csv\n"
     ]
    }
   ],
   "source": [
    "# load data info file\n",
    "with open(os.path.join(PROCESSED_DATA_DIR, DATA_NAME, \"info.json\"), \"r\") as file:\n",
    "    data_info = json.load(file)\n",
    "data_info[\"token_dim\"] = token_dim\n",
    "\n",
    "# get inverse tokenizers\n",
    "_, _, categories, d_numerical, num_inverse, cat_inverse = preprocess(os.path.join(PROCESSED_DATA_DIR, DATA_NAME),\n",
    "                                                                     transforms = raw_config[\"transforms\"],\n",
    "                                                                     task_type = raw_config[\"task_type\"],\n",
    "                                                                     inverse = True)\n",
    "\n",
    "# sample data\n",
    "num_samples = train_z.shape[0]\n",
    "in_dim = train_z.shape[1] \n",
    "mean_input_emb = train_z.mean(0)\n",
    "tabsyn.sample(num_samples,\n",
    "              in_dim,\n",
    "              mean_input_emb,\n",
    "              info = data_info,\n",
    "              num_inverse = num_inverse,\n",
    "              cat_inverse = cat_inverse,\n",
    "              save_path = os.path.join(SYNTH_DATA_DIR, DATA_NAME, \"tabsyn.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally here, we review the synthesized data. In the following `evaluate_synthetic_data.ipynb` notebook, we will evaluate this synthesized data with respect to various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>university</td>\n",
       "      <td>married</td>\n",
       "      <td>46.0</td>\n",
       "      <td>payment delay for two months</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>426.36752</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>390.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>university</td>\n",
       "      <td>married</td>\n",
       "      <td>42.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>38604.0400</td>\n",
       "      <td>29854.2970</td>\n",
       "      <td>8425.42000</td>\n",
       "      <td>8114.09130</td>\n",
       "      <td>3335.9043</td>\n",
       "      <td>1719.04970</td>\n",
       "      <td>1198.85100</td>\n",
       "      <td>1955.60230</td>\n",
       "      <td>3629.17190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>high school</td>\n",
       "      <td>married</td>\n",
       "      <td>23.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>10154.2210</td>\n",
       "      <td>11260.9220</td>\n",
       "      <td>10000.80800</td>\n",
       "      <td>1198.67710</td>\n",
       "      <td>1400.0000</td>\n",
       "      <td>998.12720</td>\n",
       "      <td>733.49927</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>357.53696</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>240000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>high school</td>\n",
       "      <td>married</td>\n",
       "      <td>53.0</td>\n",
       "      <td>payment delay for four months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>178097.2800</td>\n",
       "      <td>167049.8800</td>\n",
       "      <td>171107.90000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>14097.7300</td>\n",
       "      <td>8849.46100</td>\n",
       "      <td>6000.00000</td>\n",
       "      <td>6702.95400</td>\n",
       "      <td>10032.20900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120000.0</td>\n",
       "      <td>male</td>\n",
       "      <td>university</td>\n",
       "      <td>married</td>\n",
       "      <td>32.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>81177.9600</td>\n",
       "      <td>76788.4300</td>\n",
       "      <td>79741.32000</td>\n",
       "      <td>3098.13750</td>\n",
       "      <td>4879.0130</td>\n",
       "      <td>4000.00000</td>\n",
       "      <td>3470.72490</td>\n",
       "      <td>5000.00000</td>\n",
       "      <td>2761.03880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>210000.0</td>\n",
       "      <td>male</td>\n",
       "      <td>university</td>\n",
       "      <td>single</td>\n",
       "      <td>27.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>17803.7900</td>\n",
       "      <td>18270.0660</td>\n",
       "      <td>8776.32100</td>\n",
       "      <td>1607.79960</td>\n",
       "      <td>1592.6553</td>\n",
       "      <td>397.29932</td>\n",
       "      <td>1031.17900</td>\n",
       "      <td>645.88776</td>\n",
       "      <td>599.63580</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>130000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>graduate school</td>\n",
       "      <td>single</td>\n",
       "      <td>28.0</td>\n",
       "      <td>payment delay for two months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>4084.5452</td>\n",
       "      <td>1196.6735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>406.65112</td>\n",
       "      <td>2283.2983</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>240000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>graduate school</td>\n",
       "      <td>single</td>\n",
       "      <td>27.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>393.83884</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>250000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>high school</td>\n",
       "      <td>single</td>\n",
       "      <td>29.0</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>160703.2700</td>\n",
       "      <td>257110.9700</td>\n",
       "      <td>154884.60000</td>\n",
       "      <td>132673.27000</td>\n",
       "      <td>30015.8710</td>\n",
       "      <td>11078.56100</td>\n",
       "      <td>108035.09000</td>\n",
       "      <td>6004.35200</td>\n",
       "      <td>5054.99500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>70000.0</td>\n",
       "      <td>female</td>\n",
       "      <td>university</td>\n",
       "      <td>married</td>\n",
       "      <td>47.0</td>\n",
       "      <td>payment delay for two months</td>\n",
       "      <td>payment delay for three months</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>payment delay for one month</td>\n",
       "      <td>...</td>\n",
       "      <td>70747.6000</td>\n",
       "      <td>29295.8930</td>\n",
       "      <td>29157.73000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3000.0000</td>\n",
       "      <td>3000.00000</td>\n",
       "      <td>1018.06110</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1500.00000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL     SEX        EDUCATION MARRIAGE   AGE  \\\n",
       "0    70000.0  female       university  married  46.0   \n",
       "1   500000.0  female       university  married  42.0   \n",
       "2    50000.0  female      high school  married  23.0   \n",
       "3   240000.0  female      high school  married  53.0   \n",
       "4   120000.0    male       university  married  32.0   \n",
       "5   210000.0    male       university   single  27.0   \n",
       "6   130000.0  female  graduate school   single  28.0   \n",
       "7   240000.0  female  graduate school   single  27.0   \n",
       "8   250000.0  female      high school   single  29.0   \n",
       "9    70000.0  female       university  married  47.0   \n",
       "\n",
       "                           PAY_0                           PAY_2  \\\n",
       "0   payment delay for two months                         unknown   \n",
       "1    payment delay for one month     payment delay for one month   \n",
       "2    payment delay for one month     payment delay for one month   \n",
       "3  payment delay for four months  payment delay for three months   \n",
       "4    payment delay for one month     payment delay for one month   \n",
       "5    payment delay for one month     payment delay for one month   \n",
       "6   payment delay for two months  payment delay for three months   \n",
       "7    payment delay for one month     payment delay for one month   \n",
       "8    payment delay for one month     payment delay for one month   \n",
       "9   payment delay for two months  payment delay for three months   \n",
       "\n",
       "                         PAY_3                        PAY_4  \\\n",
       "0                      unknown                      unknown   \n",
       "1  payment delay for one month  payment delay for one month   \n",
       "2  payment delay for one month  payment delay for one month   \n",
       "3  payment delay for one month  payment delay for one month   \n",
       "4  payment delay for one month  payment delay for one month   \n",
       "5  payment delay for one month  payment delay for one month   \n",
       "6  payment delay for one month  payment delay for one month   \n",
       "7  payment delay for one month                      unknown   \n",
       "8  payment delay for one month  payment delay for one month   \n",
       "9  payment delay for one month  payment delay for one month   \n",
       "\n",
       "                         PAY_5  ...    BILL_AMT4    BILL_AMT5     BILL_AMT6  \\\n",
       "0                      unknown  ...       0.0000       0.0000     426.36752   \n",
       "1  payment delay for one month  ...   38604.0400   29854.2970    8425.42000   \n",
       "2  payment delay for one month  ...   10154.2210   11260.9220   10000.80800   \n",
       "3  payment delay for one month  ...  178097.2800  167049.8800  171107.90000   \n",
       "4  payment delay for one month  ...   81177.9600   76788.4300   79741.32000   \n",
       "5  payment delay for one month  ...   17803.7900   18270.0660    8776.32100   \n",
       "6  payment delay for one month  ...    4084.5452    1196.6735       0.00000   \n",
       "7                      unknown  ...       0.0000       0.0000       0.00000   \n",
       "8  payment delay for one month  ...  160703.2700  257110.9700  154884.60000   \n",
       "9  payment delay for one month  ...   70747.6000   29295.8930   29157.73000   \n",
       "\n",
       "       PAY_AMT1    PAY_AMT2     PAY_AMT3      PAY_AMT4    PAY_AMT5  \\\n",
       "0       0.00000      0.0000      0.00000       0.00000   390.00000   \n",
       "1    8114.09130   3335.9043   1719.04970    1198.85100  1955.60230   \n",
       "2    1198.67710   1400.0000    998.12720     733.49927     0.00000   \n",
       "3       0.00000  14097.7300   8849.46100    6000.00000  6702.95400   \n",
       "4    3098.13750   4879.0130   4000.00000    3470.72490  5000.00000   \n",
       "5    1607.79960   1592.6553    397.29932    1031.17900   645.88776   \n",
       "6     406.65112   2283.2983      0.00000       0.00000     0.00000   \n",
       "7     393.83884      0.0000      0.00000       0.00000     0.00000   \n",
       "8  132673.27000  30015.8710  11078.56100  108035.09000  6004.35200   \n",
       "9       0.00000   3000.0000   3000.00000    1018.06110  1000.00000   \n",
       "\n",
       "      PAY_AMT6  default payment next month  \n",
       "0      0.00000                           1  \n",
       "1   3629.17190                           0  \n",
       "2    357.53696                           0  \n",
       "3  10032.20900                           0  \n",
       "4   2761.03880                           0  \n",
       "5    599.63580                           0  \n",
       "6      0.00000                           1  \n",
       "7      0.00000                           0  \n",
       "8   5054.99500                           0  \n",
       "9   1500.00000                           0  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(SYNTH_DATA_DIR, DATA_NAME, \"tabsyn.csv\"))\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "visualize_default(df).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zhang, Hengrui, et al.** \"Mixed-type tabular data synthesis with score-based diffusion in latent space.\" *International Conference on Learning Representations (ICLR)* (2023).\n",
    "\n",
    "**GitHub Repository:** [Amazon Science - Tabsyn](https://github.com/amazon-science/tabsyn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_models_show",
   "language": "python",
   "name": "diffusion_models_show"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
